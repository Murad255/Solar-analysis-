{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffb95b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 09:01:54.709409: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 09:01:55.476951: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-31 09:01:55.477057: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-31 09:01:55.477066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import ephem\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from generate_features import generate_features, generate_time_features\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554c042",
   "metadata": {},
   "source": [
    "В коде используется ephem, чтобы получить данные о положении солнца. \n",
    "Для этого нужно задать координаты солнечной батареи и дату.\n",
    "На случай, если не получится портировать эту библиотеку на андроид, я обучил отдельно два вида моделей:\n",
    "- simple. Модель только на секундах\n",
    "- обычная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3e895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAT, LON = 55.775588, 37.605662 # latitude, longitude\n",
    "year=2022\n",
    "month=7\n",
    "day=13\n",
    "observer = ephem.Observer()\n",
    "observer.lat = LAT\n",
    "observer.lon = LON\n",
    "observer.date = datetime(year=2022, month=7, day=13)\n",
    "sun = ephem.Sun(observer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81af9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sun.compute(observer.date.datetime()+timedelta(seconds=20000))\n",
    "# hlon — Astrometric heliocentric longitude\n",
    "# hlat — Astrometric heliocentric latitude \n",
    "# earth_distance — Distance to Earth (AU)\n",
    "# phase — Percent of surface illuminated\n",
    "# sun.hlon\n",
    "# sun.hlat\n",
    "# sun.earth_distance\n",
    "# sun.phase\n",
    "# sun.radius\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eaa5c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"solar1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79ff51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec</th>\n",
       "      <th>nv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7291.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48805.446578</td>\n",
       "      <td>1467.770111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23176.436968</td>\n",
       "      <td>945.345932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9342.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28787.000000</td>\n",
       "      <td>147.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>47964.000000</td>\n",
       "      <td>2050.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>69943.500000</td>\n",
       "      <td>2212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89306.000000</td>\n",
       "      <td>2456.160000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sec           nv\n",
       "count   7291.000000  7291.000000\n",
       "mean   48805.446578  1467.770111\n",
       "std    23176.436968   945.345932\n",
       "min     9342.000000     0.000000\n",
       "25%    28787.000000   147.955000\n",
       "50%    47964.000000  2050.120000\n",
       "75%    69943.500000  2212.000000\n",
       "max    89306.000000  2456.160000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f508bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_features(LAT=LAT, LON=LON, data=data, year=year, month=month, day=day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f988a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec</th>\n",
       "      <th>nv</th>\n",
       "      <th>hlon</th>\n",
       "      <th>hlat</th>\n",
       "      <th>earth_distance</th>\n",
       "      <th>radius</th>\n",
       "      <th>mag</th>\n",
       "      <th>a_dec</th>\n",
       "      <th>a_ra</th>\n",
       "      <th>sec2</th>\n",
       "      <th>sec5</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9342</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.074145</td>\n",
       "      <td>9.098155e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>1.956124</td>\n",
       "      <td>87272964</td>\n",
       "      <td>96.654022</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9353</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.074147</td>\n",
       "      <td>9.098679e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>1.956126</td>\n",
       "      <td>87478609</td>\n",
       "      <td>96.710909</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9363</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.074149</td>\n",
       "      <td>9.099156e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>1.956128</td>\n",
       "      <td>87665769</td>\n",
       "      <td>96.762596</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9374</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.074151</td>\n",
       "      <td>9.099681e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>1.956131</td>\n",
       "      <td>87871876</td>\n",
       "      <td>96.819420</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9385</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.074153</td>\n",
       "      <td>9.100205e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381976</td>\n",
       "      <td>1.956133</td>\n",
       "      <td>88078225</td>\n",
       "      <td>96.876210</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7286</th>\n",
       "      <td>89265</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.089536</td>\n",
       "      <td>1.233494e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>1.972508</td>\n",
       "      <td>7968240225</td>\n",
       "      <td>298.772489</td>\n",
       "      <td>-1</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7287</th>\n",
       "      <td>89275</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.089538</td>\n",
       "      <td>1.233527e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>1.972510</td>\n",
       "      <td>7970025625</td>\n",
       "      <td>298.789223</td>\n",
       "      <td>-1</td>\n",
       "      <td>47</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7288</th>\n",
       "      <td>89285</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.089540</td>\n",
       "      <td>1.233560e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>1.972512</td>\n",
       "      <td>7971811225</td>\n",
       "      <td>298.805957</td>\n",
       "      <td>-1</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7289</th>\n",
       "      <td>89296</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.089542</td>\n",
       "      <td>1.233596e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379629</td>\n",
       "      <td>1.972514</td>\n",
       "      <td>7973775616</td>\n",
       "      <td>298.824363</td>\n",
       "      <td>-1</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7290</th>\n",
       "      <td>89306</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.089544</td>\n",
       "      <td>1.233629e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379629</td>\n",
       "      <td>1.972516</td>\n",
       "      <td>7975561636</td>\n",
       "      <td>298.841095</td>\n",
       "      <td>-1</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7291 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sec    nv      hlon          hlat  earth_distance    radius   mag  \\\n",
       "0      9342  0.00  5.074145  9.098155e-07        1.016533  0.004577 -26.8   \n",
       "1      9353  0.00  5.074147  9.098679e-07        1.016533  0.004577 -26.8   \n",
       "2      9363  0.00  5.074149  9.099156e-07        1.016533  0.004577 -26.8   \n",
       "3      9374  0.00  5.074151  9.099681e-07        1.016533  0.004577 -26.8   \n",
       "4      9385  0.00  5.074153  9.100205e-07        1.016533  0.004577 -26.8   \n",
       "...     ...   ...       ...           ...             ...       ...   ...   \n",
       "7286  89265  6.63  5.089536  1.233494e-06        1.016497  0.004577 -26.8   \n",
       "7287  89275  6.63  5.089538  1.233527e-06        1.016497  0.004577 -26.8   \n",
       "7288  89285  5.31  5.089540  1.233560e-06        1.016497  0.004577 -26.8   \n",
       "7289  89296  6.63  5.089542  1.233596e-06        1.016497  0.004577 -26.8   \n",
       "7290  89306  6.63  5.089544  1.233629e-06        1.016497  0.004577 -26.8   \n",
       "\n",
       "         a_dec      a_ra        sec2        sec5  hour  minute  second  \n",
       "0     0.381977  1.956124    87272964   96.654022     2      35      42  \n",
       "1     0.381977  1.956126    87478609   96.710909     2      35      53  \n",
       "2     0.381977  1.956128    87665769   96.762596     2      36       3  \n",
       "3     0.381977  1.956131    87871876   96.819420     2      36      14  \n",
       "4     0.381976  1.956133    88078225   96.876210     2      36      25  \n",
       "...        ...       ...         ...         ...   ...     ...     ...  \n",
       "7286  0.379630  1.972508  7968240225  298.772489    -1      47      45  \n",
       "7287  0.379630  1.972510  7970025625  298.789223    -1      47      55  \n",
       "7288  0.379630  1.972512  7971811225  298.805957    -1      48       5  \n",
       "7289  0.379629  1.972514  7973775616  298.824363    -1      48      16  \n",
       "7290  0.379629  1.972516  7975561636  298.841095    -1      48      26  \n",
       "\n",
       "[7291 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f294adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec</th>\n",
       "      <th>nv</th>\n",
       "      <th>hlon</th>\n",
       "      <th>hlat</th>\n",
       "      <th>earth_distance</th>\n",
       "      <th>radius</th>\n",
       "      <th>mag</th>\n",
       "      <th>a_dec</th>\n",
       "      <th>a_ra</th>\n",
       "      <th>sec2</th>\n",
       "      <th>sec5</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7.291000e+03</td>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7.291000e+03</td>\n",
       "      <td>7.291000e+03</td>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7.291000e+03</td>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7291.000000</td>\n",
       "      <td>7291.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48805.446578</td>\n",
       "      <td>1467.770111</td>\n",
       "      <td>5.081744</td>\n",
       "      <td>1.079409e-06</td>\n",
       "      <td>1.016515</td>\n",
       "      <td>4.576833e-03</td>\n",
       "      <td>-2.680000e+01</td>\n",
       "      <td>0.380826</td>\n",
       "      <td>1.964216</td>\n",
       "      <td>2.919045e+09</td>\n",
       "      <td>213.734629</td>\n",
       "      <td>12.106021</td>\n",
       "      <td>29.377040</td>\n",
       "      <td>29.520093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23176.436968</td>\n",
       "      <td>945.345932</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>9.396929e-08</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>4.726776e-08</td>\n",
       "      <td>7.105915e-15</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>2.343102e+09</td>\n",
       "      <td>55.887239</td>\n",
       "      <td>6.606467</td>\n",
       "      <td>17.426997</td>\n",
       "      <td>17.310827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9342.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.074145</td>\n",
       "      <td>9.098155e-07</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>4.576754e-03</td>\n",
       "      <td>-2.680000e+01</td>\n",
       "      <td>0.379629</td>\n",
       "      <td>1.956124</td>\n",
       "      <td>8.727296e+07</td>\n",
       "      <td>96.654022</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28787.000000</td>\n",
       "      <td>147.955000</td>\n",
       "      <td>5.077889</td>\n",
       "      <td>9.992815e-07</td>\n",
       "      <td>1.016505</td>\n",
       "      <td>4.576792e-03</td>\n",
       "      <td>-2.680000e+01</td>\n",
       "      <td>0.380206</td>\n",
       "      <td>1.960113</td>\n",
       "      <td>8.286914e+08</td>\n",
       "      <td>169.667321</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>47964.000000</td>\n",
       "      <td>2050.120000</td>\n",
       "      <td>5.081583</td>\n",
       "      <td>1.080938e-06</td>\n",
       "      <td>1.016516</td>\n",
       "      <td>4.576831e-03</td>\n",
       "      <td>-2.680000e+01</td>\n",
       "      <td>0.380855</td>\n",
       "      <td>1.964045</td>\n",
       "      <td>2.300545e+09</td>\n",
       "      <td>219.006849</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>69943.500000</td>\n",
       "      <td>2212.000000</td>\n",
       "      <td>5.085815</td>\n",
       "      <td>1.166179e-06</td>\n",
       "      <td>1.016524</td>\n",
       "      <td>4.576876e-03</td>\n",
       "      <td>-2.680000e+01</td>\n",
       "      <td>0.381415</td>\n",
       "      <td>1.968550</td>\n",
       "      <td>4.892093e+09</td>\n",
       "      <td>264.468334</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89306.000000</td>\n",
       "      <td>2456.160000</td>\n",
       "      <td>5.089544</td>\n",
       "      <td>1.233629e-06</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>4.576917e-03</td>\n",
       "      <td>-2.680000e+01</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>1.972516</td>\n",
       "      <td>7.975562e+09</td>\n",
       "      <td>298.841095</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sec           nv         hlon          hlat  earth_distance  \\\n",
       "count   7291.000000  7291.000000  7291.000000  7.291000e+03     7291.000000   \n",
       "mean   48805.446578  1467.770111     5.081744  1.079409e-06        1.016515   \n",
       "std    23176.436968   945.345932     0.004463  9.396929e-08        0.000010   \n",
       "min     9342.000000     0.000000     5.074145  9.098155e-07        1.016497   \n",
       "25%    28787.000000   147.955000     5.077889  9.992815e-07        1.016505   \n",
       "50%    47964.000000  2050.120000     5.081583  1.080938e-06        1.016516   \n",
       "75%    69943.500000  2212.000000     5.085815  1.166179e-06        1.016524   \n",
       "max    89306.000000  2456.160000     5.089544  1.233629e-06        1.016533   \n",
       "\n",
       "             radius           mag        a_dec         a_ra          sec2  \\\n",
       "count  7.291000e+03  7.291000e+03  7291.000000  7291.000000  7.291000e+03   \n",
       "mean   4.576833e-03 -2.680000e+01     0.380826     1.964216  2.919045e+09   \n",
       "std    4.726776e-08  7.105915e-15     0.000681     0.004751  2.343102e+09   \n",
       "min    4.576754e-03 -2.680000e+01     0.379629     1.956124  8.727296e+07   \n",
       "25%    4.576792e-03 -2.680000e+01     0.380206     1.960113  8.286914e+08   \n",
       "50%    4.576831e-03 -2.680000e+01     0.380855     1.964045  2.300545e+09   \n",
       "75%    4.576876e-03 -2.680000e+01     0.381415     1.968550  4.892093e+09   \n",
       "max    4.576917e-03 -2.680000e+01     0.381977     1.972516  7.975562e+09   \n",
       "\n",
       "              sec5         hour       minute       second  \n",
       "count  7291.000000  7291.000000  7291.000000  7291.000000  \n",
       "mean    213.734629    12.106021    29.377040    29.520093  \n",
       "std      55.887239     6.606467    17.426997    17.310827  \n",
       "min      96.654022    -1.000000     0.000000     0.000000  \n",
       "25%     169.667321     7.000000    14.000000    14.000000  \n",
       "50%     219.006849    12.000000    29.000000    30.000000  \n",
       "75%     264.468334    17.000000    44.000000    45.000000  \n",
       "max     298.841095    23.000000    59.000000    59.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f67c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_time_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc08b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e858a463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa04ec95ba0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7fe87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=\"nv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a2e4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec</th>\n",
       "      <th>hlon</th>\n",
       "      <th>hlat</th>\n",
       "      <th>earth_distance</th>\n",
       "      <th>radius</th>\n",
       "      <th>mag</th>\n",
       "      <th>a_dec</th>\n",
       "      <th>a_ra</th>\n",
       "      <th>sec2</th>\n",
       "      <th>sec5</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9342</td>\n",
       "      <td>5.074145</td>\n",
       "      <td>9.098155e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>1.956124</td>\n",
       "      <td>87272964</td>\n",
       "      <td>96.654022</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9353</td>\n",
       "      <td>5.074147</td>\n",
       "      <td>9.098679e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>1.956126</td>\n",
       "      <td>87478609</td>\n",
       "      <td>96.710909</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9363</td>\n",
       "      <td>5.074149</td>\n",
       "      <td>9.099156e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>1.956128</td>\n",
       "      <td>87665769</td>\n",
       "      <td>96.762596</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9374</td>\n",
       "      <td>5.074151</td>\n",
       "      <td>9.099681e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>1.956131</td>\n",
       "      <td>87871876</td>\n",
       "      <td>96.819420</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9385</td>\n",
       "      <td>5.074153</td>\n",
       "      <td>9.100205e-07</td>\n",
       "      <td>1.016533</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.381976</td>\n",
       "      <td>1.956133</td>\n",
       "      <td>88078225</td>\n",
       "      <td>96.876210</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7286</th>\n",
       "      <td>89265</td>\n",
       "      <td>5.089536</td>\n",
       "      <td>1.233494e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>1.972508</td>\n",
       "      <td>7968240225</td>\n",
       "      <td>298.772489</td>\n",
       "      <td>-1</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7287</th>\n",
       "      <td>89275</td>\n",
       "      <td>5.089538</td>\n",
       "      <td>1.233527e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>1.972510</td>\n",
       "      <td>7970025625</td>\n",
       "      <td>298.789223</td>\n",
       "      <td>-1</td>\n",
       "      <td>47</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7288</th>\n",
       "      <td>89285</td>\n",
       "      <td>5.089540</td>\n",
       "      <td>1.233560e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>1.972512</td>\n",
       "      <td>7971811225</td>\n",
       "      <td>298.805957</td>\n",
       "      <td>-1</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7289</th>\n",
       "      <td>89296</td>\n",
       "      <td>5.089542</td>\n",
       "      <td>1.233596e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379629</td>\n",
       "      <td>1.972514</td>\n",
       "      <td>7973775616</td>\n",
       "      <td>298.824363</td>\n",
       "      <td>-1</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7290</th>\n",
       "      <td>89306</td>\n",
       "      <td>5.089544</td>\n",
       "      <td>1.233629e-06</td>\n",
       "      <td>1.016497</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>-26.8</td>\n",
       "      <td>0.379629</td>\n",
       "      <td>1.972516</td>\n",
       "      <td>7975561636</td>\n",
       "      <td>298.841095</td>\n",
       "      <td>-1</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7291 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sec      hlon          hlat  earth_distance    radius   mag     a_dec  \\\n",
       "0      9342  5.074145  9.098155e-07        1.016533  0.004577 -26.8  0.381977   \n",
       "1      9353  5.074147  9.098679e-07        1.016533  0.004577 -26.8  0.381977   \n",
       "2      9363  5.074149  9.099156e-07        1.016533  0.004577 -26.8  0.381977   \n",
       "3      9374  5.074151  9.099681e-07        1.016533  0.004577 -26.8  0.381977   \n",
       "4      9385  5.074153  9.100205e-07        1.016533  0.004577 -26.8  0.381976   \n",
       "...     ...       ...           ...             ...       ...   ...       ...   \n",
       "7286  89265  5.089536  1.233494e-06        1.016497  0.004577 -26.8  0.379630   \n",
       "7287  89275  5.089538  1.233527e-06        1.016497  0.004577 -26.8  0.379630   \n",
       "7288  89285  5.089540  1.233560e-06        1.016497  0.004577 -26.8  0.379630   \n",
       "7289  89296  5.089542  1.233596e-06        1.016497  0.004577 -26.8  0.379629   \n",
       "7290  89306  5.089544  1.233629e-06        1.016497  0.004577 -26.8  0.379629   \n",
       "\n",
       "          a_ra        sec2        sec5  hour  minute  second  \n",
       "0     1.956124    87272964   96.654022     2      35      42  \n",
       "1     1.956126    87478609   96.710909     2      35      53  \n",
       "2     1.956128    87665769   96.762596     2      36       3  \n",
       "3     1.956131    87871876   96.819420     2      36      14  \n",
       "4     1.956133    88078225   96.876210     2      36      25  \n",
       "...        ...         ...         ...   ...     ...     ...  \n",
       "7286  1.972508  7968240225  298.772489    -1      47      45  \n",
       "7287  1.972510  7970025625  298.789223    -1      47      55  \n",
       "7288  1.972512  7971811225  298.805957    -1      48       5  \n",
       "7289  1.972514  7973775616  298.824363    -1      48      16  \n",
       "7290  1.972516  7975561636  298.841095    -1      48      26  \n",
       "\n",
       "[7291 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aec8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c58c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    np.array(list(range(-1, 24))),\n",
    "    np.array(list(range(60))),\n",
    "]\n",
    "ohe = OneHotEncoder(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c47f0aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(categories=[array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23]),\n",
       "                          array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(categories=[array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23]),\n",
       "                          array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59])])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(categories=[array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23]),\n",
       "                          array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59])])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit(data[[\"hour\", \"minute\"]],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d4b68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack(\n",
    "    [\n",
    "        data.drop(columns=\"nv\").to_numpy(),\n",
    "        ohe.fit_transform(data[[\"hour\", \"minute\"]]).todense(),\n",
    "    ]\n",
    ")\n",
    "y = data.nv\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "816da1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss_x = StandardScaler()\n",
    "# ss_y = StandardScaler()\n",
    "# X = ss_x.fit_transform(X)\n",
    "# y = ss_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49b18fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7ee4149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 09:02:13.947847: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination\n",
      "2023-01-31 09:02:13.947923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: laptop\n",
      "2023-01-31 09:02:13.947939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: laptop\n",
      "2023-01-31 09:02:13.948119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.108.3\n",
      "2023-01-31 09:02:13.948166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.78.1\n",
      "2023-01-31 09:02:13.948182: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 525.78.1 does not match DSO version 510.108.3 -- cannot find working devices in this configuration\n",
      "2023-01-31 09:02:13.948874: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dt_b = dataset_train.shuffle(buffer_size=1024).batch(batch_size=512)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "dv_b = dataset_val.batch(batch_size=256)\n",
    "metrics = pd.DataFrame(columns=[\"model_name\", \"RMSE\", \"MAPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61cb8f",
   "metadata": {},
   "source": [
    "Linear regression\n",
    "\n",
    "See elasticnet for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0126411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1cf1099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = list(map(lambda x: pow(10, x), range(-3,4)))\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c94a481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {f\"linear_regression_l1{c_l1}_l2{c_l2}\":{\n",
    "    \"model\":keras.models.Sequential([\n",
    "  keras.layers.Dense(\n",
    "      1,\n",
    "      kernel_regularizer=keras.regularizers.l1_l2(l1=c_l1, l2=c_l2))\n",
    "]),\n",
    "    \"c_l1\": c_l1,\n",
    "    \"c_l2\": c_l2,\n",
    "    \"history\": None\n",
    "}\n",
    "for c_l1 in C for c_l2 in C}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c381e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (98,)\n",
      "Target shape: ()\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74a8d34b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 61798208241664.00000, saving model to linear_regression/linear_regression_l10.001_l20.001.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 61798208241664.00000 to 85005279232.00000, saving model to linear_regression/linear_regression_l10.001_l20.001.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 85005279232.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 24674108964864.00000, saving model to linear_regression/linear_regression_l10.001_l20.01.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 24674108964864.00000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 24674108964864.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 24674108964864.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 24674108964864.00000 to 4005011390464.00000, saving model to linear_regression/linear_regression_l10.001_l20.01.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 4005011390464.00000 to 56769331200.00000, saving model to linear_regression/linear_regression_l10.001_l20.01.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 56769331200.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 138130300600320.00000, saving model to linear_regression/linear_regression_l10.001_l20.1.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 138130300600320.00000 to 103669325824.00000, saving model to linear_regression/linear_regression_l10.001_l20.1.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 103669325824.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 111095175970816.00000, saving model to linear_regression/linear_regression_l10.001_l21.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 111095175970816.00000 to 82350243315712.00000, saving model to linear_regression/linear_regression_l10.001_l21.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 82350243315712.00000 to 2182039207936.00000, saving model to linear_regression/linear_regression_l10.001_l21.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 2182039207936.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 2182039207936.00000 to 58304679936.00000, saving model to linear_regression/linear_regression_l10.001_l21.0.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 58304679936.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 58304679936.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 58304679936.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 58304679936.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 58304679936.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 58304679936.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 58304679936.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 58304679936.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 58304679936.00000 to 32585654272.00000, saving model to linear_regression/linear_regression_l10.001_l21.0.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 32585654272.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 32585654272.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 32585654272.00000\n",
      "\n",
      "Epoch 18: val_loss improved from 32585654272.00000 to 20993675264.00000, saving model to linear_regression/linear_regression_l10.001_l21.0.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 20993675264.00000 to 15248924672.00000, saving model to linear_regression/linear_regression_l10.001_l21.0.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 15248924672.00000 to 716414528.00000, saving model to linear_regression/linear_regression_l10.001_l21.0.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 716414528.00000 to 13476739.00000, saving model to linear_regression/linear_regression_l10.001_l21.0.h5\n",
      "\n",
      "Epoch 22: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 13476739.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 141335394779136.00000, saving model to linear_regression/linear_regression_l10.001_l210.0.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 141335394779136.00000\n",
      "\n",
      "Epoch 3: val_loss improved from 141335394779136.00000 to 108019358629888.00000, saving model to linear_regression/linear_regression_l10.001_l210.0.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 108019358629888.00000 to 9821734567936.00000, saving model to linear_regression/linear_regression_l10.001_l210.0.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 9821734567936.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 9821734567936.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 9821734567936.00000 to 25328345088.00000, saving model to linear_regression/linear_regression_l10.001_l210.0.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 25328345088.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 25328345088.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 25328345088.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 25328345088.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 25328345088.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 25328345088.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 25328345088.00000 to 18445682688.00000, saving model to linear_regression/linear_regression_l10.001_l210.0.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 18445682688.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2894566785024.00000, saving model to linear_regression/linear_regression_l10.001_l2100.0.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 2894566785024.00000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 2894566785024.00000\n",
      "\n",
      "Epoch 4: val_loss improved from 2894566785024.00000 to 577847099392.00000, saving model to linear_regression/linear_regression_l10.001_l2100.0.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 577847099392.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 577847099392.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 577847099392.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 577847099392.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 577847099392.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 577847099392.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 577847099392.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 577847099392.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: val_loss did not improve from 577847099392.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 577847099392.00000 to 249760923648.00000, saving model to linear_regression/linear_regression_l10.001_l2100.0.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 249760923648.00000 to 52338647040.00000, saving model to linear_regression/linear_regression_l10.001_l2100.0.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 52338647040.00000 to 48281120768.00000, saving model to linear_regression/linear_regression_l10.001_l2100.0.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 48281120768.00000 to 12718629888.00000, saving model to linear_regression/linear_regression_l10.001_l2100.0.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 12718629888.00000 to 2944103936.00000, saving model to linear_regression/linear_regression_l10.001_l2100.0.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 2944103936.00000 to 728054272.00000, saving model to linear_regression/linear_regression_l10.001_l2100.0.h5\n",
      "\n",
      "Epoch 20: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 728054272.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 143825704058880.00000, saving model to linear_regression/linear_regression_l10.001_l21000.0.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 143825704058880.00000\n",
      "\n",
      "Epoch 3: val_loss improved from 143825704058880.00000 to 101779341574144.00000, saving model to linear_regression/linear_regression_l10.001_l21000.0.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 101779341574144.00000 to 8036678107136.00000, saving model to linear_regression/linear_regression_l10.001_l21000.0.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 8036678107136.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 8036678107136.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 8036678107136.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 8036678107136.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 8036678107136.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 8036678107136.00000 to 171546378240.00000, saving model to linear_regression/linear_regression_l10.001_l21000.0.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 171546378240.00000 to 156660629504.00000, saving model to linear_regression/linear_regression_l10.001_l21000.0.h5\n",
      "\n",
      "Epoch 12: val_loss did not improve from 156660629504.00000\n",
      "\n",
      "Epoch 13: val_loss improved from 156660629504.00000 to 31565307904.00000, saving model to linear_regression/linear_regression_l10.001_l21000.0.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 31565307904.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 120453792268288.00000, saving model to linear_regression/linear_regression_l10.01_l20.001.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 120453792268288.00000 to 108575867273216.00000, saving model to linear_regression/linear_regression_l10.01_l20.001.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 108575867273216.00000 to 24569956007936.00000, saving model to linear_regression/linear_regression_l10.01_l20.001.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 24569956007936.00000 to 5082277478400.00000, saving model to linear_regression/linear_regression_l10.01_l20.001.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 5082277478400.00000 to 168649801728.00000, saving model to linear_regression/linear_regression_l10.01_l20.001.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 168649801728.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 168649801728.00000 to 12740689920.00000, saving model to linear_regression/linear_regression_l10.01_l20.001.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 12740689920.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 130294694805504.00000, saving model to linear_regression/linear_regression_l10.01_l20.01.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 130294694805504.00000\n",
      "\n",
      "Epoch 3: val_loss improved from 130294694805504.00000 to 125414001344512.00000, saving model to linear_regression/linear_regression_l10.01_l20.01.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 125414001344512.00000 to 54909554655232.00000, saving model to linear_regression/linear_regression_l10.01_l20.01.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 54909554655232.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 54909554655232.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 54909554655232.00000 to 13337502941184.00000, saving model to linear_regression/linear_regression_l10.01_l20.01.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 13337502941184.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 13337502941184.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 13337502941184.00000 to 1852555788288.00000, saving model to linear_regression/linear_regression_l10.01_l20.01.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1852555788288.00000 to 414020544.00000, saving model to linear_regression/linear_regression_l10.01_l20.01.h5\n",
      "\n",
      "Epoch 12: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 414020544.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 22457505284096.00000, saving model to linear_regression/linear_regression_l10.01_l20.1.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 22457505284096.00000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 22457505284096.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 22457505284096.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 22457505284096.00000 to 497154326528.00000, saving model to linear_regression/linear_regression_l10.01_l20.1.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 497154326528.00000 to 1831508.50000, saving model to linear_regression/linear_regression_l10.01_l20.1.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1831508.50000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 88178075178631168.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 88178075178631168.00000 to 4241819491631104.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 4241819491631104.00000\n",
      "\n",
      "Epoch 4: val_loss improved from 4241819491631104.00000 to 339298020753408.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss did not improve from 339298020753408.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 339298020753408.00000 to 159438480605184.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 159438480605184.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 159438480605184.00000 to 14634579918848.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n",
      "\n",
      "Epoch 9: val_loss did not improve from 14634579918848.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 14634579918848.00000 to 28126445568.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 28126445568.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 28126445568.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 28126445568.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 28126445568.00000\n",
      "\n",
      "Epoch 15: val_loss improved from 28126445568.00000 to 2047706880.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n",
      "\n",
      "Epoch 16: val_loss did not improve from 2047706880.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 2047706880.00000\n",
      "\n",
      "Epoch 18: val_loss improved from 2047706880.00000 to 376933696.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n",
      "\n",
      "Epoch 19: val_loss did not improve from 376933696.00000\n",
      "\n",
      "Epoch 20: val_loss improved from 376933696.00000 to 128709560.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 128709560.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 128709560.00000\n",
      "\n",
      "Epoch 23: val_loss improved from 128709560.00000 to 42843888.00000, saving model to linear_regression/linear_regression_l10.01_l21.0.h5\n",
      "\n",
      "Epoch 24: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 33: val_loss did not improve from 42843888.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 214709400856690688.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 214709400856690688.00000 to 3935625904390144.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 3935625904390144.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 3935625904390144.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 3935625904390144.00000 to 1044395065344.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1044395065344.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1044395065344.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1044395065344.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1044395065344.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1044395065344.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1044395065344.00000\n",
      "\n",
      "Epoch 12: val_loss improved from 1044395065344.00000 to 561739005952.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 13: val_loss did not improve from 561739005952.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 561739005952.00000 to 95086329856.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 95086329856.00000 to 87209320448.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 87209320448.00000 to 24394057728.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 24394057728.00000 to 6675703296.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 6675703296.00000 to 4380427776.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 4380427776.00000 to 28132718.00000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 20: val_loss did not improve from 28132718.00000\n",
      "\n",
      "Epoch 21: val_loss improved from 28132718.00000 to 3782999.25000, saving model to linear_regression/linear_regression_l10.01_l210.0.h5\n",
      "\n",
      "Epoch 22: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 3782999.25000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 29459970957246464.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 29459970957246464.00000 to 17135214409023488.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 17135214409023488.00000 to 5577242667122688.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 5577242667122688.00000 to 1919644491317248.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1919644491317248.00000 to 590891802689536.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 590891802689536.00000 to 396097285521408.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 396097285521408.00000 to 9821256417280.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 9821256417280.00000\n",
      "\n",
      "Epoch 9: val_loss improved from 9821256417280.00000 to 7431660765184.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 7431660765184.00000 to 1777387307008.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1777387307008.00000\n",
      "\n",
      "Epoch 12: val_loss improved from 1777387307008.00000 to 548577280000.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 548577280000.00000 to 3905725952.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 3905725952.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 3905725952.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 3905725952.00000\n",
      "\n",
      "Epoch 17: val_loss improved from 3905725952.00000 to 995147840.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 995147840.00000 to 554975744.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 554975744.00000 to 140023600.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 20: val_loss did not improve from 140023600.00000\n",
      "\n",
      "Epoch 21: val_loss improved from 140023600.00000 to 10824018.00000, saving model to linear_regression/linear_regression_l10.01_l2100.0.h5\n",
      "\n",
      "Epoch 22: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 10824018.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 941733185585152.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss did not improve from 941733185585152.00000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 941733185585152.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 941733185585152.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 941733185585152.00000 to 848148599668736.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 848148599668736.00000 to 570648494080.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 570648494080.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 570648494080.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 570648494080.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 570648494080.00000 to 221449846784.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 221449846784.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 221449846784.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 221449846784.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 221449846784.00000 to 146901565440.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 146901565440.00000 to 53368918016.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 53368918016.00000 to 21980323840.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 21980323840.00000 to 10509785088.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 10509785088.00000 to 5675286016.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 5675286016.00000 to 3275685888.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 3275685888.00000 to 1642807936.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1642807936.00000 to 592861120.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 592861120.00000 to 17657528.00000, saving model to linear_regression/linear_regression_l10.01_l21000.0.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 17657528.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 60093655678976.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 60093655678976.00000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 60093655678976.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 60093655678976.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 60093655678976.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 60093655678976.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 60093655678976.00000 to 27961858195456.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 27961858195456.00000\n",
      "\n",
      "Epoch 9: val_loss improved from 27961858195456.00000 to 17359411609600.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 17359411609600.00000 to 2434179268608.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 2434179268608.00000 to 37081337856.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 12: val_loss did not improve from 37081337856.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 37081337856.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 37081337856.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 37081337856.00000\n",
      "\n",
      "Epoch 16: val_loss improved from 37081337856.00000 to 36622221312.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 36622221312.00000 to 24717148160.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 24717148160.00000 to 10556904448.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 10556904448.00000 to 4831457280.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 4831457280.00000 to 262856096.00000, saving model to linear_regression/linear_regression_l10.1_l20.001.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 262856096.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 12998679724032.00000, saving model to linear_regression/linear_regression_l10.1_l20.01.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 12998679724032.00000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 12998679724032.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 12998679724032.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 12998679724032.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 12998679724032.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 12998679724032.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 12998679724032.00000 to 4382230315008.00000, saving model to linear_regression/linear_regression_l10.1_l20.01.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 4382230315008.00000 to 63389872128.00000, saving model to linear_regression/linear_regression_l10.1_l20.01.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 63389872128.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 63389872128.00000\n",
      "\n",
      "Epoch 12: val_loss improved from 63389872128.00000 to 8275863552.00000, saving model to linear_regression/linear_regression_l10.1_l20.01.h5\n",
      "\n",
      "Epoch 13: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 8275863552.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 359649119574163456.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 359649119574163456.00000 to 32897252611588096.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 32897252611588096.00000 to 5681545411035136.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 5681545411035136.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 5681545411035136.00000 to 2889564339306496.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 2889564339306496.00000 to 205259574083584.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 205259574083584.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 205259574083584.00000 to 59426488713216.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 59426488713216.00000 to 46128296886272.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss improved from 46128296886272.00000 to 27390935826432.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 27390935826432.00000 to 666790723584.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 12: val_loss did not improve from 666790723584.00000\n",
      "\n",
      "Epoch 13: val_loss improved from 666790723584.00000 to 47931375616.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 47931375616.00000\n",
      "\n",
      "Epoch 15: val_loss improved from 47931375616.00000 to 21062180864.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 16: val_loss did not improve from 21062180864.00000\n",
      "\n",
      "Epoch 17: val_loss improved from 21062180864.00000 to 9997078528.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 9997078528.00000 to 3113232384.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 3113232384.00000 to 22233976.00000, saving model to linear_regression/linear_regression_l10.1_l20.1.h5\n",
      "\n",
      "Epoch 20: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 22233976.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 65929710793654272.00000, saving model to linear_regression/linear_regression_l10.1_l21.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 65929710793654272.00000 to 8118389058830336.00000, saving model to linear_regression/linear_regression_l10.1_l21.0.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 8118389058830336.00000\n",
      "\n",
      "Epoch 4: val_loss improved from 8118389058830336.00000 to 24445183852544.00000, saving model to linear_regression/linear_regression_l10.1_l21.0.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 24445183852544.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 24445183852544.00000 to 7923710976.00000, saving model to linear_regression/linear_regression_l10.1_l21.0.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 7923710976.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 7923710976.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 7923710976.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 7923710976.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 7923710976.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 7923710976.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 7923710976.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 7923710976.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 7923710976.00000\n",
      "\n",
      "Epoch 16: val_loss improved from 7923710976.00000 to 4255563520.00000, saving model to linear_regression/linear_regression_l10.1_l21.0.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 4255563520.00000 to 962146880.00000, saving model to linear_regression/linear_regression_l10.1_l21.0.h5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 962146880.00000\n",
      "\n",
      "Epoch 19: val_loss improved from 962146880.00000 to 440237344.00000, saving model to linear_regression/linear_regression_l10.1_l21.0.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 440237344.00000 to 152386992.00000, saving model to linear_regression/linear_regression_l10.1_l21.0.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 152386992.00000\n",
      "\n",
      "Epoch 22: val_loss improved from 152386992.00000 to 2501641.00000, saving model to linear_regression/linear_regression_l10.1_l21.0.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 2501641.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 659595307515904000.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 659595307515904000.00000 to 144682725054349312.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 144682725054349312.00000 to 3641305217368064.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 3641305217368064.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 3641305217368064.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 3641305217368064.00000 to 1864212703870976.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1864212703870976.00000 to 33334193291264.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 33334193291264.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 33334193291264.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 33334193291264.00000 to 995189587968.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 995189587968.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 995189587968.00000\n",
      "\n",
      "Epoch 13: val_loss improved from 995189587968.00000 to 440021286912.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 440021286912.00000\n",
      "\n",
      "Epoch 15: val_loss improved from 440021286912.00000 to 27029194752.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 16: val_loss did not improve from 27029194752.00000\n",
      "\n",
      "Epoch 17: val_loss improved from 27029194752.00000 to 25220067328.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 25220067328.00000 to 2433773312.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 19: val_loss did not improve from 2433773312.00000\n",
      "\n",
      "Epoch 20: val_loss improved from 2433773312.00000 to 4873653.00000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 4873653.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 4873653.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 4873653.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 4873653.00000\n",
      "\n",
      "Epoch 25: val_loss improved from 4873653.00000 to 3294196.50000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 3294196.50000 to 1185168.37500, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1185168.37500\n",
      "\n",
      "Epoch 28: val_loss improved from 1185168.37500 to 1138669.50000, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 1138669.50000 to 216373.48438, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 216373.48438 to 212241.62500, saving model to linear_regression/linear_regression_l10.1_l210.0.h5\n",
      "\n",
      "Epoch 31: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 32: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 33: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 34: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 35: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 36: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 37: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 38: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 39: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 40: val_loss did not improve from 212241.62500\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 766389085215391744.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 766389085215391744.00000 to 194589420400148480.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 194589420400148480.00000 to 11841076658700288.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 11841076658700288.00000 to 4683127112859648.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 4683127112859648.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 4683127112859648.00000 to 3347279004041216.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 3347279004041216.00000 to 49435664973824.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 49435664973824.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 49435664973824.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 49435664973824.00000 to 9133885489152.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 9133885489152.00000\n",
      "\n",
      "Epoch 12: val_loss improved from 9133885489152.00000 to 8278690496512.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 8278690496512.00000 to 170377035776.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 170377035776.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 170377035776.00000\n",
      "\n",
      "Epoch 16: val_loss improved from 170377035776.00000 to 772101440.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 17: val_loss did not improve from 772101440.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 772101440.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 772101440.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 772101440.00000\n",
      "\n",
      "Epoch 21: val_loss improved from 772101440.00000 to 197709104.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 197709104.00000 to 15864507.00000, saving model to linear_regression/linear_regression_l10.1_l2100.0.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 15864507.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 125255883599904768.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 125255883599904768.00000 to 630504822931456.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 630504822931456.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 630504822931456.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 630504822931456.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 630504822931456.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 630504822931456.00000 to 81399713366016.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 81399713366016.00000\n",
      "\n",
      "Epoch 9: val_loss improved from 81399713366016.00000 to 9757379264512.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 9757379264512.00000\n",
      "\n",
      "Epoch 11: val_loss improved from 9757379264512.00000 to 2384062316544.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 2384062316544.00000 to 925263986688.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 925263986688.00000 to 586530750464.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 586530750464.00000 to 10905004032.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 10905004032.00000\n",
      "\n",
      "Epoch 16: val_loss improved from 10905004032.00000 to 5722653184.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 5722653184.00000 to 4587374080.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 4587374080.00000 to 4362243584.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 4362243584.00000 to 10653459.00000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 20: val_loss did not improve from 10653459.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 10653459.00000\n",
      "\n",
      "Epoch 22: val_loss improved from 10653459.00000 to 4847811.50000, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 4847811.50000 to 1840631.12500, saving model to linear_regression/linear_regression_l10.1_l21000.0.h5\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1840631.12500\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 50538628513792.00000, saving model to linear_regression/linear_regression_l11.0_l20.001.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 50538628513792.00000\n",
      "\n",
      "Epoch 3: val_loss improved from 50538628513792.00000 to 2966498304.00000, saving model to linear_regression/linear_regression_l11.0_l20.001.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 2966498304.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 122976489963520.00000, saving model to linear_regression/linear_regression_l11.0_l20.01.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 122976489963520.00000 to 8769001472.00000, saving model to linear_regression/linear_regression_l11.0_l20.01.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 8769001472.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 8769001472.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 8769001472.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 8769001472.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 8769001472.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 8769001472.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 8769001472.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 8769001472.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 8769001472.00000\n",
      "\n",
      "Epoch 12: val_loss improved from 8769001472.00000 to 2780285440.00000, saving model to linear_regression/linear_regression_l11.0_l20.01.h5\n",
      "\n",
      "Epoch 13: val_loss did not improve from 2780285440.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 2780285440.00000 to 1446579840.00000, saving model to linear_regression/linear_regression_l11.0_l20.01.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1446579840.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1446579840.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1446579840.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1446579840.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1446579840.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1446579840.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1446579840.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1446579840.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: val_loss did not improve from 1446579840.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1446579840.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1292663957487616.00000, saving model to linear_regression/linear_regression_l11.0_l20.1.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1292663957487616.00000 to 61170396430336.00000, saving model to linear_regression/linear_regression_l11.0_l20.1.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 61170396430336.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 61170396430336.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 61170396430336.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 61170396430336.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 61170396430336.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 61170396430336.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 61170396430336.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 61170396430336.00000\n",
      "\n",
      "Epoch 11: val_loss improved from 61170396430336.00000 to 8011734056960.00000, saving model to linear_regression/linear_regression_l11.0_l20.1.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 8011734056960.00000 to 2250577543168.00000, saving model to linear_regression/linear_regression_l11.0_l20.1.h5\n",
      "\n",
      "Epoch 13: val_loss did not improve from 2250577543168.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 2250577543168.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 2250577543168.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 2250577543168.00000\n",
      "\n",
      "Epoch 17: val_loss improved from 2250577543168.00000 to 1308655091712.00000, saving model to linear_regression/linear_regression_l11.0_l20.1.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1308655091712.00000 to 574714871808.00000, saving model to linear_regression/linear_regression_l11.0_l20.1.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 574714871808.00000 to 157645275136.00000, saving model to linear_regression/linear_regression_l11.0_l20.1.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 157645275136.00000 to 309785408.00000, saving model to linear_regression/linear_regression_l11.0_l20.1.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 309785408.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 332134493924098048.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 332134493924098048.00000 to 25858926494875648.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 25858926494875648.00000 to 7511658893148160.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 7511658893148160.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 7511658893148160.00000 to 2022425071976448.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 2022425071976448.00000 to 405922459418624.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 405922459418624.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 405922459418624.00000 to 20532017758208.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 9: val_loss did not improve from 20532017758208.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 20532017758208.00000 to 18533960384512.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 18533960384512.00000 to 2834337628160.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 12: val_loss did not improve from 2834337628160.00000\n",
      "\n",
      "Epoch 13: val_loss improved from 2834337628160.00000 to 48904486912.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 48904486912.00000\n",
      "\n",
      "Epoch 15: val_loss improved from 48904486912.00000 to 340384960.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 16: val_loss did not improve from 340384960.00000\n",
      "\n",
      "Epoch 17: val_loss improved from 340384960.00000 to 309216512.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 309216512.00000\n",
      "\n",
      "Epoch 19: val_loss improved from 309216512.00000 to 176843632.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 20: val_loss did not improve from 176843632.00000\n",
      "\n",
      "Epoch 21: val_loss improved from 176843632.00000 to 55781552.00000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 55781552.00000 to 4454885.50000, saving model to linear_regression/linear_regression_l11.0_l21.0.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 4454885.50000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 502483793215488.00000, saving model to linear_regression/linear_regression_l11.0_l210.0.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 502483793215488.00000\n",
      "\n",
      "Epoch 3: val_loss improved from 502483793215488.00000 to 251740163145728.00000, saving model to linear_regression/linear_regression_l11.0_l210.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 251740163145728.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 251740163145728.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 251740163145728.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 251740163145728.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 251740163145728.00000 to 57255751843840.00000, saving model to linear_regression/linear_regression_l11.0_l210.0.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 57255751843840.00000 to 751036399616.00000, saving model to linear_regression/linear_regression_l11.0_l210.0.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 751036399616.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 751036399616.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 751036399616.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 751036399616.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 751036399616.00000\n",
      "\n",
      "Epoch 15: val_loss improved from 751036399616.00000 to 312279990272.00000, saving model to linear_regression/linear_regression_l11.0_l210.0.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 312279990272.00000 to 38576209920.00000, saving model to linear_regression/linear_regression_l11.0_l210.0.h5\n",
      "\n",
      "Epoch 17: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 38576209920.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 461185926022823936.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 461185926022823936.00000 to 65125160339898368.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 65125160339898368.00000 to 889378473771008.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 889378473771008.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 889378473771008.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss improved from 889378473771008.00000 to 44272841654272.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 44272841654272.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 44272841654272.00000\n",
      "\n",
      "Epoch 9: val_loss improved from 44272841654272.00000 to 685255229440.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 685255229440.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 685255229440.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 685255229440.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 685255229440.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 685255229440.00000 to 12512182272.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 12512182272.00000\n",
      "\n",
      "Epoch 16: val_loss improved from 12512182272.00000 to 2740079360.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 17: val_loss did not improve from 2740079360.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 2740079360.00000\n",
      "\n",
      "Epoch 19: val_loss improved from 2740079360.00000 to 1580411136.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1580411136.00000 to 82061536.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 82061536.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 82061536.00000\n",
      "\n",
      "Epoch 23: val_loss improved from 82061536.00000 to 43251836.00000, saving model to linear_regression/linear_regression_l11.0_l2100.0.h5\n",
      "\n",
      "Epoch 24: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 33: val_loss did not improve from 43251836.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 55510971711488.00000, saving model to linear_regression/linear_regression_l11.0_l21000.0.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 55510971711488.00000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 55510971711488.00000\n",
      "\n",
      "Epoch 4: val_loss improved from 55510971711488.00000 to 383625494528.00000, saving model to linear_regression/linear_regression_l11.0_l21000.0.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 383625494528.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 383625494528.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 383625494528.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 383625494528.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 383625494528.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 383625494528.00000 to 444356736.00000, saving model to linear_regression/linear_regression_l11.0_l21000.0.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 444356736.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 203904603169751040.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 203904603169751040.00000 to 2802098672500736.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 2802098672500736.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 2802098672500736.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 2802098672500736.00000 to 13315013083136.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 13315013083136.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 13315013083136.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 13315013083136.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 13315013083136.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 13315013083136.00000 to 9068522504192.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 9068522504192.00000 to 5522905890816.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 5522905890816.00000 to 994499297280.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 994499297280.00000 to 614758481920.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 614758481920.00000 to 183834558464.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 183834558464.00000 to 57760997376.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 57760997376.00000 to 29461725184.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 29461725184.00000 to 923632640.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 923632640.00000\n",
      "\n",
      "Epoch 19: val_loss improved from 923632640.00000 to 36777972.00000, saving model to linear_regression/linear_regression_l110.0_l20.001.h5\n",
      "\n",
      "Epoch 20: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 36777972.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 51027157824045056.00000, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 51027157824045056.00000 to 11532483258482688.00000, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 11532483258482688.00000\n",
      "\n",
      "Epoch 4: val_loss improved from 11532483258482688.00000 to 422450667978752.00000, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 422450667978752.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 422450667978752.00000 to 82449354719232.00000, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 82449354719232.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 82449354719232.00000 to 39413404925952.00000, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 39413404925952.00000 to 7859983089664.00000, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 7859983089664.00000\n",
      "\n",
      "Epoch 11: val_loss improved from 7859983089664.00000 to 252594388992.00000, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n",
      "\n",
      "Epoch 12: val_loss did not improve from 252594388992.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 252594388992.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 252594388992.00000 to 19962193920.00000, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 19962193920.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 19962193920.00000\n",
      "\n",
      "Epoch 17: val_loss improved from 19962193920.00000 to 5544737280.00000, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 5544737280.00000 to 1812102.37500, saving model to linear_regression/linear_regression_l110.0_l20.01.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1812102.37500\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 7062721396736.00000, saving model to linear_regression/linear_regression_l110.0_l20.1.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 7062721396736.00000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 7062721396736.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 7062721396736.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 7062721396736.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 7062721396736.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 7062721396736.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 7062721396736.00000\n",
      "\n",
      "Epoch 9: val_loss improved from 7062721396736.00000 to 1270178512896.00000, saving model to linear_regression/linear_regression_l110.0_l20.1.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1270178512896.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1270178512896.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1270178512896.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1270178512896.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 1270178512896.00000 to 495418474496.00000, saving model to linear_regression/linear_regression_l110.0_l20.1.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 495418474496.00000 to 16409870336.00000, saving model to linear_regression/linear_regression_l110.0_l20.1.h5\n",
      "\n",
      "Epoch 16: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 16409870336.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4835559797161984.00000, saving model to linear_regression/linear_regression_l110.0_l21.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 4835559797161984.00000 to 711269065687040.00000, saving model to linear_regression/linear_regression_l110.0_l21.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 711269065687040.00000 to 559616924581888.00000, saving model to linear_regression/linear_regression_l110.0_l21.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 559616924581888.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 559616924581888.00000 to 302726072238080.00000, saving model to linear_regression/linear_regression_l110.0_l21.0.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 302726072238080.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 302726072238080.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 302726072238080.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 302726072238080.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 302726072238080.00000\n",
      "\n",
      "Epoch 11: val_loss improved from 302726072238080.00000 to 37401711869952.00000, saving model to linear_regression/linear_regression_l110.0_l21.0.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 37401711869952.00000 to 509640376320.00000, saving model to linear_regression/linear_regression_l110.0_l21.0.h5\n",
      "\n",
      "Epoch 13: val_loss did not improve from 509640376320.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 509640376320.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 509640376320.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 509640376320.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 509640376320.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 509640376320.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 509640376320.00000\n",
      "\n",
      "Epoch 20: val_loss improved from 509640376320.00000 to 184784535552.00000, saving model to linear_regression/linear_regression_l110.0_l21.0.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 184784535552.00000 to 2170217728.00000, saving model to linear_regression/linear_regression_l110.0_l21.0.h5\n",
      "\n",
      "Epoch 22: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 2170217728.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 629503117372686336.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 629503117372686336.00000 to 131554470319357952.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 131554470319357952.00000 to 2271884691374080.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 2271884691374080.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 2271884691374080.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 2271884691374080.00000 to 1508695980638208.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1508695980638208.00000 to 75667928514560.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 75667928514560.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 75667928514560.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 75667928514560.00000 to 3615282954240.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 3615282954240.00000\n",
      "\n",
      "Epoch 12: val_loss improved from 3615282954240.00000 to 2340332765184.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 2340332765184.00000 to 750125318144.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 750125318144.00000\n",
      "\n",
      "Epoch 15: val_loss improved from 750125318144.00000 to 3584833280.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 16: val_loss did not improve from 3584833280.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 3584833280.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 3584833280.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 3584833280.00000\n",
      "\n",
      "Epoch 20: val_loss improved from 3584833280.00000 to 68634232.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 68634232.00000\n",
      "\n",
      "Epoch 22: val_loss improved from 68634232.00000 to 220641.00000, saving model to linear_regression/linear_regression_l110.0_l210.0.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 220641.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1234954092544000.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1234954092544000.00000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1234954092544000.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1234954092544000.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 1234954092544000.00000 to 877270893854720.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 877270893854720.00000 to 342741343010816.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss did not improve from 342741343010816.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 342741343010816.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 342741343010816.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 342741343010816.00000 to 160261302386688.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 160261302386688.00000 to 3955747979264.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 12: val_loss did not improve from 3955747979264.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 3955747979264.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 3955747979264.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 3955747979264.00000\n",
      "\n",
      "Epoch 16: val_loss improved from 3955747979264.00000 to 3944245362688.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 3944245362688.00000 to 2072115412992.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 2072115412992.00000 to 1225102458880.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1225102458880.00000 to 737493843968.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 737493843968.00000 to 416389726208.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 416389726208.00000 to 174788788224.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 174788788224.00000 to 31111424000.00000, saving model to linear_regression/linear_regression_l110.0_l2100.0.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 31111424000.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 112841538328854528.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 112841538328854528.00000 to 1433370709983232.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1433370709983232.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1433370709983232.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1433370709983232.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 1433370709983232.00000 to 514486817521664.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 514486817521664.00000 to 144002770796544.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 144002770796544.00000 to 74694640271360.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 74694640271360.00000 to 21272079630336.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 21272079630336.00000 to 6662207307776.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 6662207307776.00000 to 4269166821376.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 4269166821376.00000 to 201492709376.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 13: val_loss did not improve from 201492709376.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 201492709376.00000 to 18307303424.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 18307303424.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 18307303424.00000\n",
      "\n",
      "Epoch 17: val_loss improved from 18307303424.00000 to 76411856.00000, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 76411856.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 76411856.00000\n",
      "\n",
      "Epoch 20: val_loss improved from 76411856.00000 to 414499.59375, saving model to linear_regression/linear_regression_l110.0_l21000.0.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 22: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 23: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 24: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 25: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 26: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 27: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 28: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 29: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 30: val_loss did not improve from 414499.59375\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 716318150320193536.00000, saving model to linear_regression/linear_regression_l1100.0_l20.001.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 716318150320193536.00000 to 170955881817571328.00000, saving model to linear_regression/linear_regression_l1100.0_l20.001.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 170955881817571328.00000 to 7454848823853056.00000, saving model to linear_regression/linear_regression_l1100.0_l20.001.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 7454848823853056.00000 to 6416149638021120.00000, saving model to linear_regression/linear_regression_l1100.0_l20.001.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 6416149638021120.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 6416149638021120.00000 to 2659457439891456.00000, saving model to linear_regression/linear_regression_l1100.0_l20.001.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 2659457439891456.00000 to 796366602240.00000, saving model to linear_regression/linear_regression_l1100.0_l20.001.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 796366602240.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 796366602240.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 796366602240.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 796366602240.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 796366602240.00000\n",
      "\n",
      "Epoch 13: val_loss improved from 796366602240.00000 to 12997603328.00000, saving model to linear_regression/linear_regression_l1100.0_l20.001.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 12997603328.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 12997603328.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 12997603328.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 12997603328.00000\n",
      "\n",
      "Epoch 18: val_loss improved from 12997603328.00000 to 48429832.00000, saving model to linear_regression/linear_regression_l1100.0_l20.001.h5\n",
      "\n",
      "Epoch 19: val_loss did not improve from 48429832.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 48429832.00000\n",
      "\n",
      "Epoch 21: val_loss improved from 48429832.00000 to 19088182.00000, saving model to linear_regression/linear_regression_l1100.0_l20.001.h5\n",
      "\n",
      "Epoch 22: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 19088182.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 160547509612052480.00000, saving model to linear_regression/linear_regression_l1100.0_l20.01.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 160547509612052480.00000 to 136232268988416.00000, saving model to linear_regression/linear_regression_l1100.0_l20.01.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 136232268988416.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 136232268988416.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 136232268988416.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss did not improve from 136232268988416.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 136232268988416.00000 to 25589358592.00000, saving model to linear_regression/linear_regression_l1100.0_l20.01.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 25589358592.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 25589358592.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 25589358592.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 25589358592.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 25589358592.00000\n",
      "\n",
      "Epoch 13: val_loss improved from 25589358592.00000 to 11016772608.00000, saving model to linear_regression/linear_regression_l1100.0_l20.01.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 11016772608.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 11016772608.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 11016772608.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 11016772608.00000\n",
      "\n",
      "Epoch 18: val_loss improved from 11016772608.00000 to 653716608.00000, saving model to linear_regression/linear_regression_l1100.0_l20.01.h5\n",
      "\n",
      "Epoch 19: val_loss did not improve from 653716608.00000\n",
      "\n",
      "Epoch 20: val_loss improved from 653716608.00000 to 616326144.00000, saving model to linear_regression/linear_regression_l1100.0_l20.01.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 616326144.00000 to 368076352.00000, saving model to linear_regression/linear_regression_l1100.0_l20.01.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 368076352.00000 to 9065384.00000, saving model to linear_regression/linear_regression_l1100.0_l20.01.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 9065384.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 309207477461712896.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 309207477461712896.00000 to 20459514801160192.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 20459514801160192.00000 to 9315770106380288.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 9315770106380288.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 9315770106380288.00000 to 1402872415649792.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1402872415649792.00000 to 603753753346048.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 603753753346048.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 603753753346048.00000 to 2121805070336.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 9: val_loss did not improve from 2121805070336.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 2121805070336.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 2121805070336.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 2121805070336.00000\n",
      "\n",
      "Epoch 13: val_loss improved from 2121805070336.00000 to 351909773312.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 351909773312.00000\n",
      "\n",
      "Epoch 15: val_loss improved from 351909773312.00000 to 29393086464.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 16: val_loss did not improve from 29393086464.00000\n",
      "\n",
      "Epoch 17: val_loss improved from 29393086464.00000 to 3439520000.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 3439520000.00000\n",
      "\n",
      "Epoch 19: val_loss improved from 3439520000.00000 to 980159872.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 980159872.00000 to 746345408.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 746345408.00000 to 114286912.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 114286912.00000 to 10284011.00000, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 10284011.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 10284011.00000\n",
      "\n",
      "Epoch 25: val_loss improved from 10284011.00000 to 871534.81250, saving model to linear_regression/linear_regression_l1100.0_l20.1.h5\n",
      "\n",
      "Epoch 26: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 27: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 28: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 29: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 30: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 31: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 32: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 33: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 34: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 35: val_loss did not improve from 871534.81250\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2299706952646656.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 2299706952646656.00000\n",
      "\n",
      "Epoch 3: val_loss improved from 2299706952646656.00000 to 502707030851584.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 502707030851584.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 502707030851584.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 502707030851584.00000 to 29288082440192.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 29288082440192.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 29288082440192.00000\n",
      "\n",
      "Epoch 9: val_loss improved from 29288082440192.00000 to 369562812416.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 369562812416.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 369562812416.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 369562812416.00000\n",
      "\n",
      "Epoch 13: val_loss improved from 369562812416.00000 to 243290013696.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 243290013696.00000 to 56691634176.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 56691634176.00000 to 5001280512.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 5001280512.00000 to 1781957760.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1781957760.00000 to 81652824.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 81652824.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 81652824.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 81652824.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 81652824.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 81652824.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 81652824.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 81652824.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 81652824.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 81652824.00000\n",
      "\n",
      "Epoch 27: val_loss improved from 81652824.00000 to 21524718.00000, saving model to linear_regression/linear_regression_l1100.0_l21.0.h5\n",
      "\n",
      "Epoch 28: val_loss did not improve from 21524718.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 21524718.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 21524718.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 21524718.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 21524718.00000\n",
      "\n",
      "Epoch 33: val_loss did not improve from 21524718.00000\n",
      "\n",
      "Epoch 34: val_loss did not improve from 21524718.00000\n",
      "\n",
      "Epoch 35: val_loss did not improve from 21524718.00000\n",
      "\n",
      "Epoch 36: val_loss did not improve from 21524718.00000\n",
      "\n",
      "Epoch 37: val_loss did not improve from 21524718.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 803889784706564096.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 803889784706564096.00000 to 213194531531653120.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 213194531531653120.00000 to 15682491113275392.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 15682491113275392.00000 to 3560481985921024.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 3560481985921024.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 3560481985921024.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 3560481985921024.00000 to 125144131436544.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 125144131436544.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 125144131436544.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 125144131436544.00000 to 17947869315072.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 17947869315072.00000 to 6380137742336.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 12: val_loss did not improve from 6380137742336.00000\n",
      "\n",
      "Epoch 13: val_loss improved from 6380137742336.00000 to 602858323968.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 602858323968.00000 to 478043799552.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 478043799552.00000 to 414697521152.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 414697521152.00000 to 5919839232.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 17: val_loss did not improve from 5919839232.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 5919839232.00000\n",
      "\n",
      "Epoch 19: val_loss improved from 5919839232.00000 to 2804188160.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 20: val_loss did not improve from 2804188160.00000\n",
      "\n",
      "Epoch 21: val_loss improved from 2804188160.00000 to 853086144.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 853086144.00000 to 81625152.00000, saving model to linear_regression/linear_regression_l1100.0_l210.0.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 81625152.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 562998366292672512.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 562998366292672512.00000 to 102884283718303744.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 102884283718303744.00000 to 253849092751360.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 253849092751360.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 253849092751360.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 253849092751360.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 253849092751360.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 253849092751360.00000\n",
      "\n",
      "Epoch 9: val_loss improved from 253849092751360.00000 to 31809479901184.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 31809479901184.00000 to 18004374978560.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 18004374978560.00000 to 16850107760640.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 16850107760640.00000 to 100453384192.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 13: val_loss did not improve from 100453384192.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 100453384192.00000\n",
      "\n",
      "Epoch 15: val_loss improved from 100453384192.00000 to 58934235136.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 16: val_loss did not improve from 58934235136.00000\n",
      "\n",
      "Epoch 17: val_loss improved from 58934235136.00000 to 888705728.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 888705728.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 888705728.00000\n",
      "\n",
      "Epoch 20: val_loss improved from 888705728.00000 to 211267904.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 211267904.00000 to 70504640.00000, saving model to linear_regression/linear_regression_l1100.0_l2100.0.h5\n",
      "\n",
      "Epoch 22: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 70504640.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 7098405729861632.00000, saving model to linear_regression/linear_regression_l1100.0_l21000.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 7098405729861632.00000 to 411982658273280.00000, saving model to linear_regression/linear_regression_l1100.0_l21000.0.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 411982658273280.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 411982658273280.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 411982658273280.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 411982658273280.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 411982658273280.00000 to 81682266849280.00000, saving model to linear_regression/linear_regression_l1100.0_l21000.0.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 81682266849280.00000 to 2233576718336.00000, saving model to linear_regression/linear_regression_l1100.0_l21000.0.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 2233576718336.00000 to 888422989824.00000, saving model to linear_regression/linear_regression_l1100.0_l21000.0.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 888422989824.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 888422989824.00000\n",
      "\n",
      "Epoch 12: val_loss improved from 888422989824.00000 to 366594621440.00000, saving model to linear_regression/linear_regression_l1100.0_l21000.0.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 366594621440.00000 to 8592657408.00000, saving model to linear_regression/linear_regression_l1100.0_l21000.0.h5\n",
      "\n",
      "Epoch 14: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 8592657408.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 871620394614784.00000, saving model to linear_regression/linear_regression_l11000.0_l20.001.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 871620394614784.00000 to 2609656102912.00000, saving model to linear_regression/linear_regression_l11000.0_l20.001.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 2609656102912.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 2609656102912.00000 to 247947198464.00000, saving model to linear_regression/linear_regression_l11000.0_l20.001.h5\n",
      "\n",
      "Epoch 5: val_loss did not improve from 247947198464.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 247947198464.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 247947198464.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 247947198464.00000 to 1389740928.00000, saving model to linear_regression/linear_regression_l11000.0_l20.001.h5\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1389740928.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1652164665540608.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1652164665540608.00000\n",
      "\n",
      "Epoch 3: val_loss improved from 1652164665540608.00000 to 795783418871808.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 795783418871808.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 795783418871808.00000 to 716142779826176.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 716142779826176.00000 to 11594791649280.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 11594791649280.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 11594791649280.00000\n",
      "\n",
      "Epoch 9: val_loss improved from 11594791649280.00000 to 1585851924480.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1585851924480.00000 to 1110811082752.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1110811082752.00000\n",
      "\n",
      "Epoch 12: val_loss improved from 1110811082752.00000 to 991301337088.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 991301337088.00000 to 315455733760.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 315455733760.00000 to 97036894208.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 97036894208.00000 to 21040850944.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 21040850944.00000 to 8806433792.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 8806433792.00000 to 1702091904.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1702091904.00000\n",
      "\n",
      "Epoch 19: val_loss improved from 1702091904.00000 to 246520080.00000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 20: val_loss did not improve from 246520080.00000\n",
      "\n",
      "Epoch 21: val_loss improved from 246520080.00000 to 8156073.50000, saving model to linear_regression/linear_regression_l11000.0_l20.01.h5\n",
      "\n",
      "Epoch 22: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 8156073.50000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 174640399908864.00000, saving model to linear_regression/linear_regression_l11000.0_l20.1.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 174640399908864.00000 to 7290866368512.00000, saving model to linear_regression/linear_regression_l11000.0_l20.1.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 7290866368512.00000 to 243230261248.00000, saving model to linear_regression/linear_regression_l11000.0_l20.1.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 243230261248.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 243230261248.00000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 243230261248.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 243230261248.00000 to 108254846976.00000, saving model to linear_regression/linear_regression_l11000.0_l20.1.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 108254846976.00000\n",
      "\n",
      "Epoch 9: val_loss improved from 108254846976.00000 to 102478389248.00000, saving model to linear_regression/linear_regression_l11000.0_l20.1.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 102478389248.00000 to 3389608192.00000, saving model to linear_regression/linear_regression_l11000.0_l20.1.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 3389608192.00000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 5296577828618240.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 5296577828618240.00000 to 5132942258995200.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 5132942258995200.00000\n",
      "\n",
      "Epoch 4: val_loss improved from 5132942258995200.00000 to 890412151603200.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 890412151603200.00000 to 157814798417920.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 157814798417920.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 157814798417920.00000 to 136483205808128.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 136483205808128.00000 to 22196732821504.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 22196732821504.00000 to 1087423315968.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1087423315968.00000 to 92266618880.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 92266618880.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 92266618880.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 92266618880.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 92266618880.00000 to 87830257664.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 87830257664.00000 to 17552230400.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 17552230400.00000 to 8291552768.00000, saving model to linear_regression/linear_regression_l11000.0_l21.0.h5\n",
      "\n",
      "Epoch 17: val_loss did not improve from 8291552768.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 8291552768.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 8291552768.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 8291552768.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 8291552768.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 8291552768.00000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 8291552768.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 8291552768.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 8291552768.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 8291552768.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2820482474704896.00000, saving model to linear_regression/linear_regression_l11000.0_l210.0.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 2820482474704896.00000\n",
      "\n",
      "Epoch 3: val_loss improved from 2820482474704896.00000 to 307492378640384.00000, saving model to linear_regression/linear_regression_l11000.0_l210.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 307492378640384.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 307492378640384.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 307492378640384.00000 to 44770390966272.00000, saving model to linear_regression/linear_regression_l11000.0_l210.0.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 44770390966272.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 44770390966272.00000 to 30534963036160.00000, saving model to linear_regression/linear_regression_l11000.0_l210.0.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 30534963036160.00000 to 32914458624.00000, saving model to linear_regression/linear_regression_l11000.0_l210.0.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 32914458624.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 32914458624.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 32914458624.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 32914458624.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 32914458624.00000 to 28596428800.00000, saving model to linear_regression/linear_regression_l11000.0_l210.0.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 28596428800.00000 to 1524890496.00000, saving model to linear_regression/linear_regression_l11000.0_l210.0.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1524890496.00000 to 1223856.25000, saving model to linear_regression/linear_regression_l11000.0_l210.0.h5\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1223856.25000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 264292839783923712.00000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 264292839783923712.00000 to 11230030587756544.00000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 11230030587756544.00000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 11230030587756544.00000\n",
      "\n",
      "Epoch 5: val_loss improved from 11230030587756544.00000 to 438467070787584.00000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 438467070787584.00000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 438467070787584.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 438467070787584.00000 to 17603294658560.00000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 9: val_loss did not improve from 17603294658560.00000\n",
      "\n",
      "Epoch 10: val_loss improved from 17603294658560.00000 to 223017385984.00000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 223017385984.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 223017385984.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 223017385984.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 223017385984.00000 to 54928994304.00000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 54928994304.00000\n",
      "\n",
      "Epoch 16: val_loss improved from 54928994304.00000 to 4382158336.00000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 17: val_loss did not improve from 4382158336.00000\n",
      "\n",
      "Epoch 18: val_loss improved from 4382158336.00000 to 22802940.00000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 19: val_loss did not improve from 22802940.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 22802940.00000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 22802940.00000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 22802940.00000\n",
      "\n",
      "Epoch 23: val_loss improved from 22802940.00000 to 15619406.00000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 15619406.00000 to 8242158.50000, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 8242158.50000 to 718674.56250, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 26: val_loss did not improve from 718674.56250\n",
      "\n",
      "Epoch 27: val_loss improved from 718674.56250 to 195412.79688, saving model to linear_regression/linear_regression_l11000.0_l2100.0.h5\n",
      "\n",
      "Epoch 28: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 29: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 30: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 31: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 32: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 33: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 34: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 35: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 36: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 37: val_loss did not improve from 195412.79688\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 435091800676368384.00000, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 435091800676368384.00000 to 56004522408935424.00000, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 56004522408935424.00000 to 1786888662810624.00000, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1786888662810624.00000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1786888662810624.00000\n",
      "\n",
      "Epoch 6: val_loss improved from 1786888662810624.00000 to 743611105280.00000, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 7: val_loss did not improve from 743611105280.00000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 743611105280.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 743611105280.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 743611105280.00000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 743611105280.00000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 743611105280.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 743611105280.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 743611105280.00000 to 112809091072.00000, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 112809091072.00000\n",
      "\n",
      "Epoch 16: val_loss improved from 112809091072.00000 to 796018688.00000, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 17: val_loss did not improve from 796018688.00000\n",
      "\n",
      "Epoch 18: val_loss improved from 796018688.00000 to 97439592.00000, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 19: val_loss did not improve from 97439592.00000\n",
      "\n",
      "Epoch 20: val_loss improved from 97439592.00000 to 52815888.00000, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 52815888.00000\n",
      "\n",
      "Epoch 22: val_loss improved from 52815888.00000 to 6245506.50000, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 6245506.50000\n",
      "\n",
      "Epoch 24: val_loss improved from 6245506.50000 to 555110.12500, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 25: val_loss did not improve from 555110.12500\n",
      "\n",
      "Epoch 26: val_loss improved from 555110.12500 to 242796.06250, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 27: val_loss did not improve from 242796.06250\n",
      "\n",
      "Epoch 28: val_loss improved from 242796.06250 to 190404.06250, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 29: val_loss did not improve from 190404.06250\n",
      "\n",
      "Epoch 30: val_loss did not improve from 190404.06250\n",
      "\n",
      "Epoch 31: val_loss did not improve from 190404.06250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: val_loss improved from 190404.06250 to 190151.68750, saving model to linear_regression/linear_regression_l11000.0_l21000.0.h5\n",
      "\n",
      "Epoch 33: val_loss did not improve from 190151.68750\n",
      "\n",
      "Epoch 34: val_loss did not improve from 190151.68750\n",
      "\n",
      "Epoch 35: val_loss did not improve from 190151.68750\n",
      "\n",
      "Epoch 36: val_loss did not improve from 190151.68750\n",
      "\n",
      "Epoch 37: val_loss did not improve from 190151.68750\n",
      "\n",
      "Epoch 38: val_loss did not improve from 190151.68750\n",
      "\n",
      "Epoch 39: val_loss did not improve from 190151.68750\n",
      "\n",
      "Epoch 40: val_loss did not improve from 190151.68750\n",
      "\n",
      "Epoch 41: val_loss did not improve from 190151.68750\n",
      "\n",
      "Epoch 42: val_loss did not improve from 190151.68750\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "model_folder = \"linear_regression\"\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "for name, model_dict in models.items():\n",
    "    model = model_dict[\"model\"]\n",
    "    model.build(input_shape=targets.shape)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.1**2),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\n",
    "            RootMeanSquaredError(name=\"root_mean_squared_error\", dtype=None),\n",
    "            MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None),\n",
    "        ],\n",
    "    )\n",
    "    path_checkpoint = f\"{model_folder}/{name}.h5\"\n",
    "\n",
    "    es_callback = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", min_delta=0, patience=10\n",
    "    )\n",
    "    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=path_checkpoint,\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    model_dict[\"history\"] = model.fit(\n",
    "        dt_b,\n",
    "        epochs=10000,\n",
    "        validation_data=dv_b,\n",
    "        callbacks=[es_callback, modelckpt_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "    metrics = pd.concat([metrics, pd.DataFrame(\n",
    "        {\n",
    "            \"model_name\":name,\n",
    "         \"RMSE\":model_dict[\"history\"].history[\"val_root_mean_squared_error\"][\n",
    "            np.argmin(model_dict[\"history\"].history[\"val_loss\"])\n",
    "        ], \n",
    "         \"MAPE\":model_dict[\"history\"].history[\"val_mean_absolute_error\"][\n",
    "            np.argmin(model_dict[\"history\"].history[\"val_loss\"])]\n",
    "        }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df3a1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eda17365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_name, RMSE, MAPE]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58064310",
   "metadata": {},
   "source": [
    "# Tree methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ca678",
   "metadata": {},
   "source": [
    "### Tensorflow Decision Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c59073",
   "metadata": {},
   "source": [
    "Following code on my notebook just get exception, however it should work on other device (I have problems with nvidia cuda and therefore with tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "364adc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_decision_forests.component.tuner.tuner.SearchSpace at 0x7fa31552f340>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow_decision_forests as tfdf\n",
    "# # Train a Random Forest model.\n",
    "# tuner = tfdf.tuner.RandomSearch(num_trials=2)\n",
    "\n",
    "# # Hyper-parameters to optimize.\n",
    "# tuner.choice(\"max_depth\", [4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c112287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = tfdf.keras.RandomForestModel()\n",
    "# rf.fit(\n",
    "#     dataset_train.batch(512),\n",
    "#     validation_data=dataset_val.batch(512),\n",
    "# #     tuner=tuner,\n",
    "# )\n",
    "\n",
    "# # Summary of the model structure.\n",
    "# rf.summary()\n",
    "\n",
    "# # Evaluate the model.\n",
    "# # rf.evaluate(dataset_val)\n",
    "\n",
    "# # Export the model to a SavedModel.\n",
    "# rf.save(\"random_forest.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc62a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmp32drw_d1 as temporary training directory\n",
      "Reading training dataset...\n",
      "WARNING:tensorflow:From /home/laptop/miniconda3/envs/solar_voltage/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/laptop/miniconda3/envs/solar_voltage/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset read in 0:00:03.847967. Found 4884 examples.\n",
      "Reading validation dataset...\n",
      "Num validation examples: tf.Tensor(2407, shape=(), dtype=int32)\n",
      "Validation dataset read in 0:00:00.660527. Found 2407 examples.\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 08:56:01.999750: W external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1790] \"goss_alpha\" set but \"sampling_method\" not equal to \"GOSS\".\n",
      "2023-01-31 08:56:01.999782: W external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1800] \"goss_beta\" set but \"sampling_method\" not equal to \"GOSS\".\n",
      "2023-01-31 08:56:01.999790: W external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1814] \"selective_gradient_boosting_ratio\" set but \"sampling_method\" not equal to \"SELGB\".\n",
      "2023-01-31 08:56:02.003503: I external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:452] Default loss set to MULTINOMIAL_LOG_LIKELIHOOD\n",
      "2023-01-31 08:56:02.003669: W external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:484] The model configuration specifies 300 trees but computation of the validation loss will only start at iteration 10 with 2457 trees per iteration. No validation loss will be computed, early stopping is not used.\n",
      "2023-01-31 08:56:02.003691: I external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1077] Training gradient boosted tree on 4884 example(s) and 98 feature(s).\n",
      "2023-01-31 08:56:02.003879: I external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1120] 4884 examples used for training and 2407 examples used for validation\n",
      "2023-01-31 08:56:37.947951: I external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1518] \tnum-trees:1 train-loss:4.005584 train-accuracy:0.266585 valid-loss:8.274650 valid-accuracy:0.002077\n",
      "2023-01-31 08:57:39.099302: I external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1520] \tnum-trees:2 train-loss:3.026718 train-accuracy:0.429975 valid-loss:8.579332 valid-accuracy:0.000415\n",
      "2023-01-31 08:58:41.598247: I external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1520] \tnum-trees:3 train-loss:2.616000 train-accuracy:0.479730 valid-loss:8.818600 valid-accuracy:0.000415\n",
      "2023-01-31 08:59:44.879544: I external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1520] \tnum-trees:4 train-loss:2.338533 train-accuracy:0.520475 valid-loss:9.022681 valid-accuracy:0.000415\n",
      "2023-01-31 09:00:48.208278: I external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1520] \tnum-trees:5 train-loss:2.107081 train-accuracy:0.568591 valid-loss:9.218490 valid-accuracy:0.000415\n"
     ]
    }
   ],
   "source": [
    "# gbt = tfdf.keras.GradientBoostedTreesModel()\n",
    "# gbt.fit(\n",
    "#     dataset_train.batch(512),\n",
    "#     validation_data=dataset_val.batch(512),\n",
    "# #     tuner=tuner,\n",
    "# )\n",
    "\n",
    "# gbt.save(\"GradientBoostedTreesModel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e9f8a",
   "metadata": {},
   "source": [
    "### Xgboost\n",
    "https://stackoverflow.com/questions/40418612/xgboost-on-android"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6440410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3980c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    " \n",
    "# Train and test set are converted to DMatrix objects,\n",
    "# as it is required by learning API.\n",
    "train_dmatrix = xgb.DMatrix(data = X_train, label = y_train)\n",
    "val_dmatrix = xgb.DMatrix(data = X_val, label = y_val)\n",
    "  \n",
    "# Parameter dictionary specifying base learner\n",
    "\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "       'booster': hp.choice('booster', ['gbtree', 'dart','gblinear']),\n",
    "        'reg_alpha' : hp.choice('reg_alpha', list(map(lambda x: pow(10, x), range(-4,5)))),\n",
    "        'reg_lambda' : hp.choice('reg_lambda', list(map(lambda x: pow(10, x), range(-4,5)))),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "#         'n_estimators': hp.choice([10,50,100,150,200]),\n",
    "       'objective':hp.choice('objective', ['reg:squarederror', 'reg:squaredlogerror', 'reg:absoluteerror', 'reg:pseudohubererror', 'reg:tweedie']),\n",
    "    }\n",
    "def objective(space):\n",
    "    evaluation = [( X_train, y_train), ( X_val, y_val)]\n",
    "    clf=xgb.XGBRegressor(\n",
    "        booster = space['booster'],\n",
    "        objective = space['objective'],\n",
    "#                     n_estimators =space['n_estimators'],\n",
    "        max_depth = int(space['max_depth']), \n",
    "#         gamma = space['gamma'],\n",
    "        reg_lambda = space['reg_lambda'],\n",
    "        reg_alpha = int(space['reg_alpha']),\n",
    "        min_child_weight=int(space['min_child_weight']),\n",
    "        colsample_bytree=int(space['colsample_bytree']),\n",
    "        early_stopping_rounds=4,\n",
    "        seed=42,\n",
    "                    eval_metric=\"rmse\",\n",
    "\n",
    "    )\n",
    "    \n",
    "    \n",
    "    clf.fit(X_train, y_train,\n",
    "                    eval_set=evaluation,\n",
    "\n",
    "            verbose=True)\n",
    "    pred = clf.predict(X_val)\n",
    "    try:\n",
    "        rmse = np.sqrt(MSE(y_val, pred))\n",
    "    except ValueError:\n",
    "        rmse = 10**5\n",
    "    print (\"SCORE:\", rmse)\n",
    "    return {'loss': rmse, 'status': STATUS_OK }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0725c413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1980.60541\tvalidation_1-rmse:1091.03134                   \n",
      "[1]\tvalidation_0-rmse:1809.25879\tvalidation_1-rmse:980.10885                    \n",
      "[2]\tvalidation_0-rmse:1809.25879\tvalidation_1-rmse:980.10885                    \n",
      "[3]\tvalidation_0-rmse:1809.25879\tvalidation_1-rmse:980.10885                    \n",
      "[4]\tvalidation_0-rmse:1809.25879\tvalidation_1-rmse:980.10885                    \n",
      "[5]\tvalidation_0-rmse:1809.25879\tvalidation_1-rmse:980.10885                    \n",
      "SCORE:                                                                          \n",
      "980.1088530749051                                                               \n",
      "[09:49:18] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1985.81385\tvalidation_1-rmse:1093.67715                   \n",
      "[1]\tvalidation_0-rmse:1963.69436\tvalidation_1-rmse:1060.64838                   \n",
      "[2]\tvalidation_0-rmse:1797.19510\tvalidation_1-rmse:1288.75126                   \n",
      "[3]\tvalidation_0-rmse:1216.27445\tvalidation_1-rmse:12772.30114                  \n",
      "[4]\tvalidation_0-rmse:1771.57546\tvalidation_1-rmse:27131.94627                  \n",
      "SCORE:                                                                          \n",
      "22771.625346042136                                                              \n",
      "[09:49:18] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1303.20379\tvalidation_1-rmse:7880.85178                   \n",
      "[1]\tvalidation_0-rmse:1112.03071\tvalidation_1-rmse:6607.39375                   \n",
      "[2]\tvalidation_0-rmse:1029.22309\tvalidation_1-rmse:5745.78719                   \n",
      "[3]\tvalidation_0-rmse:691.47328\tvalidation_1-rmse:3692.77064                    \n",
      "[4]\tvalidation_0-rmse:666.89885\tvalidation_1-rmse:3607.74960                    \n",
      "[5]\tvalidation_0-rmse:662.47125\tvalidation_1-rmse:3372.34459                    \n",
      "[6]\tvalidation_0-rmse:663.34426\tvalidation_1-rmse:3315.00837                    \n",
      "[7]\tvalidation_0-rmse:665.58736\tvalidation_1-rmse:3240.27337                    \n",
      "[8]\tvalidation_0-rmse:669.70146\tvalidation_1-rmse:3102.71239                    \n",
      "[9]\tvalidation_0-rmse:706.82030\tvalidation_1-rmse:2724.23053                    \n",
      "[10]\tvalidation_0-rmse:721.03102\tvalidation_1-rmse:2579.73795                   \n",
      "[11]\tvalidation_0-rmse:739.51125\tvalidation_1-rmse:2410.22649                   \n",
      "[12]\tvalidation_0-rmse:752.17734\tvalidation_1-rmse:2306.73848                   \n",
      "[13]\tvalidation_0-rmse:760.82372\tvalidation_1-rmse:2290.91850                   \n",
      "[14]\tvalidation_0-rmse:770.61734\tvalidation_1-rmse:2240.60496                   \n",
      "[15]\tvalidation_0-rmse:774.90270\tvalidation_1-rmse:2210.38328                   \n",
      "[16]\tvalidation_0-rmse:778.84365\tvalidation_1-rmse:2182.37172                   \n",
      "[17]\tvalidation_0-rmse:782.54076\tvalidation_1-rmse:2158.90510                   \n",
      "[18]\tvalidation_0-rmse:786.78621\tvalidation_1-rmse:2135.97000                   \n",
      "[19]\tvalidation_0-rmse:789.38196\tvalidation_1-rmse:2106.98856                   \n",
      "[20]\tvalidation_0-rmse:792.46319\tvalidation_1-rmse:2091.97633                   \n",
      "[21]\tvalidation_0-rmse:794.39846\tvalidation_1-rmse:2085.46751                   \n",
      "[22]\tvalidation_0-rmse:796.09237\tvalidation_1-rmse:2077.75804                   \n",
      "[23]\tvalidation_0-rmse:797.62254\tvalidation_1-rmse:2071.08331                   \n",
      "[24]\tvalidation_0-rmse:798.83955\tvalidation_1-rmse:2066.35101                   \n",
      "[25]\tvalidation_0-rmse:799.74930\tvalidation_1-rmse:2061.84801                   \n",
      "[26]\tvalidation_0-rmse:800.60435\tvalidation_1-rmse:2058.49779                   \n",
      "[27]\tvalidation_0-rmse:801.33138\tvalidation_1-rmse:2055.23985                   \n",
      "[28]\tvalidation_0-rmse:802.05624\tvalidation_1-rmse:2051.89410                   \n",
      "[29]\tvalidation_0-rmse:802.62047\tvalidation_1-rmse:2048.13866                   \n",
      "[30]\tvalidation_0-rmse:803.30005\tvalidation_1-rmse:2043.30943                   \n",
      "[31]\tvalidation_0-rmse:804.05362\tvalidation_1-rmse:2037.71823                   \n",
      "[32]\tvalidation_0-rmse:804.69999\tvalidation_1-rmse:2032.80414                   \n",
      "[33]\tvalidation_0-rmse:805.37693\tvalidation_1-rmse:2027.75004                   \n",
      "[34]\tvalidation_0-rmse:805.97872\tvalidation_1-rmse:2022.89687                   \n",
      "[35]\tvalidation_0-rmse:806.46027\tvalidation_1-rmse:2019.18141                   \n",
      "[36]\tvalidation_0-rmse:806.80389\tvalidation_1-rmse:2016.15065                   \n",
      "[37]\tvalidation_0-rmse:807.10856\tvalidation_1-rmse:2013.37768                   \n",
      "[38]\tvalidation_0-rmse:807.35725\tvalidation_1-rmse:2010.81589                   \n",
      "[39]\tvalidation_0-rmse:807.56397\tvalidation_1-rmse:2008.51432                   \n",
      "[40]\tvalidation_0-rmse:807.74581\tvalidation_1-rmse:2006.41283                   \n",
      "[41]\tvalidation_0-rmse:807.89334\tvalidation_1-rmse:2004.32753                   \n",
      "[42]\tvalidation_0-rmse:808.01840\tvalidation_1-rmse:2002.30011                   \n",
      "[43]\tvalidation_0-rmse:808.11905\tvalidation_1-rmse:2000.42772                   \n",
      "[44]\tvalidation_0-rmse:808.19148\tvalidation_1-rmse:1998.71895                   \n",
      "[45]\tvalidation_0-rmse:808.24095\tvalidation_1-rmse:1997.12140                   \n",
      "[46]\tvalidation_0-rmse:808.27343\tvalidation_1-rmse:1995.63233                   \n",
      "[47]\tvalidation_0-rmse:808.28943\tvalidation_1-rmse:1994.25477                   \n",
      "[48]\tvalidation_0-rmse:808.28945\tvalidation_1-rmse:1992.98569                   \n",
      "[49]\tvalidation_0-rmse:808.27543\tvalidation_1-rmse:1991.81307                   \n",
      "[50]\tvalidation_0-rmse:808.25058\tvalidation_1-rmse:1990.71818                   \n",
      "[51]\tvalidation_0-rmse:808.21819\tvalidation_1-rmse:1989.68818                   \n",
      "[52]\tvalidation_0-rmse:808.18027\tvalidation_1-rmse:1988.71552                   \n",
      "[53]\tvalidation_0-rmse:808.13692\tvalidation_1-rmse:1987.79338                   \n",
      "[54]\tvalidation_0-rmse:808.08762\tvalidation_1-rmse:1986.91735                   \n",
      "[55]\tvalidation_0-rmse:808.03171\tvalidation_1-rmse:1986.08256                   \n",
      "[56]\tvalidation_0-rmse:807.96908\tvalidation_1-rmse:1985.28436                   \n",
      "[57]\tvalidation_0-rmse:807.90008\tvalidation_1-rmse:1984.51850                   \n",
      "[58]\tvalidation_0-rmse:807.82493\tvalidation_1-rmse:1983.78061                   \n",
      "[59]\tvalidation_0-rmse:807.74412\tvalidation_1-rmse:1983.06698                   \n",
      "[60]\tvalidation_0-rmse:807.65823\tvalidation_1-rmse:1982.37364                   \n",
      "[61]\tvalidation_0-rmse:807.56788\tvalidation_1-rmse:1981.69656                   \n",
      "[62]\tvalidation_0-rmse:807.47409\tvalidation_1-rmse:1981.03278                   \n",
      "[63]\tvalidation_0-rmse:807.37748\tvalidation_1-rmse:1980.37886                   \n",
      "[64]\tvalidation_0-rmse:807.27883\tvalidation_1-rmse:1979.73260                   \n",
      "[65]\tvalidation_0-rmse:807.17852\tvalidation_1-rmse:1979.09155                   \n",
      "[66]\tvalidation_0-rmse:807.07694\tvalidation_1-rmse:1978.45422                   \n",
      "[67]\tvalidation_0-rmse:806.97418\tvalidation_1-rmse:1977.81904                   \n",
      "[68]\tvalidation_0-rmse:806.87017\tvalidation_1-rmse:1977.18444                   \n",
      "[69]\tvalidation_0-rmse:806.76505\tvalidation_1-rmse:1976.54985                   \n",
      "[70]\tvalidation_0-rmse:806.65871\tvalidation_1-rmse:1975.91393                   \n",
      "[71]\tvalidation_0-rmse:806.55108\tvalidation_1-rmse:1975.27594                   \n",
      "[72]\tvalidation_0-rmse:806.44224\tvalidation_1-rmse:1974.63578                   \n",
      "[73]\tvalidation_0-rmse:806.33205\tvalidation_1-rmse:1973.99331                   \n",
      "[74]\tvalidation_0-rmse:806.22037\tvalidation_1-rmse:1973.34908                   \n",
      "[75]\tvalidation_0-rmse:806.10703\tvalidation_1-rmse:1972.70396                   \n",
      "[76]\tvalidation_0-rmse:805.99165\tvalidation_1-rmse:1972.05846                   \n",
      "[77]\tvalidation_0-rmse:805.87395\tvalidation_1-rmse:1971.41339                   \n",
      "[78]\tvalidation_0-rmse:805.75386\tvalidation_1-rmse:1970.76948                   \n",
      "[79]\tvalidation_0-rmse:805.63123\tvalidation_1-rmse:1970.12675                   \n",
      "[80]\tvalidation_0-rmse:805.50622\tvalidation_1-rmse:1969.48544                   \n",
      "[81]\tvalidation_0-rmse:805.37886\tvalidation_1-rmse:1968.84511                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82]\tvalidation_0-rmse:805.24952\tvalidation_1-rmse:1968.20576                   \n",
      "[83]\tvalidation_0-rmse:805.11837\tvalidation_1-rmse:1967.56652                   \n",
      "[84]\tvalidation_0-rmse:804.98568\tvalidation_1-rmse:1966.92674                   \n",
      "[85]\tvalidation_0-rmse:804.85172\tvalidation_1-rmse:1966.28545                   \n",
      "[86]\tvalidation_0-rmse:804.71689\tvalidation_1-rmse:1965.64182                   \n",
      "[87]\tvalidation_0-rmse:804.58146\tvalidation_1-rmse:1964.99475                   \n",
      "[88]\tvalidation_0-rmse:804.44570\tvalidation_1-rmse:1964.34303                   \n",
      "[89]\tvalidation_0-rmse:804.30998\tvalidation_1-rmse:1963.68636                   \n",
      "[90]\tvalidation_0-rmse:804.17446\tvalidation_1-rmse:1963.02398                   \n",
      "[91]\tvalidation_0-rmse:804.03945\tvalidation_1-rmse:1962.35640                   \n",
      "[92]\tvalidation_0-rmse:803.90496\tvalidation_1-rmse:1961.68374                   \n",
      "[93]\tvalidation_0-rmse:803.77113\tvalidation_1-rmse:1961.00729                   \n",
      "[94]\tvalidation_0-rmse:803.63778\tvalidation_1-rmse:1960.32738                   \n",
      "[95]\tvalidation_0-rmse:803.50517\tvalidation_1-rmse:1959.64558                   \n",
      "[96]\tvalidation_0-rmse:803.37353\tvalidation_1-rmse:1958.96273                   \n",
      "[97]\tvalidation_0-rmse:803.24319\tvalidation_1-rmse:1958.27945                   \n",
      "[98]\tvalidation_0-rmse:803.11454\tvalidation_1-rmse:1957.59623                   \n",
      "[99]\tvalidation_0-rmse:802.98805\tvalidation_1-rmse:1956.91348                   \n",
      "SCORE:                                                                          \n",
      "1956.9134702935821                                                              \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65182\tvalidation_1-rmse:1095.99360                   \n",
      "[2]\tvalidation_0-rmse:1986.39577\tvalidation_1-rmse:1095.10694                   \n",
      "[3]\tvalidation_0-rmse:1984.11441\tvalidation_1-rmse:1093.49896                   \n",
      "[4]\tvalidation_0-rmse:1979.98024\tvalidation_1-rmse:1090.59251                   \n",
      "[5]\tvalidation_0-rmse:1972.52721\tvalidation_1-rmse:1085.37814                   \n",
      "[6]\tvalidation_0-rmse:1959.19975\tvalidation_1-rmse:1076.13616                   \n",
      "[7]\tvalidation_0-rmse:1935.71553\tvalidation_1-rmse:1060.11587                   \n",
      "[8]\tvalidation_0-rmse:1895.37679\tvalidation_1-rmse:1033.43152                   \n",
      "[9]\tvalidation_0-rmse:1828.98913\tvalidation_1-rmse:992.03150                    \n",
      "[10]\tvalidation_0-rmse:1726.89255\tvalidation_1-rmse:935.42472                   \n",
      "[11]\tvalidation_0-rmse:1584.62045\tvalidation_1-rmse:874.03691                   \n",
      "[12]\tvalidation_0-rmse:1410.09947\tvalidation_1-rmse:833.91422                   \n",
      "[13]\tvalidation_0-rmse:1224.70520\tvalidation_1-rmse:842.98809                   \n",
      "[14]\tvalidation_0-rmse:1053.93879\tvalidation_1-rmse:903.77115                   \n",
      "[15]\tvalidation_0-rmse:915.66146\tvalidation_1-rmse:992.82944                    \n",
      "SCORE:                                                                          \n",
      "833.9142211261437                                                               \n",
      "[0]\tvalidation_0-rmse:1988.38372\tvalidation_1-rmse:1096.51055                   \n",
      "[1]\tvalidation_0-rmse:1987.97907\tvalidation_1-rmse:1096.22468                   \n",
      "[2]\tvalidation_0-rmse:1987.50653\tvalidation_1-rmse:1095.89090                   \n",
      "[3]\tvalidation_0-rmse:1986.97110\tvalidation_1-rmse:1095.51292                   \n",
      "[4]\tvalidation_0-rmse:1986.37963\tvalidation_1-rmse:1095.09555                   \n",
      "[5]\tvalidation_0-rmse:1985.76214\tvalidation_1-rmse:1094.65999                   \n",
      "[6]\tvalidation_0-rmse:1985.13095\tvalidation_1-rmse:1094.21503                   \n",
      "[7]\tvalidation_0-rmse:1984.50184\tvalidation_1-rmse:1093.77186                   \n",
      "[8]\tvalidation_0-rmse:1983.88625\tvalidation_1-rmse:1093.33829                   \n",
      "[9]\tvalidation_0-rmse:1983.29056\tvalidation_1-rmse:1092.91897                   \n",
      "[10]\tvalidation_0-rmse:1982.71789\tvalidation_1-rmse:1092.51605                  \n",
      "[11]\tvalidation_0-rmse:1982.16916\tvalidation_1-rmse:1092.13020                  \n",
      "[12]\tvalidation_0-rmse:1981.64411\tvalidation_1-rmse:1091.76110                  \n",
      "[13]\tvalidation_0-rmse:1981.14162\tvalidation_1-rmse:1091.40795                  \n",
      "[14]\tvalidation_0-rmse:1980.66014\tvalidation_1-rmse:1091.06981                  \n",
      "[15]\tvalidation_0-rmse:1980.19847\tvalidation_1-rmse:1090.74555                  \n",
      "[16]\tvalidation_0-rmse:1979.75488\tvalidation_1-rmse:1090.43420                  \n",
      "[17]\tvalidation_0-rmse:1979.32798\tvalidation_1-rmse:1090.13468                  \n",
      "[18]\tvalidation_0-rmse:1978.91665\tvalidation_1-rmse:1089.84617                  \n",
      "[19]\tvalidation_0-rmse:1978.51956\tvalidation_1-rmse:1089.56778                  \n",
      "[20]\tvalidation_0-rmse:1978.13576\tvalidation_1-rmse:1089.29876                  \n",
      "[21]\tvalidation_0-rmse:1977.76435\tvalidation_1-rmse:1089.03844                  \n",
      "[22]\tvalidation_0-rmse:1977.40425\tvalidation_1-rmse:1088.78623                  \n",
      "[23]\tvalidation_0-rmse:1977.05474\tvalidation_1-rmse:1088.54145                  \n",
      "[24]\tvalidation_0-rmse:1976.71517\tvalidation_1-rmse:1088.30375                  \n",
      "[25]\tvalidation_0-rmse:1976.38502\tvalidation_1-rmse:1088.07257                  \n",
      "[26]\tvalidation_0-rmse:1976.06347\tvalidation_1-rmse:1087.84756                  \n",
      "[27]\tvalidation_0-rmse:1975.75005\tvalidation_1-rmse:1087.62829                  \n",
      "[28]\tvalidation_0-rmse:1975.44430\tvalidation_1-rmse:1087.41445                  \n",
      "[29]\tvalidation_0-rmse:1975.14577\tvalidation_1-rmse:1087.20570                  \n",
      "[30]\tvalidation_0-rmse:1974.85402\tvalidation_1-rmse:1087.00180                  \n",
      "[31]\tvalidation_0-rmse:1974.56880\tvalidation_1-rmse:1086.80245                  \n",
      "[32]\tvalidation_0-rmse:1974.28968\tvalidation_1-rmse:1086.60747                  \n",
      "[33]\tvalidation_0-rmse:1974.01640\tvalidation_1-rmse:1086.41653                  \n",
      "[34]\tvalidation_0-rmse:1973.74874\tvalidation_1-rmse:1086.22952                  \n",
      "[35]\tvalidation_0-rmse:1973.48610\tvalidation_1-rmse:1086.04622                  \n",
      "[36]\tvalidation_0-rmse:1973.22862\tvalidation_1-rmse:1085.86647                  \n",
      "[37]\tvalidation_0-rmse:1972.97606\tvalidation_1-rmse:1085.69008                  \n",
      "[38]\tvalidation_0-rmse:1972.72785\tvalidation_1-rmse:1085.51692                  \n",
      "[39]\tvalidation_0-rmse:1972.48412\tvalidation_1-rmse:1085.34686                  \n",
      "[40]\tvalidation_0-rmse:1972.24447\tvalidation_1-rmse:1085.17974                  \n",
      "[41]\tvalidation_0-rmse:1972.00886\tvalidation_1-rmse:1085.01543                  \n",
      "[42]\tvalidation_0-rmse:1971.77728\tvalidation_1-rmse:1084.85384                  \n",
      "[43]\tvalidation_0-rmse:1971.54915\tvalidation_1-rmse:1084.69490                  \n",
      "[44]\tvalidation_0-rmse:1971.32480\tvalidation_1-rmse:1084.53847                  \n",
      "[45]\tvalidation_0-rmse:1971.10365\tvalidation_1-rmse:1084.38440                  \n",
      "[46]\tvalidation_0-rmse:1970.88605\tvalidation_1-rmse:1084.23267                  \n",
      "[47]\tvalidation_0-rmse:1970.67143\tvalidation_1-rmse:1084.08321                  \n",
      "[48]\tvalidation_0-rmse:1970.45995\tvalidation_1-rmse:1083.93591                  \n",
      "[49]\tvalidation_0-rmse:1970.25141\tvalidation_1-rmse:1083.79068                  \n",
      "[50]\tvalidation_0-rmse:1970.04578\tvalidation_1-rmse:1083.64745                  \n",
      "[51]\tvalidation_0-rmse:1969.84287\tvalidation_1-rmse:1083.50619                  \n",
      "[52]\tvalidation_0-rmse:1969.64267\tvalidation_1-rmse:1083.36683                  \n",
      "[53]\tvalidation_0-rmse:1969.44497\tvalidation_1-rmse:1083.22929                  \n",
      "[54]\tvalidation_0-rmse:1969.24975\tvalidation_1-rmse:1083.09350                  \n",
      "[55]\tvalidation_0-rmse:1969.05720\tvalidation_1-rmse:1082.95948                  \n",
      "[56]\tvalidation_0-rmse:1968.86690\tvalidation_1-rmse:1082.82706                  \n",
      "[57]\tvalidation_0-rmse:1968.67869\tvalidation_1-rmse:1082.69627                  \n",
      "[58]\tvalidation_0-rmse:1968.49290\tvalidation_1-rmse:1082.56702                  \n",
      "[59]\tvalidation_0-rmse:1968.30937\tvalidation_1-rmse:1082.43935                  \n",
      "[60]\tvalidation_0-rmse:1968.12768\tvalidation_1-rmse:1082.31310                  \n",
      "[61]\tvalidation_0-rmse:1967.94822\tvalidation_1-rmse:1082.18837                  \n",
      "[62]\tvalidation_0-rmse:1967.77059\tvalidation_1-rmse:1082.06497                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63]\tvalidation_0-rmse:1967.59516\tvalidation_1-rmse:1081.94295                  \n",
      "[64]\tvalidation_0-rmse:1967.42136\tvalidation_1-rmse:1081.82226                  \n",
      "[65]\tvalidation_0-rmse:1967.24955\tvalidation_1-rmse:1081.70285                  \n",
      "[66]\tvalidation_0-rmse:1967.07936\tvalidation_1-rmse:1081.58470                  \n",
      "[67]\tvalidation_0-rmse:1966.91097\tvalidation_1-rmse:1081.46781                  \n",
      "[68]\tvalidation_0-rmse:1966.74435\tvalidation_1-rmse:1081.35206                  \n",
      "[69]\tvalidation_0-rmse:1966.57933\tvalidation_1-rmse:1081.23749                  \n",
      "[70]\tvalidation_0-rmse:1966.41609\tvalidation_1-rmse:1081.12411                  \n",
      "[71]\tvalidation_0-rmse:1966.25423\tvalidation_1-rmse:1081.01182                  \n",
      "[72]\tvalidation_0-rmse:1966.09394\tvalidation_1-rmse:1080.90058                  \n",
      "[73]\tvalidation_0-rmse:1965.93524\tvalidation_1-rmse:1080.79045                  \n",
      "[74]\tvalidation_0-rmse:1965.77790\tvalidation_1-rmse:1080.68134                  \n",
      "[75]\tvalidation_0-rmse:1965.62210\tvalidation_1-rmse:1080.57321                  \n",
      "[76]\tvalidation_0-rmse:1965.46767\tvalidation_1-rmse:1080.46609                  \n",
      "[77]\tvalidation_0-rmse:1965.31460\tvalidation_1-rmse:1080.35993                  \n",
      "[78]\tvalidation_0-rmse:1965.16289\tvalidation_1-rmse:1080.25478                  \n",
      "[79]\tvalidation_0-rmse:1965.01252\tvalidation_1-rmse:1080.15052                  \n",
      "[80]\tvalidation_0-rmse:1964.86349\tvalidation_1-rmse:1080.04714                  \n",
      "[81]\tvalidation_0-rmse:1964.71562\tvalidation_1-rmse:1079.94469                  \n",
      "[82]\tvalidation_0-rmse:1964.56908\tvalidation_1-rmse:1079.84312                  \n",
      "[83]\tvalidation_0-rmse:1964.42385\tvalidation_1-rmse:1079.74237                  \n",
      "[84]\tvalidation_0-rmse:1964.27959\tvalidation_1-rmse:1079.64247                  \n",
      "[85]\tvalidation_0-rmse:1964.13666\tvalidation_1-rmse:1079.54343                  \n",
      "[86]\tvalidation_0-rmse:1963.99484\tvalidation_1-rmse:1079.44513                  \n",
      "[87]\tvalidation_0-rmse:1963.85417\tvalidation_1-rmse:1079.34768                  \n",
      "[88]\tvalidation_0-rmse:1963.71441\tvalidation_1-rmse:1079.25095                  \n",
      "[89]\tvalidation_0-rmse:1963.57597\tvalidation_1-rmse:1079.15502                  \n",
      "[90]\tvalidation_0-rmse:1963.43847\tvalidation_1-rmse:1079.05986                  \n",
      "[91]\tvalidation_0-rmse:1963.30208\tvalidation_1-rmse:1078.96542                  \n",
      "[92]\tvalidation_0-rmse:1963.16659\tvalidation_1-rmse:1078.87163                  \n",
      "[93]\tvalidation_0-rmse:1963.03223\tvalidation_1-rmse:1078.77862                  \n",
      "[94]\tvalidation_0-rmse:1962.89879\tvalidation_1-rmse:1078.68631                  \n",
      "[95]\tvalidation_0-rmse:1962.76643\tvalidation_1-rmse:1078.59466                  \n",
      "[96]\tvalidation_0-rmse:1962.63498\tvalidation_1-rmse:1078.50365                  \n",
      "[97]\tvalidation_0-rmse:1962.50445\tvalidation_1-rmse:1078.41333                  \n",
      "[98]\tvalidation_0-rmse:1962.37482\tvalidation_1-rmse:1078.32365                  \n",
      "[99]\tvalidation_0-rmse:1962.24609\tvalidation_1-rmse:1078.23461                  \n",
      "SCORE:                                                                          \n",
      "1078.2346093030726                                                              \n",
      "[0]\tvalidation_0-rmse:1988.72207\tvalidation_1-rmse:1096.74966                   \n",
      "[1]\tvalidation_0-rmse:1988.72207\tvalidation_1-rmse:1096.74966                   \n",
      "[2]\tvalidation_0-rmse:1988.72207\tvalidation_1-rmse:1096.74966                   \n",
      "[3]\tvalidation_0-rmse:1988.72207\tvalidation_1-rmse:1096.74966                   \n",
      "SCORE:                                                                          \n",
      "1096.749657838829                                                               \n",
      "[0]\tvalidation_0-rmse:1980.51794\tvalidation_1-rmse:1090.98739                   \n",
      "[1]\tvalidation_0-rmse:1767.71129\tvalidation_1-rmse:956.54801                    \n",
      "[2]\tvalidation_0-rmse:1767.71129\tvalidation_1-rmse:956.54801                    \n",
      "[3]\tvalidation_0-rmse:1767.71129\tvalidation_1-rmse:956.54801                    \n",
      "[4]\tvalidation_0-rmse:1767.71129\tvalidation_1-rmse:956.54801                    \n",
      "SCORE:                                                                          \n",
      "956.5479893453368                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97832\tvalidation_1-rmse:1090.59103                   \n",
      "[5]\tvalidation_0-rmse:1972.52374\tvalidation_1-rmse:1085.37437                   \n",
      "[6]\tvalidation_0-rmse:1959.19694\tvalidation_1-rmse:1076.12787                   \n",
      "[7]\tvalidation_0-rmse:1935.72811\tvalidation_1-rmse:1060.10116                   \n",
      "[8]\tvalidation_0-rmse:1895.46872\tvalidation_1-rmse:1033.41540                   \n",
      "[9]\tvalidation_0-rmse:1829.38180\tvalidation_1-rmse:992.04599                    \n",
      "[10]\tvalidation_0-rmse:1728.22312\tvalidation_1-rmse:935.55479                   \n",
      "[11]\tvalidation_0-rmse:1588.37984\tvalidation_1-rmse:874.35720                   \n",
      "[12]\tvalidation_0-rmse:1418.99358\tvalidation_1-rmse:834.20807                   \n",
      "[13]\tvalidation_0-rmse:1242.54918\tvalidation_1-rmse:841.99965                   \n",
      "[14]\tvalidation_0-rmse:1084.63690\tvalidation_1-rmse:900.19784                   \n",
      "[15]\tvalidation_0-rmse:961.82747\tvalidation_1-rmse:986.03481                    \n",
      "SCORE:                                                                          \n",
      "834.2080857130345                                                               \n",
      "[09:49:28] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:619.66882\tvalidation_1-rmse:2254.22913                    \n",
      "[1]\tvalidation_0-rmse:609.47752\tvalidation_1-rmse:2439.19654                    \n",
      "[2]\tvalidation_0-rmse:604.41297\tvalidation_1-rmse:2533.22376                    \n",
      "[3]\tvalidation_0-rmse:601.19172\tvalidation_1-rmse:2596.46893                    \n",
      "SCORE:                                                                          \n",
      "2640.885649251654                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39514\tvalidation_1-rmse:1095.10659                   \n",
      "[3]\tvalidation_0-rmse:1984.11304\tvalidation_1-rmse:1093.49795                   \n",
      "[4]\tvalidation_0-rmse:1979.97877\tvalidation_1-rmse:1090.59134                   \n",
      "[5]\tvalidation_0-rmse:1972.52487\tvalidation_1-rmse:1085.37515                   \n",
      "[6]\tvalidation_0-rmse:1959.19966\tvalidation_1-rmse:1076.12980                   \n",
      "[7]\tvalidation_0-rmse:1935.73483\tvalidation_1-rmse:1060.10572                   \n",
      "[8]\tvalidation_0-rmse:1895.48459\tvalidation_1-rmse:1033.42571                   \n",
      "[9]\tvalidation_0-rmse:1829.41698\tvalidation_1-rmse:992.06712                    \n",
      "[10]\tvalidation_0-rmse:1728.29480\tvalidation_1-rmse:935.59153                   \n",
      "[11]\tvalidation_0-rmse:1588.51057\tvalidation_1-rmse:874.40373                   \n",
      "[12]\tvalidation_0-rmse:1419.22865\tvalidation_1-rmse:834.17001                   \n",
      "[13]\tvalidation_0-rmse:1242.88528\tvalidation_1-rmse:842.08440                   \n",
      "[14]\tvalidation_0-rmse:1085.05938\tvalidation_1-rmse:900.43467                   \n",
      "[15]\tvalidation_0-rmse:962.30263\tvalidation_1-rmse:986.41156                    \n",
      "[16]\tvalidation_0-rmse:877.83508\tvalidation_1-rmse:1075.61282                   \n",
      "SCORE:                                                                          \n",
      "834.1700081679094                                                               \n",
      "[09:49:28] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1987.50810\tvalidation_1-rmse:1095.80955                   \n",
      "[1]\tvalidation_0-rmse:1985.27174\tvalidation_1-rmse:1093.99227                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\tvalidation_0-rmse:1981.19733\tvalidation_1-rmse:1090.55719                   \n",
      "[3]\tvalidation_0-rmse:1973.87235\tvalidation_1-rmse:1084.21033                   \n",
      "[4]\tvalidation_0-rmse:1960.94080\tvalidation_1-rmse:1072.74305                   \n",
      "[5]\tvalidation_0-rmse:1938.68264\tvalidation_1-rmse:1052.72984                   \n",
      "[6]\tvalidation_0-rmse:1901.65791\tvalidation_1-rmse:1020.03043                   \n",
      "[7]\tvalidation_0-rmse:1842.87898\tvalidation_1-rmse:972.45249                    \n",
      "[8]\tvalidation_0-rmse:1754.38241\tvalidation_1-rmse:917.85124                    \n",
      "[9]\tvalidation_0-rmse:1629.49939\tvalidation_1-rmse:895.90423                    \n",
      "[10]\tvalidation_0-rmse:1467.06972\tvalidation_1-rmse:993.29452                   \n",
      "[11]\tvalidation_0-rmse:1279.70682\tvalidation_1-rmse:1286.70556                  \n",
      "[12]\tvalidation_0-rmse:1096.47451\tvalidation_1-rmse:1766.70234                  \n",
      "SCORE:                                                                          \n",
      "2279.4283236674864                                                              \n",
      "[0]\tvalidation_0-rmse:833.71279\tvalidation_1-rmse:1681.60823                    \n",
      "[1]\tvalidation_0-rmse:833.71278\tvalidation_1-rmse:1681.60830                    \n",
      "[2]\tvalidation_0-rmse:833.71282\tvalidation_1-rmse:1681.60833                    \n",
      "[3]\tvalidation_0-rmse:833.71292\tvalidation_1-rmse:1681.60847                    \n",
      "[4]\tvalidation_0-rmse:833.71216\tvalidation_1-rmse:1681.60691                    \n",
      "[5]\tvalidation_0-rmse:833.71215\tvalidation_1-rmse:1681.60689                    \n",
      "[6]\tvalidation_0-rmse:833.71214\tvalidation_1-rmse:1681.60687                    \n",
      "[7]\tvalidation_0-rmse:833.71247\tvalidation_1-rmse:1681.60178                    \n",
      "[8]\tvalidation_0-rmse:833.71252\tvalidation_1-rmse:1681.60192                    \n",
      "[9]\tvalidation_0-rmse:833.71254\tvalidation_1-rmse:1681.60194                    \n",
      "[10]\tvalidation_0-rmse:833.71186\tvalidation_1-rmse:1681.60055                   \n",
      "[11]\tvalidation_0-rmse:833.71188\tvalidation_1-rmse:1681.60049                   \n",
      "[12]\tvalidation_0-rmse:833.71542\tvalidation_1-rmse:1681.61958                   \n",
      "[13]\tvalidation_0-rmse:833.71475\tvalidation_1-rmse:1681.61815                   \n",
      "[14]\tvalidation_0-rmse:833.71420\tvalidation_1-rmse:1681.61683                   \n",
      "SCORE:                                                                          \n",
      "1681.600461880821                                                               \n",
      "[09:49:29] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1987.50774\tvalidation_1-rmse:1095.81014                   \n",
      "[1]\tvalidation_0-rmse:1985.27084\tvalidation_1-rmse:1093.99401                   \n",
      "[2]\tvalidation_0-rmse:1981.19053\tvalidation_1-rmse:1090.55144                   \n",
      "[3]\tvalidation_0-rmse:1973.85910\tvalidation_1-rmse:1084.19679                   \n",
      "[4]\tvalidation_0-rmse:1960.90355\tvalidation_1-rmse:1072.71296                   \n",
      "[5]\tvalidation_0-rmse:1938.60504\tvalidation_1-rmse:1052.66541                   \n",
      "[6]\tvalidation_0-rmse:1901.55950\tvalidation_1-rmse:1019.95324                   \n",
      "[7]\tvalidation_0-rmse:1842.72621\tvalidation_1-rmse:972.34186                    \n",
      "[8]\tvalidation_0-rmse:1754.26893\tvalidation_1-rmse:917.79378                    \n",
      "[9]\tvalidation_0-rmse:1629.30715\tvalidation_1-rmse:895.93469                    \n",
      "[10]\tvalidation_0-rmse:1466.64419\tvalidation_1-rmse:993.73721                   \n",
      "[11]\tvalidation_0-rmse:1279.41923\tvalidation_1-rmse:1287.34017                  \n",
      "[12]\tvalidation_0-rmse:1096.53043\tvalidation_1-rmse:1767.07678                  \n",
      "SCORE:                                                                          \n",
      "2279.813117217977                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39514\tvalidation_1-rmse:1095.10659                   \n",
      "[3]\tvalidation_0-rmse:1984.11304\tvalidation_1-rmse:1093.49795                   \n",
      "[4]\tvalidation_0-rmse:1979.97877\tvalidation_1-rmse:1090.59134                   \n",
      "[5]\tvalidation_0-rmse:1972.52487\tvalidation_1-rmse:1085.37515                   \n",
      "[6]\tvalidation_0-rmse:1959.19966\tvalidation_1-rmse:1076.12980                   \n",
      "[7]\tvalidation_0-rmse:1935.73504\tvalidation_1-rmse:1060.10580                   \n",
      "[8]\tvalidation_0-rmse:1895.48501\tvalidation_1-rmse:1033.42592                   \n",
      "[9]\tvalidation_0-rmse:1829.41766\tvalidation_1-rmse:992.06755                    \n",
      "[10]\tvalidation_0-rmse:1728.29633\tvalidation_1-rmse:935.59230                   \n",
      "[11]\tvalidation_0-rmse:1588.51293\tvalidation_1-rmse:874.40458                   \n",
      "[12]\tvalidation_0-rmse:1419.23225\tvalidation_1-rmse:834.17040                   \n",
      "[13]\tvalidation_0-rmse:1242.88981\tvalidation_1-rmse:842.08347                   \n",
      "[14]\tvalidation_0-rmse:1085.06373\tvalidation_1-rmse:900.43230                   \n",
      "[15]\tvalidation_0-rmse:962.30630\tvalidation_1-rmse:986.40835                    \n",
      "[16]\tvalidation_0-rmse:877.83776\tvalidation_1-rmse:1075.60941                   \n",
      "SCORE:                                                                          \n",
      "834.1704016007731                                                               \n",
      "[0]\tvalidation_0-rmse:1494.47957\tvalidation_1-rmse:846.86890                    \n",
      "[1]\tvalidation_0-rmse:1178.88221\tvalidation_1-rmse:859.45827                    \n",
      "[2]\tvalidation_0-rmse:988.06625\tvalidation_1-rmse:964.81316                     \n",
      "[3]\tvalidation_0-rmse:879.56243\tvalidation_1-rmse:1073.42246                    \n",
      "SCORE:                                                                          \n",
      "846.8688948632965                                                               \n",
      "[09:49:29] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1985.79974\tvalidation_1-rmse:1093.88967                   \n",
      "[1]\tvalidation_0-rmse:1963.79708\tvalidation_1-rmse:1065.35045                   \n",
      "[2]\tvalidation_0-rmse:1801.96497\tvalidation_1-rmse:1031.63602                   \n",
      "[3]\tvalidation_0-rmse:1134.52966\tvalidation_1-rmse:7048.69635                   \n",
      "[4]\tvalidation_0-rmse:1382.26670\tvalidation_1-rmse:14869.49611                  \n",
      "[5]\tvalidation_0-rmse:1319.19710\tvalidation_1-rmse:12640.89029                  \n",
      "[6]\tvalidation_0-rmse:1218.63393\tvalidation_1-rmse:10404.37190                  \n",
      "SCORE:                                                                          \n",
      "10404.371848583472                                                              \n",
      "[09:49:29] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1987.54112\tvalidation_1-rmse:1095.68651                   \n",
      "[1]\tvalidation_0-rmse:1985.42278\tvalidation_1-rmse:1093.65387                   \n",
      "[2]\tvalidation_0-rmse:1981.74565\tvalidation_1-rmse:1089.96045                   \n",
      "[3]\tvalidation_0-rmse:1975.50318\tvalidation_1-rmse:1083.58568                   \n",
      "[4]\tvalidation_0-rmse:1965.19417\tvalidation_1-rmse:1073.06427                   \n",
      "[5]\tvalidation_0-rmse:1948.39269\tvalidation_1-rmse:1056.19707                   \n",
      "[6]\tvalidation_0-rmse:1921.10028\tvalidation_1-rmse:1029.86849                   \n",
      "[7]\tvalidation_0-rmse:1877.75599\tvalidation_1-rmse:991.71371                    \n",
      "[8]\tvalidation_0-rmse:1810.88405\tvalidation_1-rmse:943.68529                    \n",
      "[9]\tvalidation_0-rmse:1712.06012\tvalidation_1-rmse:903.82612                    \n",
      "[10]\tvalidation_0-rmse:1574.60812\tvalidation_1-rmse:927.63938                   \n",
      "[11]\tvalidation_0-rmse:1401.65981\tvalidation_1-rmse:1106.24611                  \n",
      "[12]\tvalidation_0-rmse:1215.09296\tvalidation_1-rmse:1488.91274                  \n",
      "SCORE:                                                                          \n",
      "2031.6892474336864                                                              \n",
      "[09:49:29] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:833.72655\tvalidation_1-rmse:1681.94714                    \n",
      "[1]\tvalidation_0-rmse:833.73891\tvalidation_1-rmse:1682.27905                    \n",
      "[2]\tvalidation_0-rmse:833.75140\tvalidation_1-rmse:1682.61119                    \n",
      "[3]\tvalidation_0-rmse:833.76381\tvalidation_1-rmse:1682.94313                    \n",
      "SCORE:                                                                          \n",
      "1683.2752675928155                                                              \n",
      "[09:49:30] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1987.75088\tvalidation_1-rmse:1095.79351                   \n",
      "[1]\tvalidation_0-rmse:1986.09792\tvalidation_1-rmse:1094.11699                   \n",
      "[2]\tvalidation_0-rmse:1983.37702\tvalidation_1-rmse:1091.27682                   \n",
      "[3]\tvalidation_0-rmse:1979.12369\tvalidation_1-rmse:1086.67714                   \n",
      "[4]\tvalidation_0-rmse:1973.03004\tvalidation_1-rmse:1079.70921                   \n",
      "[5]\tvalidation_0-rmse:1963.41768\tvalidation_1-rmse:1068.94528                   \n",
      "[6]\tvalidation_0-rmse:1948.51633\tvalidation_1-rmse:1052.73000                   \n",
      "[7]\tvalidation_0-rmse:1925.89413\tvalidation_1-rmse:1029.26177                   \n",
      "[8]\tvalidation_0-rmse:1892.24603\tvalidation_1-rmse:997.42930                    \n",
      "[9]\tvalidation_0-rmse:1843.38553\tvalidation_1-rmse:959.26641                    \n",
      "[10]\tvalidation_0-rmse:1774.54912\tvalidation_1-rmse:925.80323                   \n",
      "[11]\tvalidation_0-rmse:1681.32847\tvalidation_1-rmse:927.48838                   \n",
      "[12]\tvalidation_0-rmse:1561.73616\tvalidation_1-rmse:1020.03055                  \n",
      "[13]\tvalidation_0-rmse:1419.28399\tvalidation_1-rmse:1259.62199                  \n",
      "[14]\tvalidation_0-rmse:1265.96727\tvalidation_1-rmse:1662.22199                  \n",
      "SCORE:                                                                          \n",
      "1662.221977330932                                                               \n",
      "[09:49:30] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1436.23200\tvalidation_1-rmse:8535.97839                   \n",
      "[1]\tvalidation_0-rmse:1062.21880\tvalidation_1-rmse:6310.00962                   \n",
      "[2]\tvalidation_0-rmse:920.83248\tvalidation_1-rmse:5142.06797                    \n",
      "[3]\tvalidation_0-rmse:3864.74569\tvalidation_1-rmse:9415.78723                   \n",
      "[4]\tvalidation_0-rmse:35983009809.54243\tvalidation_1-rmse:87450689987.59691     \n",
      "[5]\tvalidation_0-rmse:inf\tvalidation_1-rmse:inf                                 \n",
      "SCORE:                                                                          \n",
      "100000                                                                          \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65201\tvalidation_1-rmse:1095.99363                   \n",
      "[2]\tvalidation_0-rmse:1986.39579\tvalidation_1-rmse:1095.10700                   \n",
      "[3]\tvalidation_0-rmse:1984.11466\tvalidation_1-rmse:1093.49919                   \n",
      "[4]\tvalidation_0-rmse:1979.98113\tvalidation_1-rmse:1090.59309                   \n",
      "[5]\tvalidation_0-rmse:1972.52946\tvalidation_1-rmse:1085.37971                   \n",
      "[6]\tvalidation_0-rmse:1959.20538\tvalidation_1-rmse:1076.14004                   \n",
      "[7]\tvalidation_0-rmse:1935.72922\tvalidation_1-rmse:1060.12506                   \n",
      "[8]\tvalidation_0-rmse:1895.40863\tvalidation_1-rmse:1033.45212                   \n",
      "[9]\tvalidation_0-rmse:1829.05824\tvalidation_1-rmse:992.07282                    \n",
      "[10]\tvalidation_0-rmse:1727.02929\tvalidation_1-rmse:935.49414                   \n",
      "[11]\tvalidation_0-rmse:1584.85753\tvalidation_1-rmse:874.11963                   \n",
      "[12]\tvalidation_0-rmse:1410.44890\tvalidation_1-rmse:833.94998                   \n",
      "[13]\tvalidation_0-rmse:1225.13813\tvalidation_1-rmse:842.89943                   \n",
      "[14]\tvalidation_0-rmse:1054.39361\tvalidation_1-rmse:903.53990                   \n",
      "[15]\tvalidation_0-rmse:916.07430\tvalidation_1-rmse:992.50499                    \n",
      "[16]\tvalidation_0-rmse:815.69737\tvalidation_1-rmse:1084.69605                   \n",
      "SCORE:                                                                          \n",
      "833.9499800802972                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65201\tvalidation_1-rmse:1095.99363                   \n",
      "[2]\tvalidation_0-rmse:1986.39579\tvalidation_1-rmse:1095.10700                   \n",
      "[3]\tvalidation_0-rmse:1984.11466\tvalidation_1-rmse:1093.49919                   \n",
      "[4]\tvalidation_0-rmse:1979.98113\tvalidation_1-rmse:1090.59309                   \n",
      "[5]\tvalidation_0-rmse:1972.52946\tvalidation_1-rmse:1085.37971                   \n",
      "[6]\tvalidation_0-rmse:1959.20538\tvalidation_1-rmse:1076.14004                   \n",
      "[7]\tvalidation_0-rmse:1935.72922\tvalidation_1-rmse:1060.12506                   \n",
      "[8]\tvalidation_0-rmse:1895.40863\tvalidation_1-rmse:1033.45212                   \n",
      "[9]\tvalidation_0-rmse:1829.05824\tvalidation_1-rmse:992.07282                    \n",
      "[10]\tvalidation_0-rmse:1727.02929\tvalidation_1-rmse:935.49414                   \n",
      "[11]\tvalidation_0-rmse:1584.85753\tvalidation_1-rmse:874.11963                   \n",
      "[12]\tvalidation_0-rmse:1410.44890\tvalidation_1-rmse:833.94998                   \n",
      "[13]\tvalidation_0-rmse:1225.13813\tvalidation_1-rmse:842.89943                   \n",
      "[14]\tvalidation_0-rmse:1054.39361\tvalidation_1-rmse:903.53990                   \n",
      "[15]\tvalidation_0-rmse:916.07430\tvalidation_1-rmse:992.50499                    \n",
      "SCORE:                                                                          \n",
      "833.9499800802972                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65180\tvalidation_1-rmse:1095.99353                   \n",
      "[2]\tvalidation_0-rmse:1986.39533\tvalidation_1-rmse:1095.10662                   \n",
      "[3]\tvalidation_0-rmse:1984.11308\tvalidation_1-rmse:1093.49807                   \n",
      "[4]\tvalidation_0-rmse:1979.97600\tvalidation_1-rmse:1090.58876                   \n",
      "[5]\tvalidation_0-rmse:1972.51511\tvalidation_1-rmse:1085.36893                   \n",
      "[6]\tvalidation_0-rmse:1959.16688\tvalidation_1-rmse:1076.11335                   \n",
      "[7]\tvalidation_0-rmse:1935.62730\tvalidation_1-rmse:1060.06021                   \n",
      "[8]\tvalidation_0-rmse:1895.14426\tvalidation_1-rmse:1033.29986                   \n",
      "[9]\tvalidation_0-rmse:1828.39643\tvalidation_1-rmse:991.74135                    \n",
      "[10]\tvalidation_0-rmse:1725.46704\tvalidation_1-rmse:934.87311                   \n",
      "[11]\tvalidation_0-rmse:1581.47841\tvalidation_1-rmse:873.26929                   \n",
      "[12]\tvalidation_0-rmse:1403.91161\tvalidation_1-rmse:833.52481                   \n",
      "[13]\tvalidation_0-rmse:1213.93272\tvalidation_1-rmse:844.25599                   \n",
      "[14]\tvalidation_0-rmse:1037.27930\tvalidation_1-rmse:907.87775                   \n",
      "[15]\tvalidation_0-rmse:892.47969\tvalidation_1-rmse:1000.14518                   \n",
      "[16]\tvalidation_0-rmse:785.66169\tvalidation_1-rmse:1093.28999                   \n",
      "SCORE:                                                                          \n",
      "833.5248166038042                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97460\tvalidation_1-rmse:1090.58755                   \n",
      "[5]\tvalidation_0-rmse:1972.51164\tvalidation_1-rmse:1085.36603                   \n",
      "[6]\tvalidation_0-rmse:1959.15751\tvalidation_1-rmse:1076.10636                   \n",
      "[7]\tvalidation_0-rmse:1935.60246\tvalidation_1-rmse:1060.04329                   \n",
      "[8]\tvalidation_0-rmse:1895.08003\tvalidation_1-rmse:1033.26000                   \n",
      "[9]\tvalidation_0-rmse:1828.23231\tvalidation_1-rmse:991.72271                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalidation_0-rmse:1725.07810\tvalidation_1-rmse:934.79816                   \n",
      "[11]\tvalidation_0-rmse:1580.64015\tvalidation_1-rmse:873.13212                   \n",
      "[12]\tvalidation_0-rmse:1402.31201\tvalidation_1-rmse:833.44668                   \n",
      "[13]\tvalidation_0-rmse:1211.25482\tvalidation_1-rmse:844.52537                   \n",
      "[14]\tvalidation_0-rmse:1033.31027\tvalidation_1-rmse:908.76722                   \n",
      "[15]\tvalidation_0-rmse:887.17463\tvalidation_1-rmse:1001.74920                   \n",
      "SCORE:                                                                          \n",
      "833.446682206818                                                                \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65180\tvalidation_1-rmse:1095.99353                   \n",
      "[2]\tvalidation_0-rmse:1986.39533\tvalidation_1-rmse:1095.10662                   \n",
      "[3]\tvalidation_0-rmse:1984.11308\tvalidation_1-rmse:1093.49807                   \n",
      "[4]\tvalidation_0-rmse:1979.97600\tvalidation_1-rmse:1090.58876                   \n",
      "[5]\tvalidation_0-rmse:1972.51511\tvalidation_1-rmse:1085.36893                   \n",
      "[6]\tvalidation_0-rmse:1959.16688\tvalidation_1-rmse:1076.11335                   \n",
      "[7]\tvalidation_0-rmse:1935.62731\tvalidation_1-rmse:1060.06021                   \n",
      "[8]\tvalidation_0-rmse:1895.14428\tvalidation_1-rmse:1033.29986                   \n",
      "[9]\tvalidation_0-rmse:1828.39666\tvalidation_1-rmse:991.74141                    \n",
      "[10]\tvalidation_0-rmse:1725.46748\tvalidation_1-rmse:934.87324                   \n",
      "[11]\tvalidation_0-rmse:1581.47964\tvalidation_1-rmse:873.26951                   \n",
      "[12]\tvalidation_0-rmse:1403.91439\tvalidation_1-rmse:833.52494                   \n",
      "[13]\tvalidation_0-rmse:1213.93802\tvalidation_1-rmse:844.25547                   \n",
      "[14]\tvalidation_0-rmse:1037.28835\tvalidation_1-rmse:907.87589                   \n",
      "[15]\tvalidation_0-rmse:892.49275\tvalidation_1-rmse:1000.14177                   \n",
      "[16]\tvalidation_0-rmse:785.67863\tvalidation_1-rmse:1093.28514                   \n",
      "SCORE:                                                                          \n",
      "833.5249437554136                                                               \n",
      "[0]\tvalidation_0-rmse:1494.38083\tvalidation_1-rmse:846.80588                    \n",
      "[1]\tvalidation_0-rmse:1178.75401\tvalidation_1-rmse:859.47799                    \n",
      "[2]\tvalidation_0-rmse:987.94685\tvalidation_1-rmse:964.88843                     \n",
      "[3]\tvalidation_0-rmse:879.46504\tvalidation_1-rmse:1073.52909                    \n",
      "[4]\tvalidation_0-rmse:615.79553\tvalidation_1-rmse:1218.86972                    \n",
      "SCORE:                                                                          \n",
      "846.8058782498265                                                               \n",
      "[0]\tvalidation_0-rmse:833.71258\tvalidation_1-rmse:1681.60771                    \n",
      "[1]\tvalidation_0-rmse:833.71286\tvalidation_1-rmse:1681.60822                    \n",
      "[2]\tvalidation_0-rmse:833.71309\tvalidation_1-rmse:1681.60875                    \n",
      "[3]\tvalidation_0-rmse:833.71260\tvalidation_1-rmse:1681.60769                    \n",
      "[4]\tvalidation_0-rmse:833.58567\tvalidation_1-rmse:1681.54293                    \n",
      "[5]\tvalidation_0-rmse:833.58517\tvalidation_1-rmse:1681.54187                    \n",
      "[6]\tvalidation_0-rmse:833.58460\tvalidation_1-rmse:1681.54069                    \n",
      "[7]\tvalidation_0-rmse:833.58379\tvalidation_1-rmse:1681.53904                    \n",
      "[8]\tvalidation_0-rmse:833.58345\tvalidation_1-rmse:1681.53834                    \n",
      "[9]\tvalidation_0-rmse:833.57704\tvalidation_1-rmse:1681.52579                    \n",
      "[10]\tvalidation_0-rmse:833.57624\tvalidation_1-rmse:1681.52415                   \n",
      "[11]\tvalidation_0-rmse:833.57549\tvalidation_1-rmse:1681.52260                   \n",
      "[12]\tvalidation_0-rmse:833.57493\tvalidation_1-rmse:1681.52141                   \n",
      "[13]\tvalidation_0-rmse:833.57458\tvalidation_1-rmse:1681.52068                   \n",
      "[14]\tvalidation_0-rmse:833.57410\tvalidation_1-rmse:1681.51969                   \n",
      "[15]\tvalidation_0-rmse:833.57354\tvalidation_1-rmse:1681.51851                   \n",
      "[16]\tvalidation_0-rmse:833.57744\tvalidation_1-rmse:1681.53020                   \n",
      "[17]\tvalidation_0-rmse:833.57707\tvalidation_1-rmse:1681.52941                   \n",
      "[18]\tvalidation_0-rmse:833.57670\tvalidation_1-rmse:1681.52866                   \n",
      "[19]\tvalidation_0-rmse:833.57639\tvalidation_1-rmse:1681.52800                   \n",
      "SCORE:                                                                          \n",
      "1681.518497866381                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97460\tvalidation_1-rmse:1090.58755                   \n",
      "[5]\tvalidation_0-rmse:1972.51164\tvalidation_1-rmse:1085.36603                   \n",
      "[6]\tvalidation_0-rmse:1959.15751\tvalidation_1-rmse:1076.10636                   \n",
      "[7]\tvalidation_0-rmse:1935.60246\tvalidation_1-rmse:1060.04329                   \n",
      "[8]\tvalidation_0-rmse:1895.08003\tvalidation_1-rmse:1033.26000                   \n",
      "[9]\tvalidation_0-rmse:1828.23231\tvalidation_1-rmse:991.72271                    \n",
      "[10]\tvalidation_0-rmse:1725.07810\tvalidation_1-rmse:934.79816                   \n",
      "[11]\tvalidation_0-rmse:1580.64015\tvalidation_1-rmse:873.13212                   \n",
      "[12]\tvalidation_0-rmse:1402.31201\tvalidation_1-rmse:833.44668                   \n",
      "[13]\tvalidation_0-rmse:1211.25482\tvalidation_1-rmse:844.52537                   \n",
      "[14]\tvalidation_0-rmse:1033.31027\tvalidation_1-rmse:908.76722                   \n",
      "[15]\tvalidation_0-rmse:887.17463\tvalidation_1-rmse:1001.74920                   \n",
      "SCORE:                                                                          \n",
      "833.446682206818                                                                \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97460\tvalidation_1-rmse:1090.58755                   \n",
      "[5]\tvalidation_0-rmse:1972.51164\tvalidation_1-rmse:1085.36603                   \n",
      "[6]\tvalidation_0-rmse:1959.15751\tvalidation_1-rmse:1076.10636                   \n",
      "[7]\tvalidation_0-rmse:1935.60246\tvalidation_1-rmse:1060.04329                   \n",
      "[8]\tvalidation_0-rmse:1895.08003\tvalidation_1-rmse:1033.26000                   \n",
      "[9]\tvalidation_0-rmse:1828.23231\tvalidation_1-rmse:991.72271                    \n",
      "[10]\tvalidation_0-rmse:1725.07810\tvalidation_1-rmse:934.79816                   \n",
      "[11]\tvalidation_0-rmse:1580.64015\tvalidation_1-rmse:873.13212                   \n",
      "[12]\tvalidation_0-rmse:1402.31201\tvalidation_1-rmse:833.44668                   \n",
      "[13]\tvalidation_0-rmse:1211.25482\tvalidation_1-rmse:844.52537                   \n",
      "[14]\tvalidation_0-rmse:1033.31027\tvalidation_1-rmse:908.76722                   \n",
      "[15]\tvalidation_0-rmse:887.17463\tvalidation_1-rmse:1001.74920                   \n",
      "[16]\tvalidation_0-rmse:779.10791\tvalidation_1-rmse:1095.56330                   \n",
      "SCORE:                                                                          \n",
      "833.446682206818                                                                \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97460\tvalidation_1-rmse:1090.58755                   \n",
      "[5]\tvalidation_0-rmse:1972.51164\tvalidation_1-rmse:1085.36603                   \n",
      "[6]\tvalidation_0-rmse:1959.15751\tvalidation_1-rmse:1076.10636                   \n",
      "[7]\tvalidation_0-rmse:1935.60246\tvalidation_1-rmse:1060.04329                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\tvalidation_0-rmse:1895.08003\tvalidation_1-rmse:1033.26000                   \n",
      "[9]\tvalidation_0-rmse:1828.23231\tvalidation_1-rmse:991.72271                    \n",
      "[10]\tvalidation_0-rmse:1725.07810\tvalidation_1-rmse:934.79816                   \n",
      "[11]\tvalidation_0-rmse:1580.64015\tvalidation_1-rmse:873.13212                   \n",
      "[12]\tvalidation_0-rmse:1402.31201\tvalidation_1-rmse:833.44668                   \n",
      "[13]\tvalidation_0-rmse:1211.25482\tvalidation_1-rmse:844.52537                   \n",
      "[14]\tvalidation_0-rmse:1033.31027\tvalidation_1-rmse:908.76722                   \n",
      "[15]\tvalidation_0-rmse:887.17463\tvalidation_1-rmse:1001.74920                   \n",
      "[16]\tvalidation_0-rmse:779.10791\tvalidation_1-rmse:1095.56330                   \n",
      "SCORE:                                                                          \n",
      "833.446682206818                                                                \n",
      "[0]\tvalidation_0-rmse:1494.39216\tvalidation_1-rmse:846.84912                    \n",
      "[1]\tvalidation_0-rmse:1178.77359\tvalidation_1-rmse:859.49482                    \n",
      "[2]\tvalidation_0-rmse:987.97101\tvalidation_1-rmse:964.88912                     \n",
      "[3]\tvalidation_0-rmse:879.49058\tvalidation_1-rmse:1073.51860                    \n",
      "[4]\tvalidation_0-rmse:616.25185\tvalidation_1-rmse:1221.82854                    \n",
      "SCORE:                                                                          \n",
      "846.8491045558276                                                               \n",
      "[0]\tvalidation_0-rmse:833.71280\tvalidation_1-rmse:1681.60824                    \n",
      "[1]\tvalidation_0-rmse:833.71291\tvalidation_1-rmse:1681.60834                    \n",
      "[2]\tvalidation_0-rmse:833.71294\tvalidation_1-rmse:1681.60847                    \n",
      "[3]\tvalidation_0-rmse:833.71257\tvalidation_1-rmse:1681.60762                    \n",
      "[4]\tvalidation_0-rmse:833.56457\tvalidation_1-rmse:1681.34672                    \n",
      "[5]\tvalidation_0-rmse:833.56377\tvalidation_1-rmse:1681.34502                    \n",
      "[6]\tvalidation_0-rmse:833.56304\tvalidation_1-rmse:1681.34353                    \n",
      "[7]\tvalidation_0-rmse:833.56225\tvalidation_1-rmse:1681.34199                    \n",
      "[8]\tvalidation_0-rmse:833.56150\tvalidation_1-rmse:1681.34045                    \n",
      "[9]\tvalidation_0-rmse:833.55465\tvalidation_1-rmse:1681.32729                    \n",
      "[10]\tvalidation_0-rmse:833.55386\tvalidation_1-rmse:1681.32575                   \n",
      "[11]\tvalidation_0-rmse:833.55310\tvalidation_1-rmse:1681.32425                   \n",
      "[12]\tvalidation_0-rmse:833.55237\tvalidation_1-rmse:1681.32272                   \n",
      "[13]\tvalidation_0-rmse:833.55163\tvalidation_1-rmse:1681.32118                   \n",
      "[14]\tvalidation_0-rmse:833.55086\tvalidation_1-rmse:1681.31953                   \n",
      "[15]\tvalidation_0-rmse:833.55013\tvalidation_1-rmse:1681.31804                   \n",
      "[16]\tvalidation_0-rmse:833.55351\tvalidation_1-rmse:1681.32949                   \n",
      "[17]\tvalidation_0-rmse:833.55278\tvalidation_1-rmse:1681.32798                   \n",
      "[18]\tvalidation_0-rmse:833.55204\tvalidation_1-rmse:1681.32648                   \n",
      "SCORE:                                                                          \n",
      "1681.3180318325308                                                              \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97460\tvalidation_1-rmse:1090.58755                   \n",
      "[5]\tvalidation_0-rmse:1972.51164\tvalidation_1-rmse:1085.36603                   \n",
      "[6]\tvalidation_0-rmse:1959.15751\tvalidation_1-rmse:1076.10636                   \n",
      "[7]\tvalidation_0-rmse:1935.60246\tvalidation_1-rmse:1060.04329                   \n",
      "[8]\tvalidation_0-rmse:1895.08003\tvalidation_1-rmse:1033.26000                   \n",
      "[9]\tvalidation_0-rmse:1828.23231\tvalidation_1-rmse:991.72271                    \n",
      "[10]\tvalidation_0-rmse:1725.07810\tvalidation_1-rmse:934.79816                   \n",
      "[11]\tvalidation_0-rmse:1580.64015\tvalidation_1-rmse:873.13212                   \n",
      "[12]\tvalidation_0-rmse:1402.31201\tvalidation_1-rmse:833.44668                   \n",
      "[13]\tvalidation_0-rmse:1211.25482\tvalidation_1-rmse:844.52537                   \n",
      "[14]\tvalidation_0-rmse:1033.31027\tvalidation_1-rmse:908.76722                   \n",
      "[15]\tvalidation_0-rmse:887.17463\tvalidation_1-rmse:1001.74920                   \n",
      "SCORE:                                                                          \n",
      "833.446682206818                                                                \n",
      "[0]\tvalidation_0-rmse:1988.34334\tvalidation_1-rmse:1096.48203                   \n",
      "[1]\tvalidation_0-rmse:1987.65517\tvalidation_1-rmse:1095.99586                   \n",
      "[2]\tvalidation_0-rmse:1986.40596\tvalidation_1-rmse:1095.11423                   \n",
      "[3]\tvalidation_0-rmse:1984.14418\tvalidation_1-rmse:1093.51993                   \n",
      "[4]\tvalidation_0-rmse:1980.06090\tvalidation_1-rmse:1090.64910                   \n",
      "[5]\tvalidation_0-rmse:1972.73646\tvalidation_1-rmse:1085.52408                   \n",
      "[6]\tvalidation_0-rmse:1959.72377\tvalidation_1-rmse:1076.49761                   \n",
      "[7]\tvalidation_0-rmse:1936.98190\tvalidation_1-rmse:1060.97103                   \n",
      "[8]\tvalidation_0-rmse:1898.29689\tvalidation_1-rmse:1035.32667                   \n",
      "[9]\tvalidation_0-rmse:1835.28982\tvalidation_1-rmse:995.81740                    \n",
      "[10]\tvalidation_0-rmse:1739.24688\tvalidation_1-rmse:941.77156                   \n",
      "[11]\tvalidation_0-rmse:1605.89345\tvalidation_1-rmse:881.74092                   \n",
      "[12]\tvalidation_0-rmse:1441.35990\tvalidation_1-rmse:837.86230                   \n",
      "[13]\tvalidation_0-rmse:1263.44574\tvalidation_1-rmse:836.32153                   \n",
      "[14]\tvalidation_0-rmse:1094.68679\tvalidation_1-rmse:884.50401                   \n",
      "[15]\tvalidation_0-rmse:952.64925\tvalidation_1-rmse:965.12735                    \n",
      "[16]\tvalidation_0-rmse:844.91990\tvalidation_1-rmse:1054.72979                   \n",
      "[17]\tvalidation_0-rmse:770.04261\tvalidation_1-rmse:1137.71386                   \n",
      "SCORE:                                                                          \n",
      "836.3215304368897                                                               \n",
      "[0]\tvalidation_0-rmse:1980.52216\tvalidation_1-rmse:1090.99049                   \n",
      "[1]\tvalidation_0-rmse:1771.79920\tvalidation_1-rmse:958.80157                    \n",
      "[2]\tvalidation_0-rmse:1771.79920\tvalidation_1-rmse:958.80157                    \n",
      "[3]\tvalidation_0-rmse:1771.79920\tvalidation_1-rmse:958.80157                    \n",
      "[4]\tvalidation_0-rmse:1771.79920\tvalidation_1-rmse:958.80157                    \n",
      "SCORE:                                                                          \n",
      "958.8015722195307                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97475\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51215\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15902\tvalidation_1-rmse:1076.10891                   \n",
      "[7]\tvalidation_0-rmse:1935.60746\tvalidation_1-rmse:1060.04868                   \n",
      "[8]\tvalidation_0-rmse:1895.09476\tvalidation_1-rmse:1033.27162                   \n",
      "[9]\tvalidation_0-rmse:1828.27709\tvalidation_1-rmse:991.67812                    \n",
      "[10]\tvalidation_0-rmse:1725.19755\tvalidation_1-rmse:934.75283                   \n",
      "[11]\tvalidation_0-rmse:1580.92808\tvalidation_1-rmse:873.10474                   \n",
      "[12]\tvalidation_0-rmse:1402.92236\tvalidation_1-rmse:833.44610                   \n",
      "[13]\tvalidation_0-rmse:1212.38295\tvalidation_1-rmse:844.52087                   \n",
      "[14]\tvalidation_0-rmse:1035.14174\tvalidation_1-rmse:908.69182                   \n",
      "[15]\tvalidation_0-rmse:889.82992\tvalidation_1-rmse:1001.53338                   \n",
      "SCORE:                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833.4461153032241                                                               \n",
      "[0]\tvalidation_0-rmse:1980.54473\tvalidation_1-rmse:1091.00885                   \n",
      "[1]\tvalidation_0-rmse:1770.91307\tvalidation_1-rmse:958.31440                    \n",
      "[2]\tvalidation_0-rmse:1770.91307\tvalidation_1-rmse:958.31440                    \n",
      "[3]\tvalidation_0-rmse:1770.91307\tvalidation_1-rmse:958.31440                    \n",
      "[4]\tvalidation_0-rmse:1770.91307\tvalidation_1-rmse:958.31440                    \n",
      "SCORE:                                                                          \n",
      "958.3143735557463                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97473\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51210\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15890\tvalidation_1-rmse:1076.10884                   \n",
      "[7]\tvalidation_0-rmse:1935.60675\tvalidation_1-rmse:1060.04831                   \n",
      "[8]\tvalidation_0-rmse:1895.09237\tvalidation_1-rmse:1033.27045                   \n",
      "[9]\tvalidation_0-rmse:1828.27010\tvalidation_1-rmse:991.67485                    \n",
      "[10]\tvalidation_0-rmse:1725.17882\tvalidation_1-rmse:934.74546                   \n",
      "[11]\tvalidation_0-rmse:1580.88360\tvalidation_1-rmse:873.09297                   \n",
      "[12]\tvalidation_0-rmse:1402.83090\tvalidation_1-rmse:833.43973                   \n",
      "[13]\tvalidation_0-rmse:1212.22207\tvalidation_1-rmse:844.54506                   \n",
      "[14]\tvalidation_0-rmse:1034.89709\tvalidation_1-rmse:908.77283                   \n",
      "[15]\tvalidation_0-rmse:889.50010\tvalidation_1-rmse:1001.68201                   \n",
      "[16]\tvalidation_0-rmse:782.19787\tvalidation_1-rmse:1095.41130                   \n",
      "SCORE:                                                                          \n",
      "833.4397374608976                                                               \n",
      "[0]\tvalidation_0-rmse:1494.38188\tvalidation_1-rmse:846.80625                    \n",
      "[1]\tvalidation_0-rmse:1178.75561\tvalidation_1-rmse:859.47753                    \n",
      "[2]\tvalidation_0-rmse:987.94858\tvalidation_1-rmse:964.88708                     \n",
      "[3]\tvalidation_0-rmse:879.46658\tvalidation_1-rmse:1073.52719                    \n",
      "[4]\tvalidation_0-rmse:620.83165\tvalidation_1-rmse:1241.81459                    \n",
      "SCORE:                                                                          \n",
      "846.8062526879719                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97473\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51210\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15890\tvalidation_1-rmse:1076.10884                   \n",
      "[7]\tvalidation_0-rmse:1935.60675\tvalidation_1-rmse:1060.04831                   \n",
      "[8]\tvalidation_0-rmse:1895.09237\tvalidation_1-rmse:1033.27045                   \n",
      "[9]\tvalidation_0-rmse:1828.27010\tvalidation_1-rmse:991.67485                    \n",
      "[10]\tvalidation_0-rmse:1725.17882\tvalidation_1-rmse:934.74546                   \n",
      "[11]\tvalidation_0-rmse:1580.88360\tvalidation_1-rmse:873.09297                   \n",
      "[12]\tvalidation_0-rmse:1402.83090\tvalidation_1-rmse:833.43973                   \n",
      "[13]\tvalidation_0-rmse:1212.22207\tvalidation_1-rmse:844.54506                   \n",
      "[14]\tvalidation_0-rmse:1034.89709\tvalidation_1-rmse:908.77283                   \n",
      "[15]\tvalidation_0-rmse:889.50010\tvalidation_1-rmse:1001.68201                   \n",
      "[16]\tvalidation_0-rmse:782.19787\tvalidation_1-rmse:1095.41130                   \n",
      "SCORE:                                                                          \n",
      "833.4397374608976                                                               \n",
      "[0]\tvalidation_0-rmse:833.71277\tvalidation_1-rmse:1681.60810                    \n",
      "[1]\tvalidation_0-rmse:833.71286\tvalidation_1-rmse:1681.60831                    \n",
      "[2]\tvalidation_0-rmse:833.71297\tvalidation_1-rmse:1681.60851                    \n",
      "[3]\tvalidation_0-rmse:833.71300\tvalidation_1-rmse:1681.60854                    \n",
      "[4]\tvalidation_0-rmse:833.71300\tvalidation_1-rmse:1681.60854                    \n",
      "SCORE:                                                                          \n",
      "1681.6080745896531                                                              \n",
      "[0]\tvalidation_0-rmse:1988.36160\tvalidation_1-rmse:1096.49488                   \n",
      "[1]\tvalidation_0-rmse:1987.90939\tvalidation_1-rmse:1096.17551                   \n",
      "[2]\tvalidation_0-rmse:1987.34348\tvalidation_1-rmse:1095.77583                   \n",
      "[3]\tvalidation_0-rmse:1986.63620\tvalidation_1-rmse:1095.27663                   \n",
      "[4]\tvalidation_0-rmse:1985.74465\tvalidation_1-rmse:1094.64749                   \n",
      "[5]\tvalidation_0-rmse:1984.68879\tvalidation_1-rmse:1093.90340                   \n",
      "[6]\tvalidation_0-rmse:1983.35361\tvalidation_1-rmse:1092.96324                   \n",
      "[7]\tvalidation_0-rmse:1981.69798\tvalidation_1-rmse:1091.79884                   \n",
      "[8]\tvalidation_0-rmse:1979.65337\tvalidation_1-rmse:1090.36315                   \n",
      "[9]\tvalidation_0-rmse:1977.13983\tvalidation_1-rmse:1088.60120                   \n",
      "[10]\tvalidation_0-rmse:1974.06069\tvalidation_1-rmse:1086.44776                  \n",
      "[11]\tvalidation_0-rmse:1970.30469\tvalidation_1-rmse:1083.82831                  \n",
      "[12]\tvalidation_0-rmse:1965.74481\tvalidation_1-rmse:1080.65909                  \n",
      "[13]\tvalidation_0-rmse:1960.23806\tvalidation_1-rmse:1076.84790                  \n",
      "[14]\tvalidation_0-rmse:1953.62711\tvalidation_1-rmse:1072.29605                  \n",
      "[15]\tvalidation_0-rmse:1945.74344\tvalidation_1-rmse:1066.90208                  \n",
      "[16]\tvalidation_0-rmse:1936.41495\tvalidation_1-rmse:1060.56832                  \n",
      "[17]\tvalidation_0-rmse:1936.41495\tvalidation_1-rmse:1060.56832                  \n",
      "[18]\tvalidation_0-rmse:1936.41495\tvalidation_1-rmse:1060.56832                  \n",
      "[19]\tvalidation_0-rmse:1936.41495\tvalidation_1-rmse:1060.56832                  \n",
      "SCORE:                                                                          \n",
      "1060.56830937725                                                                \n",
      "[0]\tvalidation_0-rmse:1980.55398\tvalidation_1-rmse:1090.99513                   \n",
      "[1]\tvalidation_0-rmse:1776.57172\tvalidation_1-rmse:961.43363                    \n",
      "[2]\tvalidation_0-rmse:1776.57172\tvalidation_1-rmse:961.43363                    \n",
      "[3]\tvalidation_0-rmse:1776.57172\tvalidation_1-rmse:961.43363                    \n",
      "[4]\tvalidation_0-rmse:1776.57172\tvalidation_1-rmse:961.43363                    \n",
      "SCORE:                                                                          \n",
      "961.433632046642                                                                \n",
      "[0]\tvalidation_0-rmse:1988.34334\tvalidation_1-rmse:1096.48203                   \n",
      "[1]\tvalidation_0-rmse:1987.65517\tvalidation_1-rmse:1095.99586                   \n",
      "[2]\tvalidation_0-rmse:1986.40615\tvalidation_1-rmse:1095.11426                   \n",
      "[3]\tvalidation_0-rmse:1984.14419\tvalidation_1-rmse:1093.51998                   \n",
      "[4]\tvalidation_0-rmse:1980.06092\tvalidation_1-rmse:1090.64915                   \n",
      "[5]\tvalidation_0-rmse:1972.73648\tvalidation_1-rmse:1085.52414                   \n",
      "[6]\tvalidation_0-rmse:1959.72399\tvalidation_1-rmse:1076.49777                   \n",
      "[7]\tvalidation_0-rmse:1936.98230\tvalidation_1-rmse:1060.97140                   \n",
      "[8]\tvalidation_0-rmse:1898.29791\tvalidation_1-rmse:1035.32741                   \n",
      "[9]\tvalidation_0-rmse:1835.29139\tvalidation_1-rmse:995.81888                    \n",
      "[10]\tvalidation_0-rmse:1739.24869\tvalidation_1-rmse:941.77384                   \n",
      "[11]\tvalidation_0-rmse:1605.89328\tvalidation_1-rmse:881.74346                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\tvalidation_0-rmse:1441.35293\tvalidation_1-rmse:837.86371                   \n",
      "[13]\tvalidation_0-rmse:1263.42219\tvalidation_1-rmse:836.32008                   \n",
      "[14]\tvalidation_0-rmse:1094.63483\tvalidation_1-rmse:884.49966                   \n",
      "[15]\tvalidation_0-rmse:952.55795\tvalidation_1-rmse:965.12121                    \n",
      "[16]\tvalidation_0-rmse:844.78186\tvalidation_1-rmse:1054.72340                   \n",
      "SCORE:                                                                          \n",
      "836.3200869656088                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34291\tvalidation_1-rmse:1096.48177                   \n",
      "[1]\tvalidation_0-rmse:1987.65338\tvalidation_1-rmse:1095.99465                   \n",
      "[2]\tvalidation_0-rmse:1986.40054\tvalidation_1-rmse:1095.11041                   \n",
      "[3]\tvalidation_0-rmse:1984.12862\tvalidation_1-rmse:1093.50897                   \n",
      "[4]\tvalidation_0-rmse:1980.02114\tvalidation_1-rmse:1090.62109                   \n",
      "[5]\tvalidation_0-rmse:1972.63547\tvalidation_1-rmse:1085.45232                   \n",
      "[6]\tvalidation_0-rmse:1959.47884\tvalidation_1-rmse:1076.32238                   \n",
      "[7]\tvalidation_0-rmse:1936.41639\tvalidation_1-rmse:1060.56636                   \n",
      "[8]\tvalidation_0-rmse:1897.08130\tvalidation_1-rmse:1034.46376                   \n",
      "[9]\tvalidation_0-rmse:1832.94590\tvalidation_1-rmse:994.19381                    \n",
      "[10]\tvalidation_0-rmse:1735.46798\tvalidation_1-rmse:939.29307                   \n",
      "[11]\tvalidation_0-rmse:1601.52903\tvalidation_1-rmse:879.14443                   \n",
      "[12]\tvalidation_0-rmse:1439.82439\tvalidation_1-rmse:836.76092                   \n",
      "[13]\tvalidation_0-rmse:1271.03237\tvalidation_1-rmse:836.99770                   \n",
      "[14]\tvalidation_0-rmse:1118.51188\tvalidation_1-rmse:883.60653                   \n",
      "[15]\tvalidation_0-rmse:997.44442\tvalidation_1-rmse:957.46823                    \n",
      "SCORE:                                                                          \n",
      "836.7609330883218                                                               \n",
      "[0]\tvalidation_0-rmse:1494.38188\tvalidation_1-rmse:846.80625                    \n",
      "[1]\tvalidation_0-rmse:1178.75561\tvalidation_1-rmse:859.47753                    \n",
      "[2]\tvalidation_0-rmse:987.94858\tvalidation_1-rmse:964.88708                     \n",
      "[3]\tvalidation_0-rmse:879.46658\tvalidation_1-rmse:1073.52719                    \n",
      "[4]\tvalidation_0-rmse:616.25587\tvalidation_1-rmse:1222.31474                    \n",
      "SCORE:                                                                          \n",
      "846.8062526879719                                                               \n",
      "[0]\tvalidation_0-rmse:1988.36162\tvalidation_1-rmse:1096.49494                   \n",
      "[1]\tvalidation_0-rmse:1987.90983\tvalidation_1-rmse:1096.17576                   \n",
      "[2]\tvalidation_0-rmse:1987.34438\tvalidation_1-rmse:1095.77647                   \n",
      "[3]\tvalidation_0-rmse:1986.63848\tvalidation_1-rmse:1095.27828                   \n",
      "[4]\tvalidation_0-rmse:1985.75015\tvalidation_1-rmse:1094.65146                   \n",
      "[5]\tvalidation_0-rmse:1984.66768\tvalidation_1-rmse:1093.88855                   \n",
      "[6]\tvalidation_0-rmse:1983.33007\tvalidation_1-rmse:1092.94668                   \n",
      "[7]\tvalidation_0-rmse:1981.68738\tvalidation_1-rmse:1091.79146                   \n",
      "[8]\tvalidation_0-rmse:1979.68653\tvalidation_1-rmse:1090.38640                   \n",
      "[9]\tvalidation_0-rmse:1977.27584\tvalidation_1-rmse:1088.69642                   \n",
      "[10]\tvalidation_0-rmse:1974.41304\tvalidation_1-rmse:1086.69383                  \n",
      "[11]\tvalidation_0-rmse:1971.07821\tvalidation_1-rmse:1084.36698                  \n",
      "[12]\tvalidation_0-rmse:1967.28814\tvalidation_1-rmse:1081.73014                  \n",
      "[13]\tvalidation_0-rmse:1963.10459\tvalidation_1-rmse:1078.82925                  \n",
      "[14]\tvalidation_0-rmse:1958.63025\tvalidation_1-rmse:1075.73805                  \n",
      "[15]\tvalidation_0-rmse:1953.99039\tvalidation_1-rmse:1072.54491                  \n",
      "[16]\tvalidation_0-rmse:1949.30633\tvalidation_1-rmse:1069.33448                  \n",
      "[17]\tvalidation_0-rmse:1944.67596\tvalidation_1-rmse:1066.17382                  \n",
      "[18]\tvalidation_0-rmse:1940.16617\tvalidation_1-rmse:1063.10799                  \n",
      "[19]\tvalidation_0-rmse:1940.16617\tvalidation_1-rmse:1063.10799                  \n",
      "[20]\tvalidation_0-rmse:1940.16617\tvalidation_1-rmse:1063.10799                  \n",
      "[21]\tvalidation_0-rmse:1940.16617\tvalidation_1-rmse:1063.10799                  \n",
      "SCORE:                                                                          \n",
      "1063.1079692002684                                                              \n",
      "[09:49:41] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:833.72641\tvalidation_1-rmse:1681.94676                    \n",
      "[1]\tvalidation_0-rmse:833.73862\tvalidation_1-rmse:1682.27829                    \n",
      "[2]\tvalidation_0-rmse:833.75096\tvalidation_1-rmse:1682.61005                    \n",
      "[3]\tvalidation_0-rmse:833.76323\tvalidation_1-rmse:1682.94160                    \n",
      "[4]\tvalidation_0-rmse:833.77561\tvalidation_1-rmse:1683.27337                    \n",
      "SCORE:                                                                          \n",
      "1683.2733598157445                                                              \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48148                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97832\tvalidation_1-rmse:1090.59103                   \n",
      "[5]\tvalidation_0-rmse:1972.52373\tvalidation_1-rmse:1085.37436                   \n",
      "[6]\tvalidation_0-rmse:1959.19687\tvalidation_1-rmse:1076.12783                   \n",
      "[7]\tvalidation_0-rmse:1935.72799\tvalidation_1-rmse:1060.06530                   \n",
      "[8]\tvalidation_0-rmse:1895.46826\tvalidation_1-rmse:1033.35226                   \n",
      "[9]\tvalidation_0-rmse:1829.38072\tvalidation_1-rmse:991.93767                    \n",
      "[10]\tvalidation_0-rmse:1728.22070\tvalidation_1-rmse:935.37518                   \n",
      "[11]\tvalidation_0-rmse:1588.37535\tvalidation_1-rmse:874.08837                   \n",
      "[12]\tvalidation_0-rmse:1418.98470\tvalidation_1-rmse:833.85558                   \n",
      "[13]\tvalidation_0-rmse:1242.53695\tvalidation_1-rmse:841.60939                   \n",
      "[14]\tvalidation_0-rmse:1084.62094\tvalidation_1-rmse:899.83721                   \n",
      "[15]\tvalidation_0-rmse:961.80711\tvalidation_1-rmse:985.73775                    \n",
      "SCORE:                                                                          \n",
      "833.8555734711794                                                               \n",
      "[09:49:41] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1325.86950\tvalidation_1-rmse:7995.63577                   \n",
      "[1]\tvalidation_0-rmse:1450.23917\tvalidation_1-rmse:3082.86519                   \n",
      "[2]\tvalidation_0-rmse:50007819.39489\tvalidation_1-rmse:88181694.71504           \n",
      "[3]\tvalidation_0-rmse:inf\tvalidation_1-rmse:inf                                 \n",
      "[4]\tvalidation_0-rmse:nan\tvalidation_1-rmse:nan                                 \n",
      "[5]\tvalidation_0-rmse:nan\tvalidation_1-rmse:nan                                 \n",
      "SCORE:                                                                          \n",
      "100000                                                                          \n",
      "[0]\tvalidation_0-rmse:1988.34291\tvalidation_1-rmse:1096.48177                   \n",
      "[1]\tvalidation_0-rmse:1987.65338\tvalidation_1-rmse:1095.99465                   \n",
      "[2]\tvalidation_0-rmse:1986.40054\tvalidation_1-rmse:1095.11041                   \n",
      "[3]\tvalidation_0-rmse:1984.12862\tvalidation_1-rmse:1093.50897                   \n",
      "[4]\tvalidation_0-rmse:1980.01898\tvalidation_1-rmse:1090.61960                   \n",
      "[5]\tvalidation_0-rmse:1972.62791\tvalidation_1-rmse:1085.44832                   \n",
      "[6]\tvalidation_0-rmse:1959.45373\tvalidation_1-rmse:1076.31128                   \n",
      "[7]\tvalidation_0-rmse:1936.33652\tvalidation_1-rmse:1060.53500                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\tvalidation_0-rmse:1896.83481\tvalidation_1-rmse:1034.37672                   \n",
      "[9]\tvalidation_0-rmse:1832.22332\tvalidation_1-rmse:993.97025                    \n",
      "[10]\tvalidation_0-rmse:1733.50779\tvalidation_1-rmse:938.80330                   \n",
      "[11]\tvalidation_0-rmse:1596.74745\tvalidation_1-rmse:878.35758                   \n",
      "[12]\tvalidation_0-rmse:1429.58245\tvalidation_1-rmse:836.19653                   \n",
      "[13]\tvalidation_0-rmse:1251.94360\tvalidation_1-rmse:838.03697                   \n",
      "[14]\tvalidation_0-rmse:1087.37001\tvalidation_1-rmse:887.77298                   \n",
      "[15]\tvalidation_0-rmse:952.31473\tvalidation_1-rmse:965.45734                    \n",
      "SCORE:                                                                          \n",
      "836.1965394926938                                                               \n",
      "[0]\tvalidation_0-rmse:1988.36160\tvalidation_1-rmse:1096.49488                   \n",
      "[1]\tvalidation_0-rmse:1987.90960\tvalidation_1-rmse:1096.17561                   \n",
      "[2]\tvalidation_0-rmse:1987.34371\tvalidation_1-rmse:1095.77599                   \n",
      "[3]\tvalidation_0-rmse:1986.63665\tvalidation_1-rmse:1095.27694                   \n",
      "[4]\tvalidation_0-rmse:1985.75557\tvalidation_1-rmse:1094.65550                   \n",
      "[5]\tvalidation_0-rmse:1984.66032\tvalidation_1-rmse:1093.88354                   \n",
      "[6]\tvalidation_0-rmse:1983.30227\tvalidation_1-rmse:1092.92722                   \n",
      "[7]\tvalidation_0-rmse:1981.62337\tvalidation_1-rmse:1091.74643                   \n",
      "[8]\tvalidation_0-rmse:1979.55504\tvalidation_1-rmse:1090.29403                   \n",
      "[9]\tvalidation_0-rmse:1977.01789\tvalidation_1-rmse:1088.51558                   \n",
      "[10]\tvalidation_0-rmse:1973.92104\tvalidation_1-rmse:1086.34978                  \n",
      "[11]\tvalidation_0-rmse:1970.16449\tvalidation_1-rmse:1083.72992                  \n",
      "[12]\tvalidation_0-rmse:1965.64318\tvalidation_1-rmse:1080.58764                  \n",
      "[13]\tvalidation_0-rmse:1960.25601\tvalidation_1-rmse:1076.85900                  \n",
      "[14]\tvalidation_0-rmse:1953.91818\tvalidation_1-rmse:1072.49417                  \n",
      "[15]\tvalidation_0-rmse:1946.58152\tvalidation_1-rmse:1067.47145                  \n",
      "[16]\tvalidation_0-rmse:1938.25845\tvalidation_1-rmse:1061.81304                  \n",
      "[17]\tvalidation_0-rmse:1938.25845\tvalidation_1-rmse:1061.81304                  \n",
      "[18]\tvalidation_0-rmse:1938.25845\tvalidation_1-rmse:1061.81304                  \n",
      "[19]\tvalidation_0-rmse:1938.25845\tvalidation_1-rmse:1061.81304                  \n",
      "SCORE:                                                                          \n",
      "1061.8130144690174                                                              \n",
      "[0]\tvalidation_0-rmse:1988.34334\tvalidation_1-rmse:1096.48203                   \n",
      "[1]\tvalidation_0-rmse:1987.65517\tvalidation_1-rmse:1095.99586                   \n",
      "[2]\tvalidation_0-rmse:1986.40596\tvalidation_1-rmse:1095.11423                   \n",
      "[3]\tvalidation_0-rmse:1984.14418\tvalidation_1-rmse:1093.51993                   \n",
      "[4]\tvalidation_0-rmse:1980.06090\tvalidation_1-rmse:1090.64909                   \n",
      "[5]\tvalidation_0-rmse:1972.73646\tvalidation_1-rmse:1085.52408                   \n",
      "[6]\tvalidation_0-rmse:1959.72376\tvalidation_1-rmse:1076.49761                   \n",
      "[7]\tvalidation_0-rmse:1936.98162\tvalidation_1-rmse:1060.97095                   \n",
      "[8]\tvalidation_0-rmse:1898.29638\tvalidation_1-rmse:1035.32653                   \n",
      "[9]\tvalidation_0-rmse:1835.28820\tvalidation_1-rmse:995.81699                    \n",
      "[10]\tvalidation_0-rmse:1739.24211\tvalidation_1-rmse:941.77052                   \n",
      "[11]\tvalidation_0-rmse:1605.88116\tvalidation_1-rmse:881.73907                   \n",
      "[12]\tvalidation_0-rmse:1441.33263\tvalidation_1-rmse:837.86081                   \n",
      "[13]\tvalidation_0-rmse:1263.39276\tvalidation_1-rmse:836.32388                   \n",
      "[14]\tvalidation_0-rmse:1094.59652\tvalidation_1-rmse:884.51469                   \n",
      "[15]\tvalidation_0-rmse:952.51299\tvalidation_1-rmse:965.14881                    \n",
      "[16]\tvalidation_0-rmse:844.73336\tvalidation_1-rmse:1054.76228                   \n",
      "SCORE:                                                                          \n",
      "836.3238844655742                                                               \n",
      "[09:49:43] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:619.66729\tvalidation_1-rmse:2254.24697                    \n",
      "[1]\tvalidation_0-rmse:609.47683\tvalidation_1-rmse:2439.20995                    \n",
      "[2]\tvalidation_0-rmse:604.41271\tvalidation_1-rmse:2533.22861                    \n",
      "[3]\tvalidation_0-rmse:601.19162\tvalidation_1-rmse:2596.47037                    \n",
      "SCORE:                                                                          \n",
      "2640.8849607974325                                                              \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39514\tvalidation_1-rmse:1095.10659                   \n",
      "[3]\tvalidation_0-rmse:1984.11306\tvalidation_1-rmse:1093.49801                   \n",
      "[4]\tvalidation_0-rmse:1979.97589\tvalidation_1-rmse:1090.58986                   \n",
      "[5]\tvalidation_0-rmse:1972.51532\tvalidation_1-rmse:1085.37091                   \n",
      "[6]\tvalidation_0-rmse:1959.16861\tvalidation_1-rmse:1076.11707                   \n",
      "[7]\tvalidation_0-rmse:1935.63579\tvalidation_1-rmse:1060.06779                   \n",
      "[8]\tvalidation_0-rmse:1895.17728\tvalidation_1-rmse:1033.31663                   \n",
      "[9]\tvalidation_0-rmse:1828.50853\tvalidation_1-rmse:991.77976                    \n",
      "[10]\tvalidation_0-rmse:1725.80530\tvalidation_1-rmse:934.95403                   \n",
      "[11]\tvalidation_0-rmse:1582.37799\tvalidation_1-rmse:873.39682                   \n",
      "[12]\tvalidation_0-rmse:1405.98407\tvalidation_1-rmse:833.59610                   \n",
      "[13]\tvalidation_0-rmse:1218.04890\tvalidation_1-rmse:843.98797                   \n",
      "[14]\tvalidation_0-rmse:1044.38324\tvalidation_1-rmse:906.94887                   \n",
      "[15]\tvalidation_0-rmse:903.29076\tvalidation_1-rmse:998.39759                    \n",
      "SCORE:                                                                          \n",
      "833.5961004323677                                                               \n",
      "[0]\tvalidation_0-rmse:833.71254\tvalidation_1-rmse:1681.60758                    \n",
      "[1]\tvalidation_0-rmse:833.71282\tvalidation_1-rmse:1681.60813                    \n",
      "[2]\tvalidation_0-rmse:833.71308\tvalidation_1-rmse:1681.60869                    \n",
      "[3]\tvalidation_0-rmse:833.71301\tvalidation_1-rmse:1681.60852                    \n",
      "[4]\tvalidation_0-rmse:833.63848\tvalidation_1-rmse:1681.60025                    \n",
      "[5]\tvalidation_0-rmse:833.63788\tvalidation_1-rmse:1681.59899                    \n",
      "[6]\tvalidation_0-rmse:833.63720\tvalidation_1-rmse:1681.59754                    \n",
      "[7]\tvalidation_0-rmse:833.63624\tvalidation_1-rmse:1681.59553                    \n",
      "[8]\tvalidation_0-rmse:833.63593\tvalidation_1-rmse:1681.59482                    \n",
      "[9]\tvalidation_0-rmse:833.63065\tvalidation_1-rmse:1681.58401                    \n",
      "[10]\tvalidation_0-rmse:833.62969\tvalidation_1-rmse:1681.58200                   \n",
      "[11]\tvalidation_0-rmse:833.62884\tvalidation_1-rmse:1681.58019                   \n",
      "[12]\tvalidation_0-rmse:833.62815\tvalidation_1-rmse:1681.57872                   \n",
      "[13]\tvalidation_0-rmse:833.62782\tvalidation_1-rmse:1681.57807                   \n",
      "[14]\tvalidation_0-rmse:833.62723\tvalidation_1-rmse:1681.57678                   \n",
      "[15]\tvalidation_0-rmse:833.62654\tvalidation_1-rmse:1681.57532                   \n",
      "[16]\tvalidation_0-rmse:833.63045\tvalidation_1-rmse:1681.58485                   \n",
      "[17]\tvalidation_0-rmse:833.62986\tvalidation_1-rmse:1681.58365                   \n",
      "[18]\tvalidation_0-rmse:833.62927\tvalidation_1-rmse:1681.58239                   \n",
      "SCORE:                                                                          \n",
      "1681.5753079085841                                                              \n",
      "[09:49:44] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1987.59591\tvalidation_1-rmse:1095.72167                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-rmse:1985.59593\tvalidation_1-rmse:1093.80041                   \n",
      "[2]\tvalidation_0-rmse:1982.15622\tvalidation_1-rmse:1090.26760                   \n",
      "[3]\tvalidation_0-rmse:1976.29830\tvalidation_1-rmse:1084.26250                   \n",
      "[4]\tvalidation_0-rmse:1966.82730\tvalidation_1-rmse:1074.55588                   \n",
      "[5]\tvalidation_0-rmse:1951.14589\tvalidation_1-rmse:1058.75782                   \n",
      "[6]\tvalidation_0-rmse:1925.48305\tvalidation_1-rmse:1033.88634                   \n",
      "[7]\tvalidation_0-rmse:1884.55647\tvalidation_1-rmse:997.30912                    \n",
      "[8]\tvalidation_0-rmse:1821.28722\tvalidation_1-rmse:950.12685                    \n",
      "[9]\tvalidation_0-rmse:1727.47849\tvalidation_1-rmse:907.31642                    \n",
      "[10]\tvalidation_0-rmse:1596.44342\tvalidation_1-rmse:917.89386                   \n",
      "[11]\tvalidation_0-rmse:1429.09825\tvalidation_1-rmse:1069.88899                  \n",
      "[12]\tvalidation_0-rmse:1242.47114\tvalidation_1-rmse:1423.50811                  \n",
      "SCORE:                                                                          \n",
      "1953.5607720301105                                                              \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97832\tvalidation_1-rmse:1090.59103                   \n",
      "[5]\tvalidation_0-rmse:1972.52374\tvalidation_1-rmse:1085.37437                   \n",
      "[6]\tvalidation_0-rmse:1959.19694\tvalidation_1-rmse:1076.12787                   \n",
      "[7]\tvalidation_0-rmse:1935.72810\tvalidation_1-rmse:1060.10116                   \n",
      "[8]\tvalidation_0-rmse:1895.46868\tvalidation_1-rmse:1033.41532                   \n",
      "[9]\tvalidation_0-rmse:1829.38146\tvalidation_1-rmse:992.04579                    \n",
      "[10]\tvalidation_0-rmse:1728.22245\tvalidation_1-rmse:935.55444                   \n",
      "[11]\tvalidation_0-rmse:1588.37855\tvalidation_1-rmse:874.35675                   \n",
      "[12]\tvalidation_0-rmse:1418.99091\tvalidation_1-rmse:834.20785                   \n",
      "[13]\tvalidation_0-rmse:1242.54570\tvalidation_1-rmse:842.00018                   \n",
      "[14]\tvalidation_0-rmse:1084.63314\tvalidation_1-rmse:900.19939                   \n",
      "[15]\tvalidation_0-rmse:961.82217\tvalidation_1-rmse:986.04251                    \n",
      "SCORE:                                                                          \n",
      "834.2078597072791                                                               \n",
      "[0]\tvalidation_0-rmse:1982.33192\tvalidation_1-rmse:1092.24458                   \n",
      "[1]\tvalidation_0-rmse:1921.01041\tvalidation_1-rmse:1050.22396                   \n",
      "[2]\tvalidation_0-rmse:1798.32303\tvalidation_1-rmse:973.76017                    \n",
      "[3]\tvalidation_0-rmse:1798.32303\tvalidation_1-rmse:973.76017                    \n",
      "[4]\tvalidation_0-rmse:1798.32303\tvalidation_1-rmse:973.76017                    \n",
      "[5]\tvalidation_0-rmse:1798.32303\tvalidation_1-rmse:973.76017                    \n",
      "[6]\tvalidation_0-rmse:1798.32303\tvalidation_1-rmse:973.76017                    \n",
      "SCORE:                                                                          \n",
      "973.7601668058268                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34291\tvalidation_1-rmse:1096.48177                   \n",
      "[1]\tvalidation_0-rmse:1987.65338\tvalidation_1-rmse:1095.99465                   \n",
      "[2]\tvalidation_0-rmse:1986.40054\tvalidation_1-rmse:1095.11041                   \n",
      "[3]\tvalidation_0-rmse:1984.12862\tvalidation_1-rmse:1093.50897                   \n",
      "[4]\tvalidation_0-rmse:1980.01898\tvalidation_1-rmse:1090.61960                   \n",
      "[5]\tvalidation_0-rmse:1972.62791\tvalidation_1-rmse:1085.44832                   \n",
      "[6]\tvalidation_0-rmse:1959.45373\tvalidation_1-rmse:1076.31128                   \n",
      "[7]\tvalidation_0-rmse:1936.33652\tvalidation_1-rmse:1060.53500                   \n",
      "[8]\tvalidation_0-rmse:1896.83481\tvalidation_1-rmse:1034.37672                   \n",
      "[9]\tvalidation_0-rmse:1832.22332\tvalidation_1-rmse:993.97025                    \n",
      "[10]\tvalidation_0-rmse:1733.50779\tvalidation_1-rmse:938.80330                   \n",
      "[11]\tvalidation_0-rmse:1596.74745\tvalidation_1-rmse:878.35758                   \n",
      "[12]\tvalidation_0-rmse:1429.58245\tvalidation_1-rmse:836.19653                   \n",
      "[13]\tvalidation_0-rmse:1251.94360\tvalidation_1-rmse:838.03697                   \n",
      "[14]\tvalidation_0-rmse:1087.37001\tvalidation_1-rmse:887.77298                   \n",
      "[15]\tvalidation_0-rmse:952.31473\tvalidation_1-rmse:965.45734                    \n",
      "[16]\tvalidation_0-rmse:852.01287\tvalidation_1-rmse:1048.07931                   \n",
      "SCORE:                                                                          \n",
      "836.1965394926938                                                               \n",
      "[09:49:46] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:447.37082\tvalidation_1-rmse:2019.95643                    \n",
      "[1]\tvalidation_0-rmse:335.08998\tvalidation_1-rmse:2121.46009                    \n",
      "[2]\tvalidation_0-rmse:286.12712\tvalidation_1-rmse:2192.64824                    \n",
      "[3]\tvalidation_0-rmse:265.09514\tvalidation_1-rmse:2236.13578                    \n",
      "SCORE:                                                                          \n",
      "2263.261986004915                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34267\tvalidation_1-rmse:1096.48154                   \n",
      "[1]\tvalidation_0-rmse:1987.65202\tvalidation_1-rmse:1095.99370                   \n",
      "[2]\tvalidation_0-rmse:1986.39625\tvalidation_1-rmse:1095.10732                   \n",
      "[3]\tvalidation_0-rmse:1984.11599\tvalidation_1-rmse:1093.50008                   \n",
      "[4]\tvalidation_0-rmse:1979.98431\tvalidation_1-rmse:1090.59552                   \n",
      "[5]\tvalidation_0-rmse:1972.53775\tvalidation_1-rmse:1085.38577                   \n",
      "[6]\tvalidation_0-rmse:1959.22559\tvalidation_1-rmse:1076.15472                   \n",
      "[7]\tvalidation_0-rmse:1935.77633\tvalidation_1-rmse:1060.15896                   \n",
      "[8]\tvalidation_0-rmse:1895.51145\tvalidation_1-rmse:1033.52417                   \n",
      "[9]\tvalidation_0-rmse:1829.25984\tvalidation_1-rmse:992.20747                    \n",
      "[10]\tvalidation_0-rmse:1727.36016\tvalidation_1-rmse:935.69451                   \n",
      "[11]\tvalidation_0-rmse:1585.24799\tvalidation_1-rmse:874.31284                   \n",
      "[12]\tvalidation_0-rmse:1410.60241\tvalidation_1-rmse:834.00451                   \n",
      "[13]\tvalidation_0-rmse:1224.51142\tvalidation_1-rmse:842.87022                   \n",
      "[14]\tvalidation_0-rmse:1052.40049\tvalidation_1-rmse:903.84901                   \n",
      "[15]\tvalidation_0-rmse:912.37962\tvalidation_1-rmse:993.68191                    \n",
      "[16]\tvalidation_0-rmse:810.35926\tvalidation_1-rmse:1087.07869                   \n",
      "SCORE:                                                                          \n",
      "834.0045112608394                                                               \n",
      "[0]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "[1]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "[2]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "[3]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "[4]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "SCORE:                                                                          \n",
      "1681.6089315225274                                                              \n",
      "[0]\tvalidation_0-rmse:1988.34334\tvalidation_1-rmse:1096.48203                   \n",
      "[1]\tvalidation_0-rmse:1987.65517\tvalidation_1-rmse:1095.99586                   \n",
      "[2]\tvalidation_0-rmse:1986.40596\tvalidation_1-rmse:1095.11423                   \n",
      "[3]\tvalidation_0-rmse:1984.14418\tvalidation_1-rmse:1093.51993                   \n",
      "[4]\tvalidation_0-rmse:1980.06090\tvalidation_1-rmse:1090.64910                   \n",
      "[5]\tvalidation_0-rmse:1972.73646\tvalidation_1-rmse:1085.52408                   \n",
      "[6]\tvalidation_0-rmse:1959.72377\tvalidation_1-rmse:1076.49761                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\tvalidation_0-rmse:1936.98190\tvalidation_1-rmse:1060.97103                   \n",
      "[8]\tvalidation_0-rmse:1898.29689\tvalidation_1-rmse:1035.32667                   \n",
      "[9]\tvalidation_0-rmse:1835.28982\tvalidation_1-rmse:995.81740                    \n",
      "[10]\tvalidation_0-rmse:1739.24688\tvalidation_1-rmse:941.77156                   \n",
      "[11]\tvalidation_0-rmse:1605.89345\tvalidation_1-rmse:881.74092                   \n",
      "[12]\tvalidation_0-rmse:1441.35990\tvalidation_1-rmse:837.86230                   \n",
      "[13]\tvalidation_0-rmse:1263.44574\tvalidation_1-rmse:836.32153                   \n",
      "[14]\tvalidation_0-rmse:1094.68679\tvalidation_1-rmse:884.50401                   \n",
      "[15]\tvalidation_0-rmse:952.64925\tvalidation_1-rmse:965.12735                    \n",
      "[16]\tvalidation_0-rmse:844.91990\tvalidation_1-rmse:1054.72979                   \n",
      "[17]\tvalidation_0-rmse:770.04261\tvalidation_1-rmse:1137.71386                   \n",
      "SCORE:                                                                          \n",
      "836.3215304368897                                                               \n",
      "[0]\tvalidation_0-rmse:1988.36366\tvalidation_1-rmse:1096.49643                   \n",
      "[1]\tvalidation_0-rmse:1987.91661\tvalidation_1-rmse:1096.18061                   \n",
      "[2]\tvalidation_0-rmse:1987.36152\tvalidation_1-rmse:1095.78857                   \n",
      "[3]\tvalidation_0-rmse:1986.67678\tvalidation_1-rmse:1095.30521                   \n",
      "[4]\tvalidation_0-rmse:1985.83078\tvalidation_1-rmse:1094.70845                   \n",
      "[5]\tvalidation_0-rmse:1984.82475\tvalidation_1-rmse:1093.99923                   \n",
      "[6]\tvalidation_0-rmse:1983.63508\tvalidation_1-rmse:1093.16140                   \n",
      "[7]\tvalidation_0-rmse:1982.26270\tvalidation_1-rmse:1092.19591                   \n",
      "[8]\tvalidation_0-rmse:1980.72684\tvalidation_1-rmse:1091.11667                   \n",
      "[9]\tvalidation_0-rmse:1979.06538\tvalidation_1-rmse:1089.95055                   \n",
      "[10]\tvalidation_0-rmse:1977.32711\tvalidation_1-rmse:1088.73224                  \n",
      "[11]\tvalidation_0-rmse:1975.56159\tvalidation_1-rmse:1087.49655                  \n",
      "[12]\tvalidation_0-rmse:1973.80925\tvalidation_1-rmse:1086.27196                  \n",
      "[13]\tvalidation_0-rmse:1972.09835\tvalidation_1-rmse:1085.07788                  \n",
      "[14]\tvalidation_0-rmse:1970.44470\tvalidation_1-rmse:1083.92540                  \n",
      "[15]\tvalidation_0-rmse:1968.85594\tvalidation_1-rmse:1082.81968                  \n",
      "[16]\tvalidation_0-rmse:1967.33402\tvalidation_1-rmse:1081.76173                  \n",
      "[17]\tvalidation_0-rmse:1965.87738\tvalidation_1-rmse:1080.75047                  \n",
      "[18]\tvalidation_0-rmse:1964.48282\tvalidation_1-rmse:1079.78347                  \n",
      "[19]\tvalidation_0-rmse:1963.14672\tvalidation_1-rmse:1078.85801                  \n",
      "[20]\tvalidation_0-rmse:1961.86462\tvalidation_1-rmse:1077.97109                  \n",
      "[21]\tvalidation_0-rmse:1960.63279\tvalidation_1-rmse:1077.11970                  \n",
      "[22]\tvalidation_0-rmse:1959.44724\tvalidation_1-rmse:1076.30118                  \n",
      "[23]\tvalidation_0-rmse:1958.30456\tvalidation_1-rmse:1075.51305                  \n",
      "[24]\tvalidation_0-rmse:1957.20135\tvalidation_1-rmse:1074.75282                  \n",
      "[25]\tvalidation_0-rmse:1956.13472\tvalidation_1-rmse:1074.01859                  \n",
      "[26]\tvalidation_0-rmse:1955.10212\tvalidation_1-rmse:1073.30832                  \n",
      "[27]\tvalidation_0-rmse:1954.10091\tvalidation_1-rmse:1072.62036                  \n",
      "[28]\tvalidation_0-rmse:1953.12923\tvalidation_1-rmse:1071.95321                  \n",
      "[29]\tvalidation_0-rmse:1952.18487\tvalidation_1-rmse:1071.30540                  \n",
      "[30]\tvalidation_0-rmse:1951.26629\tvalidation_1-rmse:1070.67569                  \n",
      "[31]\tvalidation_0-rmse:1950.37164\tvalidation_1-rmse:1070.06292                  \n",
      "[32]\tvalidation_0-rmse:1949.49965\tvalidation_1-rmse:1069.46613                  \n",
      "[33]\tvalidation_0-rmse:1948.64885\tvalidation_1-rmse:1068.88427                  \n",
      "[34]\tvalidation_0-rmse:1947.81793\tvalidation_1-rmse:1068.31652                  \n",
      "[35]\tvalidation_0-rmse:1947.00613\tvalidation_1-rmse:1067.76211                  \n",
      "[36]\tvalidation_0-rmse:1946.21214\tvalidation_1-rmse:1067.22025                  \n",
      "[37]\tvalidation_0-rmse:1945.43491\tvalidation_1-rmse:1066.69036                  \n",
      "[38]\tvalidation_0-rmse:1945.43491\tvalidation_1-rmse:1066.69036                  \n",
      "[39]\tvalidation_0-rmse:1945.43491\tvalidation_1-rmse:1066.69036                  \n",
      "[40]\tvalidation_0-rmse:1945.43491\tvalidation_1-rmse:1066.69036                  \n",
      "[41]\tvalidation_0-rmse:1945.43491\tvalidation_1-rmse:1066.69036                  \n",
      "SCORE:                                                                          \n",
      "1066.6903478977024                                                              \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97475\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51215\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15902\tvalidation_1-rmse:1076.10891                   \n",
      "[7]\tvalidation_0-rmse:1935.60746\tvalidation_1-rmse:1060.04868                   \n",
      "[8]\tvalidation_0-rmse:1895.09476\tvalidation_1-rmse:1033.27162                   \n",
      "[9]\tvalidation_0-rmse:1828.27709\tvalidation_1-rmse:991.67812                    \n",
      "[10]\tvalidation_0-rmse:1725.19755\tvalidation_1-rmse:934.75283                   \n",
      "[11]\tvalidation_0-rmse:1580.92808\tvalidation_1-rmse:873.10474                   \n",
      "[12]\tvalidation_0-rmse:1402.92236\tvalidation_1-rmse:833.44610                   \n",
      "[13]\tvalidation_0-rmse:1212.38295\tvalidation_1-rmse:844.52087                   \n",
      "[14]\tvalidation_0-rmse:1035.14174\tvalidation_1-rmse:908.69182                   \n",
      "[15]\tvalidation_0-rmse:889.82992\tvalidation_1-rmse:1001.53338                   \n",
      "[16]\tvalidation_0-rmse:782.59850\tvalidation_1-rmse:1095.19432                   \n",
      "SCORE:                                                                          \n",
      "833.4461153032241                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97473\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51211\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15892\tvalidation_1-rmse:1076.10884                   \n",
      "[7]\tvalidation_0-rmse:1935.60686\tvalidation_1-rmse:1060.04838                   \n",
      "[8]\tvalidation_0-rmse:1895.09307\tvalidation_1-rmse:1033.27067                   \n",
      "[9]\tvalidation_0-rmse:1828.27213\tvalidation_1-rmse:991.67552                    \n",
      "[10]\tvalidation_0-rmse:1725.18460\tvalidation_1-rmse:934.74702                   \n",
      "[11]\tvalidation_0-rmse:1580.89819\tvalidation_1-rmse:873.09554                   \n",
      "[12]\tvalidation_0-rmse:1402.86287\tvalidation_1-rmse:833.44114                   \n",
      "[13]\tvalidation_0-rmse:1212.28247\tvalidation_1-rmse:844.53975                   \n",
      "[14]\tvalidation_0-rmse:1034.99684\tvalidation_1-rmse:908.75499                   \n",
      "[15]\tvalidation_0-rmse:889.64732\tvalidation_1-rmse:1001.64899                   \n",
      "SCORE:                                                                          \n",
      "833.4411357594372                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97473\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51211\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15891\tvalidation_1-rmse:1076.10884                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\tvalidation_0-rmse:1935.60683\tvalidation_1-rmse:1060.04837                   \n",
      "[8]\tvalidation_0-rmse:1895.09279\tvalidation_1-rmse:1033.27062                   \n",
      "[9]\tvalidation_0-rmse:1828.27158\tvalidation_1-rmse:991.67539                    \n",
      "[10]\tvalidation_0-rmse:1725.18288\tvalidation_1-rmse:934.74662                   \n",
      "[11]\tvalidation_0-rmse:1580.89360\tvalidation_1-rmse:873.09481                   \n",
      "[12]\tvalidation_0-rmse:1402.85282\tvalidation_1-rmse:833.44075                   \n",
      "[13]\tvalidation_0-rmse:1212.26327\tvalidation_1-rmse:844.54117                   \n",
      "[14]\tvalidation_0-rmse:1034.96456\tvalidation_1-rmse:908.75979                   \n",
      "[15]\tvalidation_0-rmse:889.59852\tvalidation_1-rmse:1001.65820                   \n",
      "[16]\tvalidation_0-rmse:782.32765\tvalidation_1-rmse:1095.37621                   \n",
      "SCORE:                                                                          \n",
      "833.440754826663                                                                \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97473\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51211\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15891\tvalidation_1-rmse:1076.10884                   \n",
      "[7]\tvalidation_0-rmse:1935.60683\tvalidation_1-rmse:1060.04837                   \n",
      "[8]\tvalidation_0-rmse:1895.09279\tvalidation_1-rmse:1033.27062                   \n",
      "[9]\tvalidation_0-rmse:1828.27158\tvalidation_1-rmse:991.67539                    \n",
      "[10]\tvalidation_0-rmse:1725.18288\tvalidation_1-rmse:934.74662                   \n",
      "[11]\tvalidation_0-rmse:1580.89360\tvalidation_1-rmse:873.09481                   \n",
      "[12]\tvalidation_0-rmse:1402.85282\tvalidation_1-rmse:833.44075                   \n",
      "[13]\tvalidation_0-rmse:1212.26327\tvalidation_1-rmse:844.54117                   \n",
      "[14]\tvalidation_0-rmse:1034.96456\tvalidation_1-rmse:908.75979                   \n",
      "[15]\tvalidation_0-rmse:889.59852\tvalidation_1-rmse:1001.65820                   \n",
      "[16]\tvalidation_0-rmse:782.32765\tvalidation_1-rmse:1095.37621                   \n",
      "SCORE:                                                                          \n",
      "833.440754826663                                                                \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97473\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51210\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15890\tvalidation_1-rmse:1076.10884                   \n",
      "[7]\tvalidation_0-rmse:1935.60675\tvalidation_1-rmse:1060.04831                   \n",
      "[8]\tvalidation_0-rmse:1895.09237\tvalidation_1-rmse:1033.27045                   \n",
      "[9]\tvalidation_0-rmse:1828.27010\tvalidation_1-rmse:991.67485                    \n",
      "[10]\tvalidation_0-rmse:1725.17882\tvalidation_1-rmse:934.74546                   \n",
      "[11]\tvalidation_0-rmse:1580.88360\tvalidation_1-rmse:873.09297                   \n",
      "[12]\tvalidation_0-rmse:1402.83090\tvalidation_1-rmse:833.43973                   \n",
      "[13]\tvalidation_0-rmse:1212.22207\tvalidation_1-rmse:844.54506                   \n",
      "[14]\tvalidation_0-rmse:1034.89709\tvalidation_1-rmse:908.77283                   \n",
      "[15]\tvalidation_0-rmse:889.50010\tvalidation_1-rmse:1001.68201                   \n",
      "SCORE:                                                                          \n",
      "833.4397374608976                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48148                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97455\tvalidation_1-rmse:1090.58756                   \n",
      "[5]\tvalidation_0-rmse:1972.51158\tvalidation_1-rmse:1085.36610                   \n",
      "[6]\tvalidation_0-rmse:1959.15763\tvalidation_1-rmse:1076.10652                   \n",
      "[7]\tvalidation_0-rmse:1935.60319\tvalidation_1-rmse:1060.04377                   \n",
      "[8]\tvalidation_0-rmse:1895.08193\tvalidation_1-rmse:1033.26126                   \n",
      "[9]\tvalidation_0-rmse:1828.23817\tvalidation_1-rmse:991.72584                    \n",
      "[10]\tvalidation_0-rmse:1725.09414\tvalidation_1-rmse:934.80477                   \n",
      "[11]\tvalidation_0-rmse:1580.67927\tvalidation_1-rmse:873.14215                   \n",
      "[12]\tvalidation_0-rmse:1402.39508\tvalidation_1-rmse:833.45222                   \n",
      "[13]\tvalidation_0-rmse:1211.40575\tvalidation_1-rmse:844.50628                   \n",
      "[14]\tvalidation_0-rmse:1033.55025\tvalidation_1-rmse:908.70412                   \n",
      "[15]\tvalidation_0-rmse:887.51409\tvalidation_1-rmse:1001.63266                   \n",
      "[16]\tvalidation_0-rmse:779.54117\tvalidation_1-rmse:1095.39316                   \n",
      "SCORE:                                                                          \n",
      "833.4522117346469                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97476\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51214\tvalidation_1-rmse:1085.36723                   \n",
      "[6]\tvalidation_0-rmse:1959.15898\tvalidation_1-rmse:1076.10882                   \n",
      "[7]\tvalidation_0-rmse:1935.60693\tvalidation_1-rmse:1060.04837                   \n",
      "[8]\tvalidation_0-rmse:1895.09285\tvalidation_1-rmse:1033.27062                   \n",
      "[9]\tvalidation_0-rmse:1828.27133\tvalidation_1-rmse:991.67539                    \n",
      "[10]\tvalidation_0-rmse:1725.18173\tvalidation_1-rmse:934.74672                   \n",
      "[11]\tvalidation_0-rmse:1580.88944\tvalidation_1-rmse:873.09497                   \n",
      "[12]\tvalidation_0-rmse:1402.84090\tvalidation_1-rmse:833.44077                   \n",
      "[13]\tvalidation_0-rmse:1212.23621\tvalidation_1-rmse:844.54126                   \n",
      "[14]\tvalidation_0-rmse:1034.91249\tvalidation_1-rmse:908.76064                   \n",
      "[15]\tvalidation_0-rmse:889.51273\tvalidation_1-rmse:1001.66059                   \n",
      "[16]\tvalidation_0-rmse:782.20468\tvalidation_1-rmse:1095.38066                   \n",
      "SCORE:                                                                          \n",
      "833.4407798872417                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48148                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97447\tvalidation_1-rmse:1090.58746                   \n",
      "[5]\tvalidation_0-rmse:1972.51156\tvalidation_1-rmse:1085.36588                   \n",
      "[6]\tvalidation_0-rmse:1959.15752\tvalidation_1-rmse:1076.10615                   \n",
      "[7]\tvalidation_0-rmse:1935.60248\tvalidation_1-rmse:1060.04300                   \n",
      "[8]\tvalidation_0-rmse:1895.07998\tvalidation_1-rmse:1033.25951                   \n",
      "[9]\tvalidation_0-rmse:1828.23240\tvalidation_1-rmse:991.72203                    \n",
      "[10]\tvalidation_0-rmse:1725.07841\tvalidation_1-rmse:934.79728                   \n",
      "[11]\tvalidation_0-rmse:1580.64084\tvalidation_1-rmse:873.13131                   \n",
      "[12]\tvalidation_0-rmse:1402.31361\tvalidation_1-rmse:833.44648                   \n",
      "[13]\tvalidation_0-rmse:1211.25717\tvalidation_1-rmse:844.52607                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14]\tvalidation_0-rmse:1033.31334\tvalidation_1-rmse:908.76883                   \n",
      "[15]\tvalidation_0-rmse:887.17818\tvalidation_1-rmse:1001.75086                   \n",
      "[16]\tvalidation_0-rmse:779.11121\tvalidation_1-rmse:1095.56442                   \n",
      "SCORE:                                                                          \n",
      "833.4464811742203                                                               \n",
      "[0]\tvalidation_0-rmse:1982.33236\tvalidation_1-rmse:1092.24484                   \n",
      "[1]\tvalidation_0-rmse:1921.07273\tvalidation_1-rmse:1050.26550                   \n",
      "[2]\tvalidation_0-rmse:1792.47167\tvalidation_1-rmse:970.40432                    \n",
      "[3]\tvalidation_0-rmse:1792.47167\tvalidation_1-rmse:970.40432                    \n",
      "[4]\tvalidation_0-rmse:1792.47167\tvalidation_1-rmse:970.40432                    \n",
      "[5]\tvalidation_0-rmse:1792.47167\tvalidation_1-rmse:970.40432                    \n",
      "SCORE:                                                                          \n",
      "970.4043167326206                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97473\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51210\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15890\tvalidation_1-rmse:1076.10884                   \n",
      "[7]\tvalidation_0-rmse:1935.60675\tvalidation_1-rmse:1060.04831                   \n",
      "[8]\tvalidation_0-rmse:1895.09237\tvalidation_1-rmse:1033.27045                   \n",
      "[9]\tvalidation_0-rmse:1828.27010\tvalidation_1-rmse:991.67485                    \n",
      "[10]\tvalidation_0-rmse:1725.17882\tvalidation_1-rmse:934.74546                   \n",
      "[11]\tvalidation_0-rmse:1580.88360\tvalidation_1-rmse:873.09297                   \n",
      "[12]\tvalidation_0-rmse:1402.83090\tvalidation_1-rmse:833.43973                   \n",
      "[13]\tvalidation_0-rmse:1212.22207\tvalidation_1-rmse:844.54506                   \n",
      "[14]\tvalidation_0-rmse:1034.89709\tvalidation_1-rmse:908.77283                   \n",
      "[15]\tvalidation_0-rmse:889.50010\tvalidation_1-rmse:1001.68201                   \n",
      "[16]\tvalidation_0-rmse:782.19787\tvalidation_1-rmse:1095.41130                   \n",
      "SCORE:                                                                          \n",
      "833.4397374608976                                                               \n",
      "[0]\tvalidation_0-rmse:1494.38014\tvalidation_1-rmse:846.80563                    \n",
      "[1]\tvalidation_0-rmse:1178.75331\tvalidation_1-rmse:859.47817                    \n",
      "[2]\tvalidation_0-rmse:987.94658\tvalidation_1-rmse:964.88861                     \n",
      "[3]\tvalidation_0-rmse:879.46514\tvalidation_1-rmse:1073.52894                    \n",
      "[4]\tvalidation_0-rmse:616.19863\tvalidation_1-rmse:1222.36707                    \n",
      "SCORE:                                                                          \n",
      "846.8056203994931                                                               \n",
      "[09:49:54] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1986.27740\tvalidation_1-rmse:1093.54623                   \n",
      "[1]\tvalidation_0-rmse:1970.72102\tvalidation_1-rmse:1060.34924                   \n",
      "[2]\tvalidation_0-rmse:1872.09088\tvalidation_1-rmse:2050.68081                   \n",
      "[3]\tvalidation_0-rmse:1514.67903\tvalidation_1-rmse:21546.95623                  \n",
      "[4]\tvalidation_0-rmse:1634.78350\tvalidation_1-rmse:70541.90837                  \n",
      "[5]\tvalidation_0-rmse:1964.67509\tvalidation_1-rmse:77316.71823                  \n",
      "SCORE:                                                                          \n",
      "77316.71807285653                                                               \n",
      "[0]\tvalidation_0-rmse:833.71258\tvalidation_1-rmse:1681.60771                    \n",
      "[1]\tvalidation_0-rmse:833.71286\tvalidation_1-rmse:1681.60822                    \n",
      "[2]\tvalidation_0-rmse:833.71309\tvalidation_1-rmse:1681.60875                    \n",
      "[3]\tvalidation_0-rmse:833.71260\tvalidation_1-rmse:1681.60769                    \n",
      "[4]\tvalidation_0-rmse:833.58567\tvalidation_1-rmse:1681.54293                    \n",
      "[5]\tvalidation_0-rmse:833.58517\tvalidation_1-rmse:1681.54187                    \n",
      "[6]\tvalidation_0-rmse:833.58460\tvalidation_1-rmse:1681.54069                    \n",
      "[7]\tvalidation_0-rmse:833.58379\tvalidation_1-rmse:1681.53904                    \n",
      "[8]\tvalidation_0-rmse:833.58345\tvalidation_1-rmse:1681.53834                    \n",
      "[9]\tvalidation_0-rmse:833.57704\tvalidation_1-rmse:1681.52579                    \n",
      "[10]\tvalidation_0-rmse:833.57624\tvalidation_1-rmse:1681.52415                   \n",
      "[11]\tvalidation_0-rmse:833.57549\tvalidation_1-rmse:1681.52260                   \n",
      "[12]\tvalidation_0-rmse:833.57493\tvalidation_1-rmse:1681.52141                   \n",
      "[13]\tvalidation_0-rmse:833.57458\tvalidation_1-rmse:1681.52068                   \n",
      "[14]\tvalidation_0-rmse:833.57410\tvalidation_1-rmse:1681.51969                   \n",
      "[15]\tvalidation_0-rmse:833.57354\tvalidation_1-rmse:1681.51851                   \n",
      "[16]\tvalidation_0-rmse:833.57744\tvalidation_1-rmse:1681.53020                   \n",
      "[17]\tvalidation_0-rmse:833.57707\tvalidation_1-rmse:1681.52941                   \n",
      "[18]\tvalidation_0-rmse:833.57670\tvalidation_1-rmse:1681.52866                   \n",
      "[19]\tvalidation_0-rmse:833.57639\tvalidation_1-rmse:1681.52800                   \n",
      "SCORE:                                                                          \n",
      "1681.518497866381                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97475\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51213\tvalidation_1-rmse:1085.36730                   \n",
      "[6]\tvalidation_0-rmse:1959.15915\tvalidation_1-rmse:1076.10900                   \n",
      "[7]\tvalidation_0-rmse:1935.60737\tvalidation_1-rmse:1060.04883                   \n",
      "[8]\tvalidation_0-rmse:1895.09393\tvalidation_1-rmse:1033.27176                   \n",
      "[9]\tvalidation_0-rmse:1828.27322\tvalidation_1-rmse:991.67779                    \n",
      "[10]\tvalidation_0-rmse:1725.18391\tvalidation_1-rmse:934.75092                   \n",
      "[11]\tvalidation_0-rmse:1580.88859\tvalidation_1-rmse:873.10024                   \n",
      "[12]\tvalidation_0-rmse:1402.82691\tvalidation_1-rmse:833.44304                   \n",
      "[13]\tvalidation_0-rmse:1212.18891\tvalidation_1-rmse:844.53435                   \n",
      "[14]\tvalidation_0-rmse:1034.80436\tvalidation_1-rmse:908.74177                   \n",
      "[15]\tvalidation_0-rmse:889.31584\tvalidation_1-rmse:1001.63235                   \n",
      "SCORE:                                                                          \n",
      "833.4430408079565                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34334\tvalidation_1-rmse:1096.48203                   \n",
      "[1]\tvalidation_0-rmse:1987.65517\tvalidation_1-rmse:1095.99586                   \n",
      "[2]\tvalidation_0-rmse:1986.40596\tvalidation_1-rmse:1095.11423                   \n",
      "[3]\tvalidation_0-rmse:1984.14418\tvalidation_1-rmse:1093.51993                   \n",
      "[4]\tvalidation_0-rmse:1980.06327\tvalidation_1-rmse:1090.65061                   \n",
      "[5]\tvalidation_0-rmse:1972.74402\tvalidation_1-rmse:1085.52801                   \n",
      "[6]\tvalidation_0-rmse:1959.74840\tvalidation_1-rmse:1076.50849                   \n",
      "[7]\tvalidation_0-rmse:1937.05990\tvalidation_1-rmse:1061.00170                   \n",
      "[8]\tvalidation_0-rmse:1898.53600\tvalidation_1-rmse:1035.41099                   \n",
      "[9]\tvalidation_0-rmse:1835.98423\tvalidation_1-rmse:996.03258                    \n",
      "[10]\tvalidation_0-rmse:1741.11552\tvalidation_1-rmse:942.24212                   \n",
      "[11]\tvalidation_0-rmse:1610.43345\tvalidation_1-rmse:882.50867                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\tvalidation_0-rmse:1451.11047\tvalidation_1-rmse:838.46888                   \n",
      "[13]\tvalidation_0-rmse:1281.80781\tvalidation_1-rmse:835.45110                   \n",
      "[14]\tvalidation_0-rmse:1125.15602\tvalidation_1-rmse:880.56389                   \n",
      "[15]\tvalidation_0-rmse:997.73616\tvalidation_1-rmse:957.24396                    \n",
      "[16]\tvalidation_0-rmse:905.28962\tvalidation_1-rmse:1042.90875                   \n",
      "SCORE:                                                                          \n",
      "835.4510902517811                                                               \n",
      "[0]\tvalidation_0-rmse:1980.73163\tvalidation_1-rmse:1091.12000                   \n",
      "[1]\tvalidation_0-rmse:1792.43827\tvalidation_1-rmse:970.38526                    \n",
      "[2]\tvalidation_0-rmse:1792.43827\tvalidation_1-rmse:970.38526                    \n",
      "[3]\tvalidation_0-rmse:1792.43827\tvalidation_1-rmse:970.38526                    \n",
      "[4]\tvalidation_0-rmse:1792.43827\tvalidation_1-rmse:970.38526                    \n",
      "[5]\tvalidation_0-rmse:1792.43827\tvalidation_1-rmse:970.38526                    \n",
      "SCORE:                                                                          \n",
      "970.3852405034359                                                               \n",
      "[0]\tvalidation_0-rmse:1988.38235\tvalidation_1-rmse:1096.50954                   \n",
      "[1]\tvalidation_0-rmse:1987.97522\tvalidation_1-rmse:1096.22191                   \n",
      "[2]\tvalidation_0-rmse:1987.49865\tvalidation_1-rmse:1095.88539                   \n",
      "[3]\tvalidation_0-rmse:1986.95758\tvalidation_1-rmse:1095.50342                   \n",
      "[4]\tvalidation_0-rmse:1986.35867\tvalidation_1-rmse:1095.08075                   \n",
      "[5]\tvalidation_0-rmse:1985.73193\tvalidation_1-rmse:1094.63868                   \n",
      "[6]\tvalidation_0-rmse:1985.09013\tvalidation_1-rmse:1094.18626                   \n",
      "[7]\tvalidation_0-rmse:1984.44951\tvalidation_1-rmse:1093.73495                   \n",
      "[8]\tvalidation_0-rmse:1983.82155\tvalidation_1-rmse:1093.29272                   \n",
      "[9]\tvalidation_0-rmse:1983.21300\tvalidation_1-rmse:1092.86439                   \n",
      "[10]\tvalidation_0-rmse:1982.62724\tvalidation_1-rmse:1092.45223                  \n",
      "[11]\tvalidation_0-rmse:1982.06501\tvalidation_1-rmse:1092.05697                  \n",
      "[12]\tvalidation_0-rmse:1981.52642\tvalidation_1-rmse:1091.67834                  \n",
      "[13]\tvalidation_0-rmse:1981.01018\tvalidation_1-rmse:1091.31557                  \n",
      "[14]\tvalidation_0-rmse:1980.51473\tvalidation_1-rmse:1090.96765                  \n",
      "[15]\tvalidation_0-rmse:1980.03890\tvalidation_1-rmse:1090.63359                  \n",
      "[16]\tvalidation_0-rmse:1979.58113\tvalidation_1-rmse:1090.31238                  \n",
      "[17]\tvalidation_0-rmse:1979.14023\tvalidation_1-rmse:1090.00297                  \n",
      "[18]\tvalidation_0-rmse:1978.71451\tvalidation_1-rmse:1089.70447                  \n",
      "[19]\tvalidation_0-rmse:1978.30319\tvalidation_1-rmse:1089.41606                  \n",
      "[20]\tvalidation_0-rmse:1977.90498\tvalidation_1-rmse:1089.13703                  \n",
      "[21]\tvalidation_0-rmse:1977.51895\tvalidation_1-rmse:1088.86658                  \n",
      "[22]\tvalidation_0-rmse:1977.14442\tvalidation_1-rmse:1088.60425                  \n",
      "[23]\tvalidation_0-rmse:1976.78028\tvalidation_1-rmse:1088.34935                  \n",
      "[24]\tvalidation_0-rmse:1976.42626\tvalidation_1-rmse:1088.10146                  \n",
      "[25]\tvalidation_0-rmse:1976.08129\tvalidation_1-rmse:1087.86008                  \n",
      "[26]\tvalidation_0-rmse:1975.74509\tvalidation_1-rmse:1087.62483                  \n",
      "[27]\tvalidation_0-rmse:1975.41685\tvalidation_1-rmse:1087.39535                  \n",
      "[28]\tvalidation_0-rmse:1975.09644\tvalidation_1-rmse:1087.17127                  \n",
      "[29]\tvalidation_0-rmse:1974.78307\tvalidation_1-rmse:1086.95226                  \n",
      "[30]\tvalidation_0-rmse:1974.47665\tvalidation_1-rmse:1086.73807                  \n",
      "[31]\tvalidation_0-rmse:1974.17658\tvalidation_1-rmse:1086.52846                  \n",
      "[32]\tvalidation_0-rmse:1973.88277\tvalidation_1-rmse:1086.32313                  \n",
      "[33]\tvalidation_0-rmse:1973.59464\tvalidation_1-rmse:1086.12192                  \n",
      "[34]\tvalidation_0-rmse:1973.31194\tvalidation_1-rmse:1085.92459                  \n",
      "[35]\tvalidation_0-rmse:1973.03462\tvalidation_1-rmse:1085.73096                  \n",
      "[36]\tvalidation_0-rmse:1972.76229\tvalidation_1-rmse:1085.54090                  \n",
      "[37]\tvalidation_0-rmse:1972.49449\tvalidation_1-rmse:1085.35416                  \n",
      "[38]\tvalidation_0-rmse:1972.23159\tvalidation_1-rmse:1085.17066                  \n",
      "[39]\tvalidation_0-rmse:1971.97280\tvalidation_1-rmse:1084.99023                  \n",
      "[40]\tvalidation_0-rmse:1971.71828\tvalidation_1-rmse:1084.81276                  \n",
      "[41]\tvalidation_0-rmse:1971.46780\tvalidation_1-rmse:1084.63809                  \n",
      "[42]\tvalidation_0-rmse:1971.22098\tvalidation_1-rmse:1084.46617                  \n",
      "[43]\tvalidation_0-rmse:1970.97796\tvalidation_1-rmse:1084.29683                  \n",
      "[44]\tvalidation_0-rmse:1970.73854\tvalidation_1-rmse:1084.12997                  \n",
      "[45]\tvalidation_0-rmse:1970.50252\tvalidation_1-rmse:1083.96556                  \n",
      "[46]\tvalidation_0-rmse:1970.26986\tvalidation_1-rmse:1083.80351                  \n",
      "[47]\tvalidation_0-rmse:1970.04018\tvalidation_1-rmse:1083.64365                  \n",
      "[48]\tvalidation_0-rmse:1969.81381\tvalidation_1-rmse:1083.48596                  \n",
      "[49]\tvalidation_0-rmse:1969.59021\tvalidation_1-rmse:1083.33037                  \n",
      "[50]\tvalidation_0-rmse:1969.36953\tvalidation_1-rmse:1083.17679                  \n",
      "[51]\tvalidation_0-rmse:1969.15154\tvalidation_1-rmse:1083.02513                  \n",
      "[52]\tvalidation_0-rmse:1968.93626\tvalidation_1-rmse:1082.87537                  \n",
      "[53]\tvalidation_0-rmse:1968.72349\tvalidation_1-rmse:1082.72743                  \n",
      "[54]\tvalidation_0-rmse:1968.51338\tvalidation_1-rmse:1082.58127                  \n",
      "[55]\tvalidation_0-rmse:1968.30557\tvalidation_1-rmse:1082.43683                  \n",
      "[56]\tvalidation_0-rmse:1968.10021\tvalidation_1-rmse:1082.29403                  \n",
      "[57]\tvalidation_0-rmse:1967.89710\tvalidation_1-rmse:1082.15285                  \n",
      "[58]\tvalidation_0-rmse:1967.69626\tvalidation_1-rmse:1082.01327                  \n",
      "[59]\tvalidation_0-rmse:1967.49747\tvalidation_1-rmse:1081.87520                  \n",
      "[60]\tvalidation_0-rmse:1967.30090\tvalidation_1-rmse:1081.73858                  \n",
      "[61]\tvalidation_0-rmse:1967.10637\tvalidation_1-rmse:1081.60346                  \n",
      "[62]\tvalidation_0-rmse:1966.91366\tvalidation_1-rmse:1081.46968                  \n",
      "[63]\tvalidation_0-rmse:1966.72297\tvalidation_1-rmse:1081.33727                  \n",
      "[64]\tvalidation_0-rmse:1966.53429\tvalidation_1-rmse:1081.20622                  \n",
      "[65]\tvalidation_0-rmse:1966.34722\tvalidation_1-rmse:1081.07642                  \n",
      "[66]\tvalidation_0-rmse:1966.16214\tvalidation_1-rmse:1080.94789                  \n",
      "[67]\tvalidation_0-rmse:1965.97867\tvalidation_1-rmse:1080.82060                  \n",
      "[68]\tvalidation_0-rmse:1965.79698\tvalidation_1-rmse:1080.69449                  \n",
      "[69]\tvalidation_0-rmse:1965.61689\tvalidation_1-rmse:1080.56955                  \n",
      "[70]\tvalidation_0-rmse:1965.43840\tvalidation_1-rmse:1080.44579                  \n",
      "[71]\tvalidation_0-rmse:1965.26146\tvalidation_1-rmse:1080.32309                  \n",
      "[72]\tvalidation_0-rmse:1965.08610\tvalidation_1-rmse:1080.20149                  \n",
      "[73]\tvalidation_0-rmse:1964.91232\tvalidation_1-rmse:1080.08100                  \n",
      "[74]\tvalidation_0-rmse:1964.73991\tvalidation_1-rmse:1079.96154                  \n",
      "[75]\tvalidation_0-rmse:1964.56905\tvalidation_1-rmse:1079.84306                  \n",
      "[76]\tvalidation_0-rmse:1964.39956\tvalidation_1-rmse:1079.72560                  \n",
      "[77]\tvalidation_0-rmse:1964.23141\tvalidation_1-rmse:1079.60909                  \n",
      "[78]\tvalidation_0-rmse:1964.06462\tvalidation_1-rmse:1079.49353                  \n",
      "[79]\tvalidation_0-rmse:1963.89919\tvalidation_1-rmse:1079.37893                  \n",
      "[80]\tvalidation_0-rmse:1963.73508\tvalidation_1-rmse:1079.26522                  \n",
      "[81]\tvalidation_0-rmse:1963.57216\tvalidation_1-rmse:1079.15243                  \n",
      "[82]\tvalidation_0-rmse:1963.41053\tvalidation_1-rmse:1079.04047                  \n",
      "[83]\tvalidation_0-rmse:1963.25008\tvalidation_1-rmse:1078.92943                  \n",
      "[84]\tvalidation_0-rmse:1963.09073\tvalidation_1-rmse:1078.81913                  \n",
      "[85]\tvalidation_0-rmse:1962.93274\tvalidation_1-rmse:1078.70976                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86]\tvalidation_0-rmse:1962.77585\tvalidation_1-rmse:1078.60114                  \n",
      "[87]\tvalidation_0-rmse:1962.61993\tvalidation_1-rmse:1078.49335                  \n",
      "[88]\tvalidation_0-rmse:1962.46530\tvalidation_1-rmse:1078.38631                  \n",
      "[89]\tvalidation_0-rmse:1962.31161\tvalidation_1-rmse:1078.28004                  \n",
      "[90]\tvalidation_0-rmse:1962.15903\tvalidation_1-rmse:1078.17450                  \n",
      "[91]\tvalidation_0-rmse:1962.00756\tvalidation_1-rmse:1078.06969                  \n",
      "[92]\tvalidation_0-rmse:1961.85711\tvalidation_1-rmse:1077.96565                  \n",
      "[93]\tvalidation_0-rmse:1961.70760\tvalidation_1-rmse:1077.86232                  \n",
      "[94]\tvalidation_0-rmse:1961.55908\tvalidation_1-rmse:1077.75964                  \n",
      "[95]\tvalidation_0-rmse:1961.41166\tvalidation_1-rmse:1077.65769                  \n",
      "[96]\tvalidation_0-rmse:1961.26514\tvalidation_1-rmse:1077.55636                  \n",
      "[97]\tvalidation_0-rmse:1961.11937\tvalidation_1-rmse:1077.45573                  \n",
      "[98]\tvalidation_0-rmse:1960.97468\tvalidation_1-rmse:1077.35575                  \n",
      "[99]\tvalidation_0-rmse:1960.83089\tvalidation_1-rmse:1077.25640                  \n",
      "SCORE:                                                                          \n",
      "1077.2564030325684                                                              \n",
      "[09:50:03] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1985.73383\tvalidation_1-rmse:1094.18860                   \n",
      "[1]\tvalidation_0-rmse:1962.75330\tvalidation_1-rmse:1070.42266                   \n",
      "[2]\tvalidation_0-rmse:1797.24615\tvalidation_1-rmse:961.68780                    \n",
      "[3]\tvalidation_0-rmse:1095.73280\tvalidation_1-rmse:4105.84310                   \n",
      "[4]\tvalidation_0-rmse:1038.44117\tvalidation_1-rmse:8405.58023                   \n",
      "[5]\tvalidation_0-rmse:1000.47222\tvalidation_1-rmse:7466.67465                   \n",
      "[6]\tvalidation_0-rmse:947.97972\tvalidation_1-rmse:6552.44832                    \n",
      "SCORE:                                                                          \n",
      "6552.448237147594                                                               \n",
      "[0]\tvalidation_0-rmse:1494.37890\tvalidation_1-rmse:846.80519                    \n",
      "[1]\tvalidation_0-rmse:1178.75148\tvalidation_1-rmse:859.47871                    \n",
      "[2]\tvalidation_0-rmse:987.94471\tvalidation_1-rmse:964.89007                     \n",
      "[3]\tvalidation_0-rmse:879.46347\tvalidation_1-rmse:1073.53100                    \n",
      "SCORE:                                                                          \n",
      "846.8051794444043                                                               \n",
      "[0]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "[1]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "[2]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "[3]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "[4]\tvalidation_0-rmse:833.71320\tvalidation_1-rmse:1681.60895                    \n",
      "SCORE:                                                                          \n",
      "1681.6089315225274                                                              \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97473\tvalidation_1-rmse:1090.58818                   \n",
      "[5]\tvalidation_0-rmse:1972.51211\tvalidation_1-rmse:1085.36729                   \n",
      "[6]\tvalidation_0-rmse:1959.15892\tvalidation_1-rmse:1076.10884                   \n",
      "[7]\tvalidation_0-rmse:1935.60686\tvalidation_1-rmse:1060.04838                   \n",
      "[8]\tvalidation_0-rmse:1895.09307\tvalidation_1-rmse:1033.27067                   \n",
      "[9]\tvalidation_0-rmse:1828.27213\tvalidation_1-rmse:991.67552                    \n",
      "[10]\tvalidation_0-rmse:1725.18460\tvalidation_1-rmse:934.74702                   \n",
      "[11]\tvalidation_0-rmse:1580.89819\tvalidation_1-rmse:873.09554                   \n",
      "[12]\tvalidation_0-rmse:1402.86287\tvalidation_1-rmse:833.44114                   \n",
      "[13]\tvalidation_0-rmse:1212.28247\tvalidation_1-rmse:844.53975                   \n",
      "[14]\tvalidation_0-rmse:1034.99684\tvalidation_1-rmse:908.75499                   \n",
      "[15]\tvalidation_0-rmse:889.64732\tvalidation_1-rmse:1001.64899                   \n",
      "[16]\tvalidation_0-rmse:782.39301\tvalidation_1-rmse:1095.36283                   \n",
      "SCORE:                                                                          \n",
      "833.4411357594372                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34291\tvalidation_1-rmse:1096.48177                   \n",
      "[1]\tvalidation_0-rmse:1987.65338\tvalidation_1-rmse:1095.99465                   \n",
      "[2]\tvalidation_0-rmse:1986.40054\tvalidation_1-rmse:1095.11041                   \n",
      "[3]\tvalidation_0-rmse:1984.12862\tvalidation_1-rmse:1093.50897                   \n",
      "[4]\tvalidation_0-rmse:1980.01898\tvalidation_1-rmse:1090.61960                   \n",
      "[5]\tvalidation_0-rmse:1972.62791\tvalidation_1-rmse:1085.44832                   \n",
      "[6]\tvalidation_0-rmse:1959.45373\tvalidation_1-rmse:1076.31128                   \n",
      "[7]\tvalidation_0-rmse:1936.33652\tvalidation_1-rmse:1060.53500                   \n",
      "[8]\tvalidation_0-rmse:1896.83481\tvalidation_1-rmse:1034.37672                   \n",
      "[9]\tvalidation_0-rmse:1832.22332\tvalidation_1-rmse:993.97025                    \n",
      "[10]\tvalidation_0-rmse:1733.50779\tvalidation_1-rmse:938.80330                   \n",
      "[11]\tvalidation_0-rmse:1596.74745\tvalidation_1-rmse:878.35758                   \n",
      "[12]\tvalidation_0-rmse:1429.58245\tvalidation_1-rmse:836.19653                   \n",
      "[13]\tvalidation_0-rmse:1251.94360\tvalidation_1-rmse:838.03697                   \n",
      "[14]\tvalidation_0-rmse:1087.37001\tvalidation_1-rmse:887.77298                   \n",
      "[15]\tvalidation_0-rmse:952.31473\tvalidation_1-rmse:965.45734                    \n",
      "SCORE:                                                                          \n",
      "836.1965394926938                                                               \n",
      "[09:50:04] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:1318.30328\tvalidation_1-rmse:7954.74543                   \n",
      "[1]\tvalidation_0-rmse:1120.46856\tvalidation_1-rmse:6679.84660                   \n",
      "[2]\tvalidation_0-rmse:1210.01030\tvalidation_1-rmse:3489.07990                   \n",
      "[3]\tvalidation_0-rmse:1057.22217\tvalidation_1-rmse:3837.00606                   \n",
      "[4]\tvalidation_0-rmse:2008.87854\tvalidation_1-rmse:6609.89233                   \n",
      "[5]\tvalidation_0-rmse:2742112787.02698\tvalidation_1-rmse:7566551643.68782       \n",
      "[6]\tvalidation_0-rmse:inf\tvalidation_1-rmse:inf                                 \n",
      "SCORE:                                                                          \n",
      "1.7127280936386677e+27                                                          \n",
      "[0]\tvalidation_0-rmse:1988.36139\tvalidation_1-rmse:1096.49478                   \n",
      "[1]\tvalidation_0-rmse:1987.90913\tvalidation_1-rmse:1096.17523                   \n",
      "[2]\tvalidation_0-rmse:1987.34258\tvalidation_1-rmse:1095.77519                   \n",
      "[3]\tvalidation_0-rmse:1986.63440\tvalidation_1-rmse:1095.27536                   \n",
      "[4]\tvalidation_0-rmse:1985.73637\tvalidation_1-rmse:1094.64122                   \n",
      "[5]\tvalidation_0-rmse:1984.98738\tvalidation_1-rmse:1094.11332                   \n",
      "[6]\tvalidation_0-rmse:1983.80841\tvalidation_1-rmse:1093.28298                   \n",
      "[7]\tvalidation_0-rmse:1982.27242\tvalidation_1-rmse:1092.20241                   \n",
      "[8]\tvalidation_0-rmse:1980.36601\tvalidation_1-rmse:1090.86303                   \n",
      "[9]\tvalidation_0-rmse:1978.01297\tvalidation_1-rmse:1089.21267                   \n",
      "[10]\tvalidation_0-rmse:1975.12188\tvalidation_1-rmse:1087.18928                  \n",
      "[11]\tvalidation_0-rmse:1971.58318\tvalidation_1-rmse:1084.71901                  \n",
      "[12]\tvalidation_0-rmse:1967.26852\tvalidation_1-rmse:1081.71684                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13]\tvalidation_0-rmse:1962.03158\tvalidation_1-rmse:1078.08743                  \n",
      "[14]\tvalidation_0-rmse:1955.70703\tvalidation_1-rmse:1073.72574                  \n",
      "[15]\tvalidation_0-rmse:1948.11308\tvalidation_1-rmse:1068.51992                  \n",
      "[16]\tvalidation_0-rmse:1939.05640\tvalidation_1-rmse:1062.35697                  \n",
      "[17]\tvalidation_0-rmse:1928.34184\tvalidation_1-rmse:1055.13131                  \n",
      "[18]\tvalidation_0-rmse:1928.34184\tvalidation_1-rmse:1055.13131                  \n",
      "[19]\tvalidation_0-rmse:1928.34184\tvalidation_1-rmse:1055.13131                  \n",
      "[20]\tvalidation_0-rmse:1928.34184\tvalidation_1-rmse:1055.13131                  \n",
      "SCORE:                                                                          \n",
      "1055.131315277635                                                               \n",
      "[0]\tvalidation_0-rmse:1988.34265\tvalidation_1-rmse:1096.48149                   \n",
      "[1]\tvalidation_0-rmse:1987.65178\tvalidation_1-rmse:1095.99348                   \n",
      "[2]\tvalidation_0-rmse:1986.39512\tvalidation_1-rmse:1095.10653                   \n",
      "[3]\tvalidation_0-rmse:1984.11284\tvalidation_1-rmse:1093.49785                   \n",
      "[4]\tvalidation_0-rmse:1979.97832\tvalidation_1-rmse:1090.59103                   \n",
      "[5]\tvalidation_0-rmse:1972.52395\tvalidation_1-rmse:1085.37446                   \n",
      "[6]\tvalidation_0-rmse:1959.19716\tvalidation_1-rmse:1076.12802                   \n",
      "[7]\tvalidation_0-rmse:1935.72855\tvalidation_1-rmse:1060.10147                   \n",
      "[8]\tvalidation_0-rmse:1895.46987\tvalidation_1-rmse:1033.41619                   \n",
      "[9]\tvalidation_0-rmse:1829.38435\tvalidation_1-rmse:992.04752                    \n",
      "[10]\tvalidation_0-rmse:1728.22848\tvalidation_1-rmse:935.55753                   \n",
      "[11]\tvalidation_0-rmse:1588.38970\tvalidation_1-rmse:874.36072                   \n",
      "[12]\tvalidation_0-rmse:1419.01449\tvalidation_1-rmse:834.20986                   \n",
      "[13]\tvalidation_0-rmse:1242.57631\tvalidation_1-rmse:841.99526                   \n",
      "[14]\tvalidation_0-rmse:1084.66790\tvalidation_1-rmse:900.18475                   \n",
      "[15]\tvalidation_0-rmse:961.85932\tvalidation_1-rmse:986.01303                    \n",
      "SCORE:                                                                          \n",
      "834.209865755904                                                                \n",
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583                    \n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782                    \n",
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61614                    \n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884                    \n",
      "[4]\tvalidation_0-rmse:1311.07119\tvalidation_1-rmse:831.66310                    \n",
      "[5]\tvalidation_0-rmse:1221.23112\tvalidation_1-rmse:844.06963                    \n",
      "[6]\tvalidation_0-rmse:1143.01782\tvalidation_1-rmse:866.82635                    \n",
      "[7]\tvalidation_0-rmse:1075.26218\tvalidation_1-rmse:896.04739                    \n",
      "SCORE:                                                                          \n",
      "831.6631038370143                                                               \n",
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583                    \n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782                    \n",
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61614                    \n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884                    \n",
      "[4]\tvalidation_0-rmse:1311.07044\tvalidation_1-rmse:831.66333                    \n",
      "[5]\tvalidation_0-rmse:1221.22633\tvalidation_1-rmse:844.07087                    \n",
      "[6]\tvalidation_0-rmse:1143.00926\tvalidation_1-rmse:866.82894                    \n",
      "[7]\tvalidation_0-rmse:1075.25004\tvalidation_1-rmse:896.05151                    \n",
      "SCORE:                                                                          \n",
      "831.6633354482676                                                               \n",
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583                    \n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782                    \n",
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61614                    \n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884                    \n",
      "[4]\tvalidation_0-rmse:1311.07946\tvalidation_1-rmse:831.66140                    \n",
      "[5]\tvalidation_0-rmse:1221.26765\tvalidation_1-rmse:844.06073                    \n",
      "[6]\tvalidation_0-rmse:1143.08086\tvalidation_1-rmse:866.80788                    \n",
      "[7]\tvalidation_0-rmse:1075.34991\tvalidation_1-rmse:896.01825                    \n",
      "[8]\tvalidation_0-rmse:1016.97972\tvalidation_1-rmse:928.76688                    \n",
      "SCORE:                                                                          \n",
      "831.6614102028839                                                               \n",
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583                    \n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782                    \n",
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61614                    \n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884                    \n",
      "[4]\tvalidation_0-rmse:1311.07946\tvalidation_1-rmse:831.66140                    \n",
      "[5]\tvalidation_0-rmse:1221.26765\tvalidation_1-rmse:844.06073                    \n",
      "[6]\tvalidation_0-rmse:1143.08086\tvalidation_1-rmse:866.80788                    \n",
      "[7]\tvalidation_0-rmse:1075.34991\tvalidation_1-rmse:896.01825                    \n",
      "[8]\tvalidation_0-rmse:1016.97972\tvalidation_1-rmse:928.76688                    \n",
      "SCORE:                                                                          \n",
      "831.6614102028839                                                               \n",
      "[09:50:08] WARNING: ../src/learner.cc:767:                                      \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-rmse:619.64063\tvalidation_1-rmse:2418.40150                    \n",
      "[1]\tvalidation_0-rmse:602.90348\tvalidation_1-rmse:2615.88765                    \n",
      "[2]\tvalidation_0-rmse:601.62109\tvalidation_1-rmse:2641.35559                    \n",
      "[3]\tvalidation_0-rmse:600.73172\tvalidation_1-rmse:2648.06535                    \n",
      "SCORE:                                                                          \n",
      "2649.4114689342273                                                              \n",
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583                    \n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782                    \n",
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61614                    \n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884                    \n",
      "[4]\tvalidation_0-rmse:1311.07946\tvalidation_1-rmse:831.66140                    \n",
      "[5]\tvalidation_0-rmse:1221.26765\tvalidation_1-rmse:844.06073                    \n",
      "[6]\tvalidation_0-rmse:1143.08086\tvalidation_1-rmse:866.80788                    \n",
      "[7]\tvalidation_0-rmse:1075.34991\tvalidation_1-rmse:896.01825                    \n",
      "[8]\tvalidation_0-rmse:1016.97972\tvalidation_1-rmse:928.76688                    \n",
      "SCORE:                                                                          \n",
      "831.6614102028839                                                               \n",
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583                    \n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782                    \n",
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61614                    \n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884                    \n",
      "[4]\tvalidation_0-rmse:1332.89286\tvalidation_1-rmse:831.05919                    \n",
      "[5]\tvalidation_0-rmse:1245.95205\tvalidation_1-rmse:841.45594                    \n",
      "[6]\tvalidation_0-rmse:1170.53804\tvalidation_1-rmse:862.33611                    \n",
      "[7]\tvalidation_0-rmse:1105.45716\tvalidation_1-rmse:889.87108                    \n",
      "SCORE:                                                                          \n",
      "831.0591956717094                                                               \n",
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583                    \n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61614                    \n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884                    \n",
      "[4]\tvalidation_0-rmse:1332.89286\tvalidation_1-rmse:831.05919                    \n",
      "[5]\tvalidation_0-rmse:1245.95205\tvalidation_1-rmse:841.45594                    \n",
      "[6]\tvalidation_0-rmse:1170.53804\tvalidation_1-rmse:862.33611                    \n",
      "[7]\tvalidation_0-rmse:1105.45716\tvalidation_1-rmse:889.87108                    \n",
      "SCORE:                                                                          \n",
      "831.0591956717094                                                               \n",
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583                    \n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782                    \n",
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61614                    \n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884                    \n",
      "[4]\tvalidation_0-rmse:1332.89286\tvalidation_1-rmse:831.05919                    \n",
      "[5]\tvalidation_0-rmse:1245.95205\tvalidation_1-rmse:841.45594                    \n",
      "[6]\tvalidation_0-rmse:1170.53804\tvalidation_1-rmse:862.33611                    \n",
      "[7]\tvalidation_0-rmse:1105.45716\tvalidation_1-rmse:889.87108                    \n",
      "SCORE:                                                                          \n",
      "831.0591956717094                                                               \n",
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583                    \n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782                    \n",
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61614                    \n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884                    \n",
      "[4]\tvalidation_0-rmse:1332.89286\tvalidation_1-rmse:831.05919                    \n",
      "[5]\tvalidation_0-rmse:1245.95205\tvalidation_1-rmse:841.45594                    \n",
      "[6]\tvalidation_0-rmse:1170.53804\tvalidation_1-rmse:862.33611                    \n",
      "[7]\tvalidation_0-rmse:1105.45716\tvalidation_1-rmse:889.87108                    \n",
      "SCORE:                                                                          \n",
      "831.0591956717094                                                               \n",
      "100%|████████| 100/100 [00:51<00:00,  1.96trial/s, best loss: 831.0591956717094]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ab80e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are :  \n",
      "\n",
      "{'booster': 0, 'colsample_bytree': 0.9983166182439028, 'max_depth': 12.0, 'min_child_weight': 10.0, 'objective': 0, 'reg_alpha': 3, 'reg_lambda': 8}\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a858a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params ={\n",
    "    'max_depth': int(best_hyperparams['max_depth']),\n",
    "       'booster': ['gbtree', 'dart','gblinear'][best_hyperparams['booster']],\n",
    "        'reg_alpha' : list(map(lambda x: pow(10, x), range(-4,5)))[best_hyperparams['reg_alpha']],\n",
    "        'reg_lambda' : list(map(lambda x: pow(10, x), range(-4,5)))[best_hyperparams['reg_lambda']],\n",
    "        'colsample_bytree' : int(best_hyperparams['colsample_bytree']),\n",
    "        'min_child_weight' : int(best_hyperparams['min_child_weight']),\n",
    "#         'n_estimators': hp.choice([10,50,100,150,200]),\n",
    "       'objective':['reg:squarederror', 'reg:squaredlogerror', 'reg:absoluteerror', 'reg:pseudohubererror', 'reg:tweedie'][best_hyperparams['objective']],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "da011e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 12,\n",
       " 'booster': 'gbtree',\n",
       " 'reg_alpha': 0.1,\n",
       " 'reg_lambda': 10000.0,\n",
       " 'colsample_bytree': 0,\n",
       " 'min_child_weight': 10,\n",
       " 'objective': 'reg:squarederror'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = {'max_depth': 12,\n",
    " 'booster': 'gbtree',\n",
    " 'reg_alpha': 0.1,\n",
    " 'reg_lambda': 10000.0,\n",
    " 'colsample_bytree': 0,\n",
    " 'min_child_weight': 10,\n",
    " 'objective': 'reg:squarederror'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76c8264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1822.91733\tvalidation_1-rmse:988.17583\n",
      "[1]\tvalidation_0-rmse:1676.10938\tvalidation_1-rmse:910.21782\n",
      "[2]\tvalidation_0-rmse:1546.54953\tvalidation_1-rmse:860.61613\n",
      "[3]\tvalidation_0-rmse:1432.63596\tvalidation_1-rmse:835.77884\n",
      "[4]\tvalidation_0-rmse:1332.89287\tvalidation_1-rmse:831.05919\n",
      "[5]\tvalidation_0-rmse:1245.95205\tvalidation_1-rmse:841.45593\n",
      "[6]\tvalidation_0-rmse:1166.54205\tvalidation_1-rmse:863.08766\n",
      "[7]\tvalidation_0-rmse:1101.26990\tvalidation_1-rmse:890.86228\n",
      "[8]\tvalidation_0-rmse:1045.21743\tvalidation_1-rmse:922.33003\n",
      "[9]\tvalidation_0-rmse:997.33934\tvalidation_1-rmse:955.41146\n",
      "RMSE :  831.059194\n"
     ]
    }
   ],
   "source": [
    "xgb_best = xgb.XGBRegressor(**best_params, early_stopping_rounds=6,\n",
    "        seed=142,\n",
    "                    eval_metric=\"rmse\",)\n",
    "xgb_best.fit(X_train, y_train,\n",
    "                    eval_set=[( X_train, y_train), ( X_val, y_val)],\n",
    "\n",
    "            verbose=True)\n",
    "pred = xgb_best.predict(X_val)\n",
    "\n",
    "# RMSE Computation\n",
    "rmse = np.sqrt(MSE(y_val, pred))\n",
    "print(\"RMSE : % f\" %(rmse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e032717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best.save_model('xgboost_best.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be738b9d",
   "metadata": {},
   "source": [
    "## Old models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdb1a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 6\n",
    "dataset_train = keras.utils.timeseries_dataset_from_array(\n",
    "    data=X_train,\n",
    "    targets=y_train,\n",
    "    sequence_length=sequence_length,\n",
    "    batch_size=1024,\n",
    ")\n",
    "dataset_val = keras.utils.timeseries_dataset_from_array(\n",
    "    data=X_val,\n",
    "    targets=y_val,\n",
    "    sequence_length=sequence_length,\n",
    "    batch_size=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fd4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbbc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "lstm_out = keras.layers.LSTM(32)(inputs)\n",
    "outputs = keras.layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1**2), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=100,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(file=\"Standart_Scaler_X.pckl\", mode='wb') as f:\n",
    "#     pickle.dump(obj=ss_x, file=f)\n",
    "# with open(file=\"Standart_Scaler_y.pckl\", mode='wb') as f:\n",
    "#     pickle.dump(obj=ss_y, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import ephem\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# with open(file=\"Standart_Scaler_X.pckl\", mode='rb') as f:\n",
    "#     Standart_Scaler_X = pickle.load(f)\n",
    "# with open(file=\"Standart_Scaler_y.pckl\", mode='rb') as f:\n",
    "#     Standart_Scaler_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sec:int, year:int, month:int, day:int, sequence_length:int, model):\n",
    "    \"\"\"input: \n",
    "        sec - number of sec from day beginning\n",
    "        year:int, month:int, day:int\n",
    "        \n",
    "        output \n",
    "    \"\"\"\n",
    "    ### Ephem \n",
    "    LAT, LON = 55.775588, 37.605662 # latitude, longitude\n",
    "    observer = ephem.Observer()\n",
    "    observer.lat = LAT\n",
    "    observer.lon = LON\n",
    "    observer.date = datetime(year=year, month=month, day=day)\n",
    "    sun = ephem.Sun(observer)\n",
    "    \n",
    "    features = ['hlon', 'hlat', 'earth_distance', 'phase', 'radius']\n",
    "    data = pd.DataFrame({'nv':[0]*sequence_length, 'sec':[sec]*sequence_length})\n",
    "    \n",
    "    # Generate X data:\n",
    "    for i in features:\n",
    "        data[i] = np.nan\n",
    "    for i, row in enumerate(data.iterrows()):\n",
    "        sun.compute(observer.date.datetime()+timedelta(seconds=int(data.sec[i])))\n",
    "        for feature in features:\n",
    "            data.loc[i, feature] = sun.__getattribute__(feature)\n",
    "    y = data.nv\n",
    "    X = data.drop(columns=\"nv\")\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "#     X = Standart_Scaler_X.transform(X)\n",
    "    if sequence_length==1:\n",
    "        y_pred = model.predict(X.reshape(-1, 6))\n",
    "    else:\n",
    "        y_pred = model.predict(X.reshape(1,sequence_length,6))\n",
    "#     y_pred = Standart_Scaler_y.inverse_transform(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(sec=29254, year=2021, month=7, day=13, sequence_length=6, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34911f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_plot(plot_data, delta, title):\n",
    "#     labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "#     marker = [\".-\", \"rx\", \"go\"]\n",
    "#     time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "#     if delta:\n",
    "#         future = delta\n",
    "#     else:\n",
    "#         future = 0\n",
    "\n",
    "#     plt.title(title)\n",
    "#     for i, val in enumerate(plot_data):\n",
    "#         if i:\n",
    "#             plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "#         else:\n",
    "#             plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "#     plt.legend()\n",
    "#     plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "#     plt.xlabel(\"Time-Step\")\n",
    "#     plt.show()\n",
    "#     return\n",
    "\n",
    "\n",
    "# for x, y in dataset_val.take(5):\n",
    "#     show_plot(\n",
    "#         [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "#         12,\n",
    "#         \"Single Step Prediction\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38bab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset_train = dataset_train.shuffle(256*3).batch(1, drop_remainder=True)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "dataset_val = dataset_val.batch(1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224bc748",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_length = 1\n",
    "# dataset_train = keras.utils.timeseries_dataset_from_array(\n",
    "#     data=X_train,\n",
    "#     targets=y_train,\n",
    "#     sequence_length=sequence_length,\n",
    "#     batch_size=1024,\n",
    "# )\n",
    "# dataset_val = keras.utils.timeseries_dataset_from_array(\n",
    "#     data=X_val,\n",
    "#     targets=y_val,\n",
    "#     sequence_length=sequence_length,\n",
    "#     batch_size=1024,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5393b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b46a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = tf.keras.Sequential([\n",
    "#     keras.layers.BatchNormalization(axis=0),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=1)\n",
    "])\n",
    "mlp_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1**3), loss=\"mse\")\n",
    "mlp_model(inputs)\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f881fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "# bn_out = keras.layers.BatchNormalization()(inputs)\n",
    "# fc1 = keras.layers.Dense(64, activation=\"relu\")(bn_out)\n",
    "# d1 = keras.layers.Dropout(0.2)(fc1)\n",
    "# fc2 = keras.layers.Dense(32, activation=\"relu\")(d1)\n",
    "# d2 = keras.layers.Dropout(0.2)(fc2)\n",
    "# fc3 = keras.layers.Dense(16, activation=\"relu\")(d2)\n",
    "# outputs = keras.layers.Dense(1)(fc3)\n",
    "\n",
    "# mlp_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "# mlp_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1**3), loss=\"mse\")\n",
    "# mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = mlp_model.fit(\n",
    "    dataset_train,\n",
    "    epochs=100,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(sec=23000, year=2021, month=7, day=13, sequence_length=1, model=mlp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197d711",
   "metadata": {},
   "source": [
    "LightGBM\n",
    "\n",
    "Java predictor: https://docs.djl.ai/engines/ml/lightgbm/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f973c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task': 'train', \n",
    "    'objective': 'regression',\n",
    "    'learnnig_rage': 0.005,\n",
    "    'metric': {'l1'},\n",
    "    'verbose': -1\n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974eb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "model = lgb.train(params,\n",
    "                 train_set=lgb_train,\n",
    "                 valid_sets=lgb_val,\n",
    "                 early_stopping_rounds=100,\n",
    "                 num_boost_round=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440aa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(sec=20000, year=2022, month=7, day=13, sequence_length=1, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff99d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
